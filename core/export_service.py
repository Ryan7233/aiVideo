"""
äº§ç‰©æ‰“åŒ…å¯¼å‡ºæœåŠ¡
æ”¯æŒå¯¹è±¡å­˜å‚¨ä¸Šä¼ å’ŒCDNåˆ†å‘
"""

import json
import logging
import zipfile
import shutil
from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime
import hashlib
import uuid

logger = logging.getLogger(__name__)


class ExportService:
    """å¯¼å‡ºæœåŠ¡"""
    
    def __init__(self):
        self.export_base_path = Path("output_data/exports")
        self.export_base_path.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„å¯¼å‡ºæ ¼å¼
        self.supported_formats = {
            'json': self._export_json,
            'zip': self._export_zip,
            'folder': self._export_folder
        }
    
    def export_xiaohongshu_content(self, pipeline_result: Dict, 
                                  export_format: str = "zip",
                                  include_source: bool = False) -> Dict:
        """
        å¯¼å‡ºå°çº¢ä¹¦å†…å®¹äº§ç‰©
        
        Args:
            pipeline_result: æµæ°´çº¿å¤„ç†ç»“æœ
            export_format: å¯¼å‡ºæ ¼å¼ (json/zip/folder)
            include_source: æ˜¯å¦åŒ…å«æºæ–‡ä»¶
            
        Returns:
            å¯¼å‡ºç»“æœä¿¡æ¯
        """
        try:
            # ç”Ÿæˆå¯¼å‡ºID
            export_id = self._generate_export_id(pipeline_result)
            export_path = self.export_base_path / export_id
            export_path.mkdir(parents=True, exist_ok=True)
            
            # æ”¶é›†æ‰€æœ‰äº§ç‰©
            assets = self._collect_assets(pipeline_result, include_source)
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = self._generate_metadata(pipeline_result, assets, export_id)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if export_format in self.supported_formats:
                export_result = self.supported_formats[export_format](
                    export_path, assets, metadata
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}")
            
            # ç”Ÿæˆåˆ†äº«é“¾æ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
            share_urls = self._generate_share_urls(export_result, export_id)
            
            return {
                'export_id': export_id,
                'export_format': export_format,
                'export_path': str(export_result['path']),
                'file_size': export_result['size'],
                'asset_count': len(assets),
                'share_urls': share_urls,
                'metadata': metadata,
                'created_at': datetime.now().isoformat(),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'created_at': datetime.now().isoformat()
            }
    
    def _generate_export_id(self, pipeline_result: Dict) -> str:
        """ç”Ÿæˆå¯¼å‡ºID"""
        # åŸºäºå†…å®¹ç”Ÿæˆå”¯ä¸€ID
        content_hash = hashlib.md5(
            json.dumps(pipeline_result, sort_keys=True, ensure_ascii=False).encode()
        ).hexdigest()[:8]
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"xhs_export_{timestamp}_{content_hash}"
    
    def _collect_assets(self, pipeline_result: Dict, include_source: bool) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰äº§ç‰©æ–‡ä»¶"""
        assets = []
        
        # è§†é¢‘ç‰‡æ®µ
        clips = pipeline_result.get('clips', [])
        for clip in clips:
            if 'output_path' in clip and Path(clip['output_path']).exists():
                assets.append({
                    'type': 'video',
                    'category': 'clip',
                    'path': clip['output_path'],
                    'filename': Path(clip['output_path']).name,
                    'size': Path(clip['output_path']).stat().st_size,
                    'clip_index': clip.get('clip_index', 0),
                    'duration': clip.get('duration', 0)
                })
        
        # å­—å¹•æ–‡ä»¶
        subtitles = pipeline_result.get('subtitles', {})
        for srt_file in subtitles.get('srt_files', []):
            if Path(srt_file).exists():
                assets.append({
                    'type': 'subtitle',
                    'category': 'srt',
                    'path': srt_file,
                    'filename': Path(srt_file).name,
                    'size': Path(srt_file).stat().st_size
                })
        
        for ass_file in subtitles.get('ass_files', []):
            if Path(ass_file).exists():
                assets.append({
                    'type': 'subtitle',
                    'category': 'ass',
                    'path': ass_file,
                    'filename': Path(ass_file).name,
                    'size': Path(ass_file).stat().st_size
                })
        
        # å°é¢å›¾ç‰‡
        cover = pipeline_result.get('cover', {})
        best_cover = cover.get('best_cover', {})
        if best_cover.get('source') and Path(best_cover['source']).exists():
            assets.append({
                'type': 'image',
                'category': 'cover',
                'path': best_cover['source'],
                'filename': Path(best_cover['source']).name,
                'size': Path(best_cover['source']).stat().st_size,
                'cover_type': best_cover.get('type', 'unknown')
            })
        
        # ç²¾é€‰ç…§ç‰‡
        photos = pipeline_result.get('photos_ranked', [])
        for photo in photos[:5]:  # åªåŒ…å«å‰5å¼ 
            if Path(photo['path']).exists():
                assets.append({
                    'type': 'image',
                    'category': 'photo',
                    'path': photo['path'],
                    'filename': Path(photo['path']).name,
                    'size': Path(photo['path']).stat().st_size,
                    'photo_rank': photo.get('rank', 0),
                    'photo_score': photo.get('score', 0)
                })
        
        # æºæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if include_source:
            source_video = pipeline_result.get('source_video', '')
            if source_video and Path(source_video).exists():
                assets.append({
                    'type': 'video',
                    'category': 'source',
                    'path': source_video,
                    'filename': Path(source_video).name,
                    'size': Path(source_video).stat().st_size
                })
        
        return assets
    
    def _generate_metadata(self, pipeline_result: Dict, assets: List[Dict], 
                          export_id: str) -> Dict:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        return {
            'export_info': {
                'export_id': export_id,
                'created_at': datetime.now().isoformat(),
                'generator': 'AI Video Clipper - XHS Pipeline',
                'version': '1.0.0'
            },
            'content_info': {
                'title': pipeline_result.get('draft', {}).get('title', ''),
                'city': pipeline_result.get('storyline', {}).get('city', ''),
                'style': pipeline_result.get('storyline', {}).get('style', ''),
                'duration_estimate': pipeline_result.get('storyline', {}).get('duration_estimate', ''),
                'clip_count': len([a for a in assets if a['type'] == 'video' and a['category'] == 'clip']),
                'photo_count': len([a for a in assets if a['type'] == 'image' and a['category'] == 'photo'])
            },
            'draft_content': pipeline_result.get('draft', {}),
            'storyline': pipeline_result.get('storyline', {}),
            'processing_stats': {
                'total_assets': len(assets),
                'total_size_mb': sum(a['size'] for a in assets) / (1024 * 1024),
                'video_count': len([a for a in assets if a['type'] == 'video']),
                'image_count': len([a for a in assets if a['type'] == 'image']),
                'subtitle_count': len([a for a in assets if a['type'] == 'subtitle'])
            }
        }
    
    def _export_json(self, export_path: Path, assets: List[Dict], metadata: Dict) -> Dict:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        # åˆ›å»ºJSONæ•°æ®
        json_data = {
            'metadata': metadata,
            'assets': assets,
            'file_list': [asset['filename'] for asset in assets]
        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = export_path / 'xiaohongshu_content.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # å¤åˆ¶èµ„æºæ–‡ä»¶
        for asset in assets:
            src_path = Path(asset['path'])
            dst_path = export_path / asset['filename']
            if src_path.exists():
                shutil.copy2(src_path, dst_path)
        
        return {
            'path': export_path,
            'size': sum(f.stat().st_size for f in export_path.iterdir() if f.is_file()),
            'main_file': str(json_file)
        }
    
    def _export_zip(self, export_path: Path, assets: List[Dict], metadata: Dict) -> Dict:
        """å¯¼å‡ºä¸ºZIPæ ¼å¼"""
        zip_file = export_path.parent / f"{export_path.name}.zip"
        
        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:
            # æ·»åŠ å…ƒæ•°æ®
            zf.writestr('metadata.json', json.dumps(metadata, ensure_ascii=False, indent=2))
            
            # æ·»åŠ èµ„æºæ–‡ä»¶
            for asset in assets:
                src_path = Path(asset['path'])
                if src_path.exists():
                    # æ ¹æ®ç±»å‹åˆ›å»ºç›®å½•ç»“æ„
                    if asset['type'] == 'video':
                        arc_name = f"videos/{asset['filename']}"
                    elif asset['type'] == 'image':
                        arc_name = f"images/{asset['filename']}"
                    elif asset['type'] == 'subtitle':
                        arc_name = f"subtitles/{asset['filename']}"
                    else:
                        arc_name = asset['filename']
                    
                    zf.write(src_path, arc_name)
            
            # æ·»åŠ å°çº¢ä¹¦æ–‡æ¡ˆ
            draft = metadata.get('draft_content', {})
            if draft:
                readme_content = self._generate_readme(draft, metadata)
                zf.writestr('å°çº¢ä¹¦æ–‡æ¡ˆ.txt', readme_content)
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if export_path.exists():
            shutil.rmtree(export_path)
        
        return {
            'path': zip_file,
            'size': zip_file.stat().st_size,
            'main_file': str(zip_file)
        }
    
    def _export_folder(self, export_path: Path, assets: List[Dict], metadata: Dict) -> Dict:
        """å¯¼å‡ºä¸ºæ–‡ä»¶å¤¹æ ¼å¼"""
        # åˆ›å»ºå­ç›®å½•
        (export_path / 'videos').mkdir(exist_ok=True)
        (export_path / 'images').mkdir(exist_ok=True)
        (export_path / 'subtitles').mkdir(exist_ok=True)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(export_path / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # å¤åˆ¶èµ„æºæ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
        for asset in assets:
            src_path = Path(asset['path'])
            if src_path.exists():
                if asset['type'] == 'video':
                    dst_path = export_path / 'videos' / asset['filename']
                elif asset['type'] == 'image':
                    dst_path = export_path / 'images' / asset['filename']
                elif asset['type'] == 'subtitle':
                    dst_path = export_path / 'subtitles' / asset['filename']
                else:
                    dst_path = export_path / asset['filename']
                
                shutil.copy2(src_path, dst_path)
        
        # ç”ŸæˆREADME
        draft = metadata.get('draft_content', {})
        if draft:
            readme_content = self._generate_readme(draft, metadata)
            with open(export_path / 'å°çº¢ä¹¦æ–‡æ¡ˆ.txt', 'w', encoding='utf-8') as f:
                f.write(readme_content)
        
        return {
            'path': export_path,
            'size': sum(f.stat().st_size for f in export_path.rglob('*') if f.is_file()),
            'main_file': str(export_path / 'metadata.json')
        }
    
    def _generate_readme(self, draft: Dict, metadata: Dict) -> str:
        """ç”ŸæˆREADMEæ–‡ä»¶å†…å®¹"""
        content = []
        
        # æ ‡é¢˜
        title = draft.get('title', 'å°çº¢ä¹¦å†…å®¹')
        content.append(f"ğŸ“ {title}")
        content.append("=" * 50)
        content.append("")
        
        # æ­£æ–‡
        body = draft.get('body', '')
        if body:
            content.append("ğŸ“– æ­£æ–‡å†…å®¹ï¼š")
            content.append(body)
            content.append("")
        
        # è¯é¢˜æ ‡ç­¾
        hashtags = draft.get('hashtags', [])
        if hashtags:
            content.append("ğŸ·ï¸ è¯é¢˜æ ‡ç­¾ï¼š")
            content.append(" ".join(hashtags))
            content.append("")
        
        # POIä¿¡æ¯
        poi = draft.get('poi', [])
        if poi:
            content.append("ğŸ“ åœ°ç‚¹ä¿¡æ¯ï¼š")
            for p in poi:
                content.append(f"- {p.get('name', '')}")
            content.append("")
        
        # ç”Ÿæˆä¿¡æ¯
        content.append("ğŸ¤– ç”Ÿæˆä¿¡æ¯ï¼š")
        content.append(f"ç”Ÿæˆæ—¶é—´ï¼š{metadata['export_info']['created_at']}")
        content.append(f"å¯¼å‡ºIDï¼š{metadata['export_info']['export_id']}")
        content.append(f"åŸå¸‚ï¼š{metadata['content_info']['city']}")
        content.append(f"é£æ ¼ï¼š{metadata['content_info']['style']}")
        content.append("")
        
        # æ–‡ä»¶åˆ—è¡¨
        content.append("ğŸ“ æ–‡ä»¶è¯´æ˜ï¼š")
        content.append("- videos/: è§†é¢‘ç‰‡æ®µæ–‡ä»¶")
        content.append("- images/: å›¾ç‰‡å’Œå°é¢æ–‡ä»¶")
        content.append("- subtitles/: å­—å¹•æ–‡ä»¶ (SRT/ASSæ ¼å¼)")
        content.append("- metadata.json: è¯¦ç»†å…ƒæ•°æ®")
        
        return "\n".join(content)
    
    def _generate_share_urls(self, export_result: Dict, export_id: str) -> Dict:
        """ç”Ÿæˆåˆ†äº«é“¾æ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        base_url = "https://example.com/exports"  # å®é™…éƒ¨ç½²æ—¶æ›¿æ¢ä¸ºçœŸå®åŸŸå
        
        return {
            'download_url': f"{base_url}/{export_id}/download",
            'preview_url': f"{base_url}/{export_id}/preview",
            'share_code': export_id[-8:].upper(),  # 8ä½åˆ†äº«ç 
            'expires_at': (datetime.now()).isoformat(),  # ç®€åŒ–ç‰ˆä¸è®¾è¿‡æœŸ
            'qr_code_url': f"{base_url}/{export_id}/qr"
        }
    
    def cleanup_old_exports(self, days: int = 7) -> Dict:
        """æ¸…ç†æ—§çš„å¯¼å‡ºæ–‡ä»¶"""
        try:
            cleaned_count = 0
            total_size_freed = 0
            
            cutoff_time = datetime.now().timestamp() - (days * 24 * 3600)
            
            for export_dir in self.export_base_path.iterdir():
                if export_dir.is_dir():
                    dir_time = export_dir.stat().st_mtime
                    if dir_time < cutoff_time:
                        size = sum(f.stat().st_size for f in export_dir.rglob('*') if f.is_file())
                        shutil.rmtree(export_dir)
                        cleaned_count += 1
                        total_size_freed += size
                
                elif export_dir.is_file() and export_dir.suffix == '.zip':
                    file_time = export_dir.stat().st_mtime
                    if file_time < cutoff_time:
                        size = export_dir.stat().st_size
                        export_dir.unlink()
                        cleaned_count += 1
                        total_size_freed += size
            
            return {
                'cleaned_count': cleaned_count,
                'size_freed_mb': total_size_freed / (1024 * 1024),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†å¯¼å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }


# å…¨å±€æœåŠ¡å®ä¾‹
export_service = None

def get_export_service() -> ExportService:
    """è·å–å¯¼å‡ºæœåŠ¡å®ä¾‹"""
    global export_service
    if export_service is None:
        export_service = ExportService()
    return export_service
