"""
å­—å¹•ç”Ÿæˆä¸æ–­å¥æœåŠ¡
æ”¯æŒSRT/ASSæ ¼å¼ï¼Œè¡¨æƒ…ç¬¦å·ï¼ŒåŒè¯­å­—å¹•
"""

import re
import logging
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from datetime import timedelta
import random

logger = logging.getLogger(__name__)


class SubtitleGenerator:
    """å­—å¹•ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.emoji_mapping = {
            # æƒ…æ„Ÿè¡¨æƒ…
            'å¼€å¿ƒ': ['ğŸ˜Š', 'ğŸ˜„', 'ğŸ¥°', 'ğŸ˜'],
            'æƒŠè®¶': ['ğŸ˜±', 'ğŸ˜²', 'ğŸ¤©', 'ğŸ˜¯'],
            'å–œæ¬¢': ['â¤ï¸', 'ğŸ’•', 'ğŸ‘', 'ğŸ¥°'],
            'ç¾å‘³': ['ğŸ˜‹', 'ğŸ¤¤', 'ğŸ‘Œ', 'ğŸ’¯'],
            'ç¾ä¸½': ['âœ¨', 'ğŸŒŸ', 'ğŸ’–', 'ğŸ˜'],
            'ç´¯': ['ğŸ˜´', 'ğŸ˜ª', 'ğŸ¥±'],
            'è´µ': ['ğŸ’¸', 'ğŸ’°', 'ğŸ˜­'],
            'ä¾¿å®œ': ['ğŸ’°', 'ğŸ‘', 'ğŸ˜Š'],
            # ç‰©å“è¡¨æƒ…
            'é£Ÿç‰©': ['ğŸœ', 'ğŸ±', 'ğŸ¥˜', 'ğŸ²'],
            'é£æ™¯': ['ğŸï¸', 'ğŸŒ…', 'ğŸ“¸', 'ğŸŒ¸'],
            'å»ºç­‘': ['ğŸ›ï¸', 'ğŸ°', 'ğŸ•Œ', 'ğŸ—¼'],
            'äº¤é€š': ['ğŸš‡', 'ğŸšŒ', 'ğŸš—', 'ğŸš¶â€â™€ï¸']
        }
        
        self.punctuation_pause = {
            'ï¼Œ': 0.3,
            'ã€‚': 0.5,
            'ï¼': 0.4,
            'ï¼Ÿ': 0.4,
            'ï¼›': 0.4,
            'ï¼š': 0.3
        }
    
    def generate_subtitles(self, clips: List[Dict], transcript_mmss: List[Dict], 
                          style: str = "å£è¯­") -> Dict:
        """
        ç”Ÿæˆå­—å¹•æ–‡ä»¶
        
        Args:
            clips: è§†é¢‘ç‰‡æ®µä¿¡æ¯
            transcript_mmss: å¸¦æ—¶é—´æˆ³çš„è½¬å½•æ–‡æœ¬
            style: å­—å¹•é£æ ¼ï¼ˆå£è¯­/ä¹¦é¢/å¯çˆ±ï¼‰
            
        Returns:
            å­—å¹•æ•°æ®å’Œæ–‡ä»¶è·¯å¾„
        """
        try:
            subtitles_data = []
            
            for clip_idx, clip in enumerate(clips):
                clip_start = clip.get('start_time_seconds', 0)
                clip_end = clip.get('end_time_seconds', 0)
                
                # æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è½¬å½•ç‰‡æ®µ
                relevant_segments = self._find_relevant_segments(
                    transcript_mmss, clip_start, clip_end
                )
                
                # ç”Ÿæˆè¯¥ç‰‡æ®µçš„å­—å¹•
                clip_subtitles = self._generate_clip_subtitles(
                    relevant_segments, clip_start, style, clip_idx
                )
                
                subtitles_data.append({
                    'clip_index': clip_idx,
                    'clip_start': clip_start,
                    'clip_end': clip_end,
                    'subtitles': clip_subtitles,
                    'subtitle_count': len(clip_subtitles)
                })
            
            # ç”Ÿæˆå­—å¹•æ–‡ä»¶
            srt_files = self._generate_srt_files(subtitles_data)
            ass_files = self._generate_ass_files(subtitles_data, style)
            
            return {
                'subtitles_data': subtitles_data,
                'srt_files': srt_files,
                'ass_files': ass_files,
                'total_clips': len(clips),
                'total_subtitles': sum(len(sub['subtitles']) for sub in subtitles_data),
                'style': style
            }
            
        except Exception as e:
            logger.error(f"å­—å¹•ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def _find_relevant_segments(self, transcript_mmss: List[Dict], 
                               start_time: float, end_time: float) -> List[Dict]:
        """æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è½¬å½•ç‰‡æ®µ"""
        relevant = []
        
        for segment in transcript_mmss:
            seg_start = segment.get('start', 0)
            seg_end = segment.get('end', 0)
            
            # æ£€æŸ¥æ—¶é—´é‡å 
            if not (seg_end < start_time or seg_start > end_time):
                # è°ƒæ•´æ—¶é—´æˆ³ä¸ºç›¸å¯¹äºç‰‡æ®µå¼€å§‹çš„æ—¶é—´
                adjusted_segment = segment.copy()
                adjusted_segment['start'] = max(0, seg_start - start_time)
                adjusted_segment['end'] = min(end_time - start_time, seg_end - start_time)
                relevant.append(adjusted_segment)
        
        return relevant
    
    def _generate_clip_subtitles(self, segments: List[Dict], clip_start: float, 
                                style: str, clip_idx: int) -> List[Dict]:
        """ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„å­—å¹•"""
        subtitles = []
        subtitle_id = 1
        
        for segment in segments:
            text = segment.get('text', '').strip()
            start_time = segment.get('start', 0)
            end_time = segment.get('end', 0)
            
            if not text:
                continue
            
            # æ ¹æ®é£æ ¼å¤„ç†æ–‡æœ¬
            processed_text = self._process_text_by_style(text, style)
            
            # æ–­å¥å¤„ç†
            sentences = self._split_sentences(processed_text)
            
            # ä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆå­—å¹•
            sentence_duration = (end_time - start_time) / max(len(sentences), 1)
            
            for i, sentence in enumerate(sentences):
                if not sentence.strip():
                    continue
                
                sub_start = start_time + i * sentence_duration
                sub_end = min(start_time + (i + 1) * sentence_duration, end_time)
                
                # æ·»åŠ è¡¨æƒ…ç¬¦å·
                if style == "å¯çˆ±":
                    sentence = self._add_emoji_to_text(sentence)
                
                subtitles.append({
                    'id': subtitle_id,
                    'start_time': sub_start,
                    'end_time': sub_end,
                    'start_timecode': self._seconds_to_timecode(sub_start),
                    'end_timecode': self._seconds_to_timecode(sub_end),
                    'text': sentence,
                    'original_text': text,
                    'style': style
                })
                
                subtitle_id += 1
        
        return subtitles
    
    def _process_text_by_style(self, text: str, style: str) -> str:
        """æ ¹æ®é£æ ¼å¤„ç†æ–‡æœ¬"""
        if style == "ä¹¦é¢":
            # ä¹¦é¢è¯­å¤„ç†ï¼šè§„èŒƒåŒ–è¡¨è¾¾
            text = re.sub(r'å—¯+', 'å—¯', text)
            text = re.sub(r'å•Š+', 'å•Š', text)
            text = re.sub(r'å‘ƒ+', '', text)
            text = re.sub(r'\s+', ' ', text)
        elif style == "å£è¯­":
            # ä¿æŒå£è¯­åŒ–ç‰¹å¾
            pass
        elif style == "å¯çˆ±":
            # å¯çˆ±é£æ ¼ï¼šæ·»åŠ è¯­æ°”è¯
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])$', r'\1~', text)
        
        return text.strip()
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–­å¥"""
        # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å¥
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›])', text)
        
        # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹
        result = []
        for i in range(0, len(sentences) - 1, 2):
            sentence = sentences[i]
            punctuation = sentences[i + 1] if i + 1 < len(sentences) else ''
            
            if sentence.strip():
                combined = sentence + punctuation
                
                # å¦‚æœå¥å­å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                if len(combined) > 20:
                    sub_sentences = self._split_long_sentence(combined)
                    result.extend(sub_sentences)
                else:
                    result.append(combined)
        
        # å¤„ç†æœ€åä¸€ä¸ªå¥å­ï¼ˆå¦‚æœæ²¡æœ‰æ ‡ç‚¹ï¼‰
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            last_sentence = sentences[-1]
            if len(last_sentence) > 20:
                result.extend(self._split_long_sentence(last_sentence))
            else:
                result.append(last_sentence)
        
        return [s.strip() for s in result if s.strip()]
    
    def _split_long_sentence(self, sentence: str) -> List[str]:
        """åˆ†å‰²é•¿å¥å­"""
        # æŒ‰é€—å·åˆ†å‰²
        parts = sentence.split('ï¼Œ')
        
        result = []
        current_part = ""
        
        for part in parts:
            if len(current_part + part) <= 15:
                current_part += part + "ï¼Œ"
            else:
                if current_part:
                    result.append(current_part.rstrip('ï¼Œ'))
                current_part = part + "ï¼Œ"
        
        if current_part:
            result.append(current_part.rstrip('ï¼Œ'))
        
        return result
    
    def _add_emoji_to_text(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬æ·»åŠ è¡¨æƒ…ç¬¦å·"""
        try:
            # æ ¹æ®å…³é”®è¯æ·»åŠ è¡¨æƒ…
            for keyword, emojis in self.emoji_mapping.items():
                if any(word in text for word in keyword.split()):
                    emoji = random.choice(emojis)
                    text = text + emoji
                    break
            
            return text
        except Exception:
            return text
    
    def _seconds_to_timecode(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºæ—¶é—´ç æ ¼å¼"""
        td = timedelta(seconds=seconds)
        hours = int(td.seconds // 3600)
        minutes = int((td.seconds % 3600) // 60)
        secs = int(td.seconds % 60)
        milliseconds = int(td.microseconds // 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _generate_srt_files(self, subtitles_data: List[Dict]) -> List[str]:
        """ç”ŸæˆSRTå­—å¹•æ–‡ä»¶"""
        srt_files = []
        
        for clip_data in subtitles_data:
            clip_idx = clip_data['clip_index']
            subtitles = clip_data['subtitles']
            
            srt_content = []
            
            for subtitle in subtitles:
                srt_content.append(f"{subtitle['id']}")
                srt_content.append(f"{subtitle['start_timecode']} --> {subtitle['end_timecode']}")
                srt_content.append(subtitle['text'])
                srt_content.append("")  # ç©ºè¡Œ
            
            # ä¿å­˜æ–‡ä»¶
            srt_filename = f"output_data/subtitles_clip_{clip_idx:02d}.srt"
            Path("output_data").mkdir(parents=True, exist_ok=True)
            
            with open(srt_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(srt_content))
            
            srt_files.append(srt_filename)
        
        return srt_files
    
    def _generate_ass_files(self, subtitles_data: List[Dict], style: str) -> List[str]:
        """ç”ŸæˆASSé«˜çº§å­—å¹•æ–‡ä»¶"""
        ass_files = []
        
        # ASSæ ·å¼å®šä¹‰
        style_definitions = {
            "å£è¯­": {
                "fontsize": "20",
                "color": "&H00FFFFFF",  # ç™½è‰²
                "outline": "2",
                "shadow": "0"
            },
            "ä¹¦é¢": {
                "fontsize": "18",
                "color": "&H00FFFF00",  # é»„è‰²
                "outline": "1",
                "shadow": "1"
            },
            "å¯çˆ±": {
                "fontsize": "22",
                "color": "&H00FF69B4",  # ç²‰è‰²
                "outline": "2",
                "shadow": "0"
            }
        }
        
        current_style = style_definitions.get(style, style_definitions["å£è¯­"])
        
        for clip_data in subtitles_data:
            clip_idx = clip_data['clip_index']
            subtitles = clip_data['subtitles']
            
            ass_content = [
                "[Script Info]",
                "Title: AI Video Clipper Subtitles",
                "ScriptType: v4.00+",
                "",
                "[V4+ Styles]",
                "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding",
                f"Style: Default,Arial,{current_style['fontsize']},{current_style['color']},&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,{current_style['outline']},{current_style['shadow']},2,10,10,10,1",
                "",
                "[Events]",
                "Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text"
            ]
            
            for subtitle in subtitles:
                start_time = self._seconds_to_ass_time(subtitle['start_time'])
                end_time = self._seconds_to_ass_time(subtitle['end_time'])
                text = subtitle['text']
                
                ass_content.append(f"Dialogue: 0,{start_time},{end_time},Default,,0,0,0,,{text}")
            
            # ä¿å­˜æ–‡ä»¶
            ass_filename = f"output_data/subtitles_clip_{clip_idx:02d}.ass"
            
            with open(ass_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(ass_content))
            
            ass_files.append(ass_filename)
        
        return ass_files
    
    def _seconds_to_ass_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºASSæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        centiseconds = int((seconds % 1) * 100)
        
        return f"{hours}:{minutes:02d}:{secs:02d}.{centiseconds:02d}"


class CoverSuggestionService:
    """å°é¢å»ºè®®æœåŠ¡"""
    
    def __init__(self):
        self.title_styles = {
            "ç®€çº¦": {
                "font": "Arial",
                "color": "#FFFFFF",
                "background": "rgba(0,0,0,0.5)",
                "position": "bottom"
            },
            "æ´»æ³¼": {
                "font": "Comic Sans MS",
                "color": "#FF69B4",
                "background": "rgba(255,255,255,0.8)",
                "position": "top"
            },
            "ä¸“ä¸š": {
                "font": "Helvetica",
                "color": "#333333",
                "background": "rgba(255,255,255,0.9)",
                "position": "center"
            }
        }
    
    def suggest_cover(self, clips: List[Dict], photos_topk: List[Dict], 
                     title: str = "") -> Dict:
        """
        ç”Ÿæˆå°é¢å»ºè®®
        
        Args:
            clips: è§†é¢‘ç‰‡æ®µä¿¡æ¯
            photos_topk: æ’åºåçš„ç…§ç‰‡
            title: æ ‡é¢˜æ–‡æœ¬
            
        Returns:
            å°é¢å»ºè®®ä¿¡æ¯
        """
        try:
            suggestions = []
            
            # ä»è§†é¢‘å¸§ä¸­é€‰æ‹©å°é¢
            for clip in clips[:3]:  # åªè€ƒè™‘å‰3ä¸ªç‰‡æ®µ
                frame_suggestions = self._analyze_video_frames(clip, title)
                suggestions.extend(frame_suggestions)
            
            # ä»ç…§ç‰‡ä¸­é€‰æ‹©å°é¢
            for photo in photos_topk[:5]:  # åªè€ƒè™‘å‰5å¼ ç…§ç‰‡
                photo_suggestion = self._analyze_photo_cover(photo, title)
                suggestions.append(photo_suggestion)
            
            # æŒ‰è¯„åˆ†æ’åº
            suggestions.sort(key=lambda x: x['score'], reverse=True)
            
            # é€‰æ‹©æœ€ä½³å»ºè®®
            best_suggestion = suggestions[0] if suggestions else self._get_fallback_cover()
            
            return {
                'best_cover': best_suggestion,
                'alternatives': suggestions[1:4],  # æä¾›3ä¸ªå¤‡é€‰
                'title_overlay': self._generate_title_overlay(title, best_suggestion),
                'color_palette': self._extract_color_palette(best_suggestion),
                'total_suggestions': len(suggestions)
            }
            
        except Exception as e:
            logger.error(f"å°é¢å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_video_frames(self, clip: Dict, title: str) -> List[Dict]:
        """åˆ†æè§†é¢‘å¸§ä½œä¸ºå°é¢"""
        suggestions = []
        
        start_time = clip.get('start_time_seconds', 0)
        duration = clip.get('duration', 0)
        clip_path = clip.get('output_path', '')
        
        # é€‰æ‹©å…³é”®å¸§æ—¶é—´ç‚¹
        key_moments = [
            start_time + duration * 0.1,   # å¼€å§‹å10%
            start_time + duration * 0.3,   # 30%å¤„
            start_time + duration * 0.5,   # ä¸­é—´
            start_time + duration * 0.7,   # 70%å¤„
        ]
        
        for i, moment in enumerate(key_moments):
            suggestions.append({
                'type': 'video_frame',
                'source': clip_path,
                'timecode': self._seconds_to_timecode(moment - start_time),
                'timestamp': moment,
                'score': self._calculate_frame_score(moment, duration, i),
                'description': f'è§†é¢‘ç¬¬{i+1}ä¸ªå…³é”®å¸§',
                'extraction_command': f'ffmpeg -i "{clip_path}" -ss {moment-start_time:.2f} -vframes 1 -q:v 2'
            })
        
        return suggestions
    
    def _analyze_photo_cover(self, photo: Dict, title: str) -> Dict:
        """åˆ†æç…§ç‰‡ä½œä¸ºå°é¢"""
        return {
            'type': 'photo',
            'source': photo['path'],
            'photo_id': photo['photo_id'],
            'score': photo['score'] * 0.9,  # ç…§ç‰‡åˆ†æ•°ç¨å¾®é™ä½
            'description': f'ç²¾é€‰ç…§ç‰‡ (æ’å#{photo["rank"]})',
            'tags': photo.get('tags', []),
            'aesthetic_score': photo.get('aesthetic_score', 0.5)
        }
    
    def _calculate_frame_score(self, timestamp: float, total_duration: float, 
                              frame_index: int) -> float:
        """è®¡ç®—è§†é¢‘å¸§è¯„åˆ†"""
        # åŸºç¡€åˆ†æ•°
        base_score = 0.6
        
        # ä½ç½®åå¥½ï¼ˆä¸­é—´ä½ç½®åˆ†æ•°æ›´é«˜ï¼‰
        position_factor = 1.0 - abs(0.5 - timestamp / total_duration)
        base_score += position_factor * 0.3
        
        # é¿å…å¼€å¤´å’Œç»“å°¾
        if timestamp < total_duration * 0.1 or timestamp > total_duration * 0.9:
            base_score -= 0.2
        
        # å¤šæ ·æ€§åŠ åˆ†
        diversity_bonus = frame_index * 0.05
        base_score += diversity_bonus
        
        return min(base_score, 1.0)
    
    def _generate_title_overlay(self, title: str, cover_suggestion: Dict) -> Dict:
        """ç”Ÿæˆæ ‡é¢˜å åŠ å»ºè®®"""
        # æ ¹æ®å°é¢ç±»å‹é€‰æ‹©é£æ ¼
        if cover_suggestion['type'] == 'photo':
            style_name = "ç®€çº¦"
        else:
            style_name = "æ´»æ³¼"
        
        style = self.title_styles[style_name]
        
        # å¤„ç†æ ‡é¢˜æ–‡æœ¬
        short_title = title[:12] + "..." if len(title) > 12 else title
        
        return {
            'text': short_title,
            'full_text': title,
            'style': style_name,
            'font': style['font'],
            'color': style['color'],
            'background': style['background'],
            'position': style['position'],
            'font_size': self._calculate_font_size(short_title),
            'shadow': True,
            'animation': 'fade_in'
        }
    
    def _calculate_font_size(self, text: str) -> int:
        """æ ¹æ®æ–‡æœ¬é•¿åº¦è®¡ç®—å­—ä½“å¤§å°"""
        base_size = 36
        if len(text) > 10:
            return base_size - 4
        elif len(text) > 6:
            return base_size
        else:
            return base_size + 4
    
    def _extract_color_palette(self, cover_suggestion: Dict) -> Dict:
        """æå–é¢œè‰²è°ƒè‰²æ¿ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€åŒ–ç‰ˆé¢œè‰²æå–
        default_palettes = {
            'warm': ['#FF6B6B', '#FFE66D', '#FF8E53'],
            'cool': ['#4ECDC4', '#45B7D1', '#96CEB4'],
            'neutral': ['#95A5A6', '#BDC3C7', '#ECF0F1']
        }
        
        # æ ¹æ®å°é¢ç±»å‹é€‰æ‹©è°ƒè‰²æ¿
        if cover_suggestion.get('tags'):
            if any(tag in ['é£æ™¯', 'è‡ªç„¶'] for tag in cover_suggestion['tags']):
                palette_name = 'cool'
            elif any(tag in ['ç¾é£Ÿ', 'äººç‰©'] for tag in cover_suggestion['tags']):
                palette_name = 'warm'
            else:
                palette_name = 'neutral'
        else:
            palette_name = 'neutral'
        
        return {
            'name': palette_name,
            'primary': default_palettes[palette_name][0],
            'secondary': default_palettes[palette_name][1],
            'accent': default_palettes[palette_name][2],
            'colors': default_palettes[palette_name]
        }
    
    def _get_fallback_cover(self) -> Dict:
        """è·å–é»˜è®¤å°é¢å»ºè®®"""
        return {
            'type': 'default',
            'source': 'default_cover.jpg',
            'score': 0.3,
            'description': 'é»˜è®¤å°é¢',
            'timecode': '00:00:00'
        }
    
    def _seconds_to_timecode(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºæ—¶é—´ç """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"


# å…¨å±€æœåŠ¡å®ä¾‹
subtitle_generator = None
cover_service = None

def get_subtitle_generator() -> SubtitleGenerator:
    """è·å–å­—å¹•ç”Ÿæˆå™¨å®ä¾‹"""
    global subtitle_generator
    if subtitle_generator is None:
        subtitle_generator = SubtitleGenerator()
    return subtitle_generator

def get_cover_service() -> CoverSuggestionService:
    """è·å–å°é¢å»ºè®®æœåŠ¡å®ä¾‹"""
    global cover_service
    if cover_service is None:
        cover_service = CoverSuggestionService()
    return cover_service
