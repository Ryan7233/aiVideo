"""
Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æœåŠ¡
æä¾›é«˜æ€§èƒ½çš„è¯­éŸ³è½¬æ–‡å­—åŠŸèƒ½ï¼Œæ”¯æŒå¤šè¯­è¨€å’Œå­—å¹•ç”Ÿæˆ
"""

import os
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
import subprocess

try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    WhisperModel = None

logger = logging.getLogger(__name__)


class WhisperASRService:
    """Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æœåŠ¡"""
    
    def __init__(self, model_size: str = "base", device: str = "auto", compute_type: str = "auto"):
        """
        åˆå§‹åŒ–Whisper ASRæœåŠ¡
        
        Args:
            model_size: æ¨¡å‹å¤§å° (tiny, base, small, medium, large, large-v2, large-v3)
            device: è®¾å¤‡ç±»å‹ (cpu, cuda, auto)
            compute_type: è®¡ç®—ç±»å‹ (int8, int16, float16, float32, auto)
        """
        self.model_size = model_size
        self.device = device
        self.compute_type = compute_type
        self.model = None
        self.model_loaded = False
        
        # æ”¯æŒçš„è¯­è¨€ä»£ç 
        self.supported_languages = {
            'zh': 'ä¸­æ–‡', 'en': 'è‹±è¯­', 'ja': 'æ—¥è¯­', 'ko': 'éŸ©è¯­',
            'es': 'è¥¿ç­ç‰™è¯­', 'fr': 'æ³•è¯­', 'de': 'å¾·è¯­', 'ru': 'ä¿„è¯­',
            'ar': 'é˜¿æ‹‰ä¼¯è¯­', 'hi': 'å°åœ°è¯­', 'pt': 'è‘¡è„ç‰™è¯­', 'it': 'æ„å¤§åˆ©è¯­'
        }
        
        # å­—å¹•æ ¼å¼æ”¯æŒ
        self.subtitle_formats = ['srt', 'vtt', 'txt', 'json']
        
        logger.info(f"Whisper ASRæœåŠ¡åˆå§‹åŒ– - æ¨¡å‹: {model_size}, è®¾å¤‡: {device}")
    
    def _load_model(self) -> bool:
        """å»¶è¿ŸåŠ è½½Whisperæ¨¡å‹"""
        if not WHISPER_AVAILABLE:
            logger.error("faster-whisperæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install faster-whisper")
            return False
        
        if self.model_loaded:
            return True
        
        try:
            logger.info(f"æ­£åœ¨åŠ è½½Whisperæ¨¡å‹: {self.model_size}")
            start_time = time.time()
            
            # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡å’Œè®¡ç®—ç±»å‹
            if self.device == "auto":
                try:
                    import torch
                    self.device = "cuda" if torch.cuda.is_available() else "cpu"
                except ImportError:
                    self.device = "cpu"
            
            if self.compute_type == "auto":
                self.compute_type = "float16" if self.device == "cuda" else "int8"
            
            # åŠ è½½æ¨¡å‹
            self.model = WhisperModel(
                self.model_size,
                device=self.device,
                compute_type=self.compute_type,
                download_root="models/whisper"  # æ¨¡å‹ç¼“å­˜ç›®å½•
            )
            
            load_time = time.time() - start_time
            self.model_loaded = True
            
            logger.info(f"âœ… Whisperæ¨¡å‹åŠ è½½æˆåŠŸ - è€—æ—¶: {load_time:.2f}ç§’")
            logger.info(f"   è®¾å¤‡: {self.device}, è®¡ç®—ç±»å‹: {self.compute_type}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def extract_audio_from_video(self, video_path: str, audio_path: str = None, 
                                sample_rate: int = 16000) -> str:
        """
        ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            audio_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            sample_rate: é‡‡æ ·ç‡
            
        Returns:
            æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        try:
            if not audio_path:
                video_stem = Path(video_path).stem
                audio_path = f"temp_audio_{video_stem}_{int(time.time())}.wav"
            
            # ä½¿ç”¨FFmpegæå–éŸ³é¢‘
            cmd = [
                "ffmpeg", "-hide_banner", "-loglevel", "error",
                "-i", video_path,
                "-ar", str(sample_rate),  # é‡‡æ ·ç‡
                "-ac", "1",               # å•å£°é“
                "-c:a", "pcm_s16le",      # PCMç¼–ç 
                "-y",                     # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                audio_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                raise RuntimeError(f"éŸ³é¢‘æå–å¤±è´¥: {result.stderr}")
            
            if not os.path.exists(audio_path):
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {audio_path}")
            
            logger.info(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
            return audio_path
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {str(e)}")
            raise
    
    def detect_language(self, audio_path: str) -> Tuple[str, float]:
        """
        æ£€æµ‹éŸ³é¢‘è¯­è¨€
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            (è¯­è¨€ä»£ç , ç½®ä¿¡åº¦)
        """
        if not self._load_model():
            return "en", 0.0
        
        try:
            # ä½¿ç”¨Whisperæ£€æµ‹è¯­è¨€
            segments, info = self.model.transcribe(
                audio_path, 
                language=None,  # è‡ªåŠ¨æ£€æµ‹
                task="transcribe",
                vad_filter=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            detected_language = info.language
            confidence = info.language_probability
            
            language_name = self.supported_languages.get(detected_language, detected_language)
            
            logger.info(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {language_name} ({detected_language}) - ç½®ä¿¡åº¦: {confidence:.3f}")
            
            return detected_language, confidence
            
        except Exception as e:
            logger.error(f"âŒ è¯­è¨€æ£€æµ‹å¤±è´¥: {str(e)}")
            return "en", 0.0
    
    def transcribe_audio(self, audio_path: str, language: str = None, 
                        task: str = "transcribe", **kwargs) -> Dict:
        """
        è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: æŒ‡å®šè¯­è¨€ï¼ˆNoneä¸ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            task: ä»»åŠ¡ç±»å‹ (transcribe/translate)
            **kwargs: å…¶ä»–Whisperå‚æ•°
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        if not self._load_model():
            raise RuntimeError("Whisperæ¨¡å‹æœªåŠ è½½")
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path}")
            start_time = time.time()
            
            # é»˜è®¤å‚æ•°
            default_params = {
                "beam_size": 5,
                "best_of": 5,
                "patience": 1,
                "length_penalty": 1,
                "repetition_penalty": 1,
                "no_repeat_ngram_size": 0,
                "temperature": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
                "compression_ratio_threshold": 2.4,
                "log_prob_threshold": -1.0,
                "no_speech_threshold": 0.6,
                "condition_on_previous_text": True,
                "prompt_reset_on_temperature": 0.5,
                "initial_prompt": None,
                "prefix": None,
                "suppress_blank": True,
                "suppress_tokens": [-1],
                "without_timestamps": False,
                "max_initial_timestamp": 1.0,
                "word_timestamps": False,
                "prepend_punctuations": "\"'([{-",
                "append_punctuations": "\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼š\")]}ã€",
                "vad_filter": True,
                "vad_parameters": dict(
                    min_silence_duration_ms=500,
                    speech_pad_ms=400
                )
            }
            
            # åˆå¹¶ç”¨æˆ·å‚æ•°
            params = {**default_params, **kwargs}
            
            # æ‰§è¡Œè½¬å½•
            segments, info = self.model.transcribe(
                audio_path,
                language=language,
                task=task,
                **params
            )
            
            # å¤„ç†è½¬å½•ç»“æœ
            transcription_result = self._process_transcription_result(segments, info)
            
            processing_time = time.time() - start_time
            transcription_result['processing_time'] = processing_time
            
            logger.info(f"âœ… è½¬å½•å®Œæˆ - è€—æ—¶: {processing_time:.2f}ç§’")
            logger.info(f"   è¯­è¨€: {info.language} - æ—¶é•¿: {info.duration:.2f}ç§’")
            
            return transcription_result
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {str(e)}")
            raise
    
    def _process_transcription_result(self, segments, info) -> Dict:
        """å¤„ç†è½¬å½•ç»“æœ"""
        result = {
            'language': info.language,
            'language_probability': info.language_probability,
            'duration': info.duration,
            'segments': [],
            'full_text': '',
            'word_count': 0,
            'segment_count': 0
        }
        
        full_text_parts = []
        
        for segment in segments:
            segment_data = {
                'id': segment.id,
                'seek': segment.seek,
                'start': segment.start,
                'end': segment.end,
                'text': segment.text.strip(),
                'tokens': segment.tokens,
                'temperature': segment.temperature,
                'avg_logprob': segment.avg_logprob,
                'compression_ratio': segment.compression_ratio,
                'no_speech_prob': segment.no_speech_prob
            }
            
            result['segments'].append(segment_data)
            full_text_parts.append(segment.text.strip())
        
        result['full_text'] = ' '.join(full_text_parts)
        result['word_count'] = len(result['full_text'].split())
        result['segment_count'] = len(result['segments'])
        
        return result
    
    def transcribe_video(self, video_path: str, language: str = None, 
                        cleanup_audio: bool = True, **kwargs) -> Dict:
        """
        ç›´æ¥è½¬å½•è§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            language: æŒ‡å®šè¯­è¨€
            cleanup_audio: æ˜¯å¦æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            **kwargs: å…¶ä»–è½¬å½•å‚æ•°
            
        Returns:
            è½¬å½•ç»“æœ
        """
        temp_audio_path = None
        
        try:
            # æå–éŸ³é¢‘
            temp_audio_path = self.extract_audio_from_video(video_path)
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¯­è¨€ï¼Œå…ˆæ£€æµ‹è¯­è¨€
            if language is None:
                detected_lang, confidence = self.detect_language(temp_audio_path)
                if confidence > 0.7:  # ç½®ä¿¡åº¦é˜ˆå€¼
                    language = detected_lang
                    logger.info(f"ğŸ¯ ä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€: {language} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # è½¬å½•éŸ³é¢‘
            result = self.transcribe_audio(temp_audio_path, language=language, **kwargs)
            
            # æ·»åŠ è§†é¢‘ä¿¡æ¯
            result['video_path'] = video_path
            result['audio_extracted'] = True
            
            return result
            
        finally:
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            if cleanup_audio and temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.remove(temp_audio_path)
                    logger.info(f"ğŸ—‘ï¸  ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å·²æ¸…ç†: {temp_audio_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
    
    def generate_subtitles(self, transcription_result: Dict, 
                          format: str = "srt", output_path: str = None) -> str:
        """
        ç”Ÿæˆå­—å¹•æ–‡ä»¶
        
        Args:
            transcription_result: è½¬å½•ç»“æœ
            format: å­—å¹•æ ¼å¼ (srt, vtt, txt, json)
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å­—å¹•æ–‡ä»¶è·¯å¾„
        """
        if format not in self.subtitle_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„å­—å¹•æ ¼å¼: {format}")
        
        if not output_path:
            timestamp = int(time.time())
            output_path = f"subtitles_{timestamp}.{format}"
        
        try:
            segments = transcription_result.get('segments', [])
            
            if format == "srt":
                content = self._generate_srt(segments)
            elif format == "vtt":
                content = self._generate_vtt(segments)
            elif format == "txt":
                content = self._generate_txt(segments)
            elif format == "json":
                content = json.dumps(transcription_result, ensure_ascii=False, indent=2)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"âœ… å­—å¹•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"âŒ å­—å¹•ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    def _generate_srt(self, segments: List[Dict]) -> str:
        """ç”ŸæˆSRTæ ¼å¼å­—å¹•"""
        srt_content = []
        
        for i, segment in enumerate(segments, 1):
            start_time = self._seconds_to_srt_time(segment['start'])
            end_time = self._seconds_to_srt_time(segment['end'])
            text = segment['text'].strip()
            
            srt_content.append(f"{i}")
            srt_content.append(f"{start_time} --> {end_time}")
            srt_content.append(text)
            srt_content.append("")  # ç©ºè¡Œ
        
        return "\n".join(srt_content)
    
    def _generate_vtt(self, segments: List[Dict]) -> str:
        """ç”ŸæˆVTTæ ¼å¼å­—å¹•"""
        vtt_content = ["WEBVTT", ""]
        
        for segment in segments:
            start_time = self._seconds_to_vtt_time(segment['start'])
            end_time = self._seconds_to_vtt_time(segment['end'])
            text = segment['text'].strip()
            
            vtt_content.append(f"{start_time} --> {end_time}")
            vtt_content.append(text)
            vtt_content.append("")
        
        return "\n".join(vtt_content)
    
    def _generate_txt(self, segments: List[Dict]) -> str:
        """ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼"""
        txt_content = []
        
        for segment in segments:
            timestamp = self._seconds_to_readable_time(segment['start'])
            text = segment['text'].strip()
            txt_content.append(f"[{timestamp}] {text}")
        
        return "\n".join(txt_content)
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _seconds_to_vtt_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºVTTæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"
    
    def _seconds_to_readable_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºå¯è¯»æ—¶é—´æ ¼å¼"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_size': self.model_size,
            'device': self.device,
            'compute_type': self.compute_type,
            'model_loaded': self.model_loaded,
            'whisper_available': WHISPER_AVAILABLE,
            'supported_languages': self.supported_languages,
            'subtitle_formats': self.subtitle_formats
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.model:
            del self.model
            self.model = None
            self.model_loaded = False
            logger.info("ğŸ§¹ Whisperæ¨¡å‹èµ„æºå·²æ¸…ç†")


# å…¨å±€ASRæœåŠ¡å®ä¾‹
asr_service = None

def get_asr_service(model_size: str = "base", device: str = "auto") -> WhisperASRService:
    """è·å–å…¨å±€ASRæœåŠ¡å®ä¾‹"""
    global asr_service
    
    if asr_service is None:
        asr_service = WhisperASRService(model_size=model_size, device=device)
    
    return asr_service

def transcribe_video_file(video_path: str, language: str = None, 
                         subtitle_format: str = "srt", **kwargs) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šè½¬å½•è§†é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆå­—å¹•
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        language: æŒ‡å®šè¯­è¨€
        subtitle_format: å­—å¹•æ ¼å¼
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åŒ…å«è½¬å½•ç»“æœå’Œå­—å¹•æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    service = get_asr_service()
    
    # è½¬å½•è§†é¢‘
    result = service.transcribe_video(video_path, language=language, **kwargs)
    
    # ç”Ÿæˆå­—å¹•æ–‡ä»¶
    if subtitle_format and subtitle_format != "none":
        video_stem = Path(video_path).stem
        subtitle_path = f"output_data/{video_stem}_subtitles.{subtitle_format}"
        
        subtitle_file = service.generate_subtitles(
            result, 
            format=subtitle_format, 
            output_path=subtitle_path
        )
        
        result['subtitle_file'] = subtitle_file
    
    return result
