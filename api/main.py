import json
import os
import subprocess
import time
from pathlib import Path
from datetime import datetime
from urllib.request import urlretrieve
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from fastapi import FastAPI, Request, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse, FileResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, field_validator
from loguru import logger
from starlette.middleware.base import BaseHTTPMiddleware
import uuid

# Import configuration and validation functions
from core.config import (
    SEGMENT_PROMPT, CAPTIONS_PROMPT, GEMINI_API_BASE, CUT_API_BASE,
    UPLOAD_BUCKET, UPLOAD_BASE_URL, VIDEO_FPS, VIDEO_CRF, AUDIO_BITRATE,
    validate_video_extension, validate_file_size, MIN_CLIP_DURATION, MAX_CLIP_DURATION
)
from core.settings import settings
from core.smart_clipping import get_smart_segments, analyze_video_intelligence
from core.whisper_asr import get_asr_service, transcribe_video_file
from core.semantic_analysis import get_semantic_analyzer, analyze_transcription_semantics
from core.asr_smart_clipping import get_asr_smart_engine, select_segments_with_asr
from core.xiaohongshu_pipeline import (
    get_photo_ranking_service, get_storyline_generator, get_draft_generator
)
from core.subtitle_service import get_subtitle_generator, get_cover_service
from core.export_service import get_export_service
from core.advanced_photo_ranking import get_advanced_photo_service
from core.semantic_highlights import get_semantic_highlight_detector
from core.personalized_writing import get_personalized_writing_service
from core.smart_cover_design import get_smart_cover_designer
from core.audio_processing import get_audio_processing_service
from core.xiaohongshu_publisher import get_xiaohongshu_publisher
from core.image_decorator import get_image_decorator
from core.smart_cover_generator import get_smart_cover_generator
from core.llm_service import get_llm_service
from core.advanced_collage_generator import get_advanced_collage_generator
from core.xiaohongshu_collage_generator import get_xiaohongshu_collage_generator, CollageConfig, TextConfig
from routers.tasks import router as tasks_router

# Setup logging
logger.add("logs/api.log", rotation="10 MB", level="INFO")
logger.add("logs/api.jsonl", rotation="10 MB", level="INFO", serialize=True)

app = FastAPI(
    title="AI Video Clipper API",
    description="æ™ºèƒ½çŸ­è§†é¢‘è‡ªåŠ¨åˆ‡ç‰‡å’Œæ–‡æ¡ˆç”ŸæˆæœåŠ¡",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request ID middleware for tracing
class RequestIDMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        req_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        request.state.request_id = req_id
        response = await call_next(request)
        response.headers["X-Request-ID"] = req_id
        return response

app.add_middleware(RequestIDMiddleware)

# Create logs directory if it doesn't exist
Path("logs").mkdir(exist_ok=True)

# Routers
app.include_router(tasks_router)

# Mount static files and frontend
app.mount("/static", StaticFiles(directory="frontend"), name="static")
app.mount("/static/uploads", StaticFiles(directory="output_data/uploads"), name="uploads")
app.mount("/output", StaticFiles(directory="output_data"), name="output")

@app.get("/")
async def serve_frontend():
    """Serve the main frontend page"""
    return FileResponse("frontend/index.html")

# Exception handler for validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    logger.error(f"Validation error: {exc.errors()}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "message": "è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥"},
    )

# --- Pydantic Models with Validation ---
class SegmentReq(BaseModel):
    transcript: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    
    @field_validator('transcript')
    @classmethod
    def validate_transcript(cls, v):
        if not v.strip():
            raise ValueError('å­—å¹•å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()
    
    @field_validator('min_sec', 'max_sec')
    @classmethod
    def validate_duration(cls, v):
        if v <= 0:
            raise ValueError('æ—¶é•¿å¿…é¡»å¤§äº0')
        return v

class CaptionsReq(BaseModel):
    topic: str = "AI and Technology"
    transcript: str
    clip_text: str
    
    @field_validator('topic', 'transcript', 'clip_text')
    @classmethod
    def validate_text_fields(cls, v):
        if not v.strip():
            raise ValueError('æ–‡æœ¬å­—æ®µä¸èƒ½ä¸ºç©º')
        return v.strip()

class CutReq(BaseModel):
    src: str
    start: str
    end: str
    out: str
    
    @field_validator('src')
    @classmethod
    def validate_src_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æºæ–‡ä»¶ä¸å­˜åœ¨: {v}')
        if not validate_video_extension(v):
            raise ValueError(f'ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼: {v}')
        return v
    
    @field_validator('start', 'end')
    @classmethod
    def validate_time_format(cls, v):
        import re
        if not re.match(r'^\d{2}:\d{2}(:\d{2})?$', v):
            raise ValueError(f'æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º mm:ss æˆ– hh:mm:ss: {v}')
        return v

class BurnSubReq(BaseModel):
    src: str
    srt: str
    out: str
    
    @field_validator('src')
    @classmethod
    def validate_src_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æºæ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v
    
    @field_validator('srt')
    @classmethod
    def validate_srt_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v

class UploadReq(BaseModel):
    path: str
    bucket: str = UPLOAD_BUCKET
    
    @field_validator('path')
    @classmethod
    def validate_path(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v

# --- Utility Functions ---
def normalize_time(time_str: str) -> str:
    """Normalize time format to hh:mm:ss"""
    parts = time_str.split(':')
    if len(parts) == 2:
        return f"00:{time_str}"
    return time_str

def hms_to_seconds(hms: str) -> float:
    parts = list(map(int, hms.split(':')))
    if len(parts) == 2:
        return parts[0] * 60 + parts[1]
    return parts[0] * 3600 + parts[1] * 60 + parts[2]

def seconds_to_hms(t: float) -> str:
    h = int(t // 3600)
    m = int((t % 3600) // 60)
    s = int(t % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def safe_run_ffmpeg(cmd: List[str], timeout: int = 300) -> Dict[str, Any]:
    """Safely run ffmpeg command with timeout and error handling"""
    try:
        logger.info(f"Running FFmpeg command: {' '.join(cmd)}")
        start_time = time.time()
        
        process = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            timeout=timeout
        )
        
        duration = time.time() - start_time
        logger.info(f"FFmpeg completed in {duration:.2f}s with return code: {process.returncode}")
        
        if process.returncode != 0:
            logger.error(f"FFmpeg failed: {process.stderr}")
            raise HTTPException(
                status_code=500, 
                detail=f"è§†é¢‘å¤„ç†å¤±è´¥: {process.stderr}"
            )
        
        return {
            "code": process.returncode,
            "stdout": process.stdout,
            "stderr": process.stderr,
            "duration": duration
        }
        
    except subprocess.TimeoutExpired:
        logger.error("FFmpeg command timed out")
        raise HTTPException(status_code=504, detail="è§†é¢‘å¤„ç†è¶…æ—¶")
    except Exception as e:
        logger.error(f"FFmpeg execution error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘å¤„ç†å¼‚å¸¸: {str(e)}")

def black_fraction_in_segment(path: str, start_hms: str, duration_s: float) -> float:
    """Approximate fraction of time that is black in the given segment using ffmpeg blackdetect.
    Use a center ROI to ignore letterboxing borders.
    """
    try:
        # Limit runtime to short duration analysis
        cmd = [
            "ffmpeg", "-hide_banner", "-ss", start_hms, "-t", f"{max(0.5, duration_s):.2f}",
            "-i", path,
            "-vf", "crop=in_w*0.9:in_h*0.9:(in_w-out_w)/2:(in_h-out_h)/2,blackdetect=d=0.3:pic_th=0.98",
            "-an", "-f", "null", "-"
        ]
        completed = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", timeout=60)
        out = (completed.stdout or "") + "\n" + (completed.stderr or "")
        import re
        # Parse last blackdetect line: black_start:.. black_end:.. black_duration:..
        matches = re.findall(r"black_duration:([0-9]+\.?[0-9]*)", out)
        if not matches:
            return 0.0
        total_black = sum(float(x) for x in matches)
        return min(max(total_black / max(duration_s, 0.001), 0.0), 1.0)
    except Exception:
        return 0.0

def silence_fraction_in_segment(path: str, start_hms: str, duration_s: float) -> float:
    """Approximate fraction of time that is silence in the given segment using silencedetect."""
    try:
        cmd = [
            "ffmpeg", "-hide_banner", "-ss", start_hms, "-t", f"{max(0.5, duration_s):.2f}",
            "-i", path, "-af", "silencedetect=noise=-35dB:d=0.3", "-f", "null", "-"
        ]
        completed = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", timeout=60)
        out = (completed.stdout or "") + "\n" + (completed.stderr or "")
        import re
        sil_starts = [float(x) for x in re.findall(r"silence_start:([0-9]+\.?[0-9]*)", out)]
        sil_ends = [float(x) for x in re.findall(r"silence_end:([0-9]+\.?[0-9]*)", out)]
        # Pair up starts and ends; if unmatched end, ignore
        total_sil = 0.0
        for i, s in enumerate(sil_starts):
            if i < len(sil_ends):
                total_sil += max(0.0, sil_ends[i] - s)
        return min(max(total_sil / max(duration_s, 0.001), 0.0), 1.0)
    except Exception:
        return 0.0

def ffprobe_json(path: str, select_streams: str = "") -> Dict[str, Any]:
    """Return ffprobe json info for given path (optionally select streams)."""
    probe_cmd = [
        "ffprobe", "-v", "error", "-print_format", "json", "-show_streams"
    ]
    if select_streams:
        probe_cmd += ["-select_streams", select_streams]
    probe_cmd += [path]
    try:
        completed = subprocess.run(probe_cmd, capture_output=True, text=True, encoding="utf-8")
        if completed.returncode != 0:
            return {"streams": []}
        return json.loads(completed.stdout or "{}")
    except Exception:
        return {"streams": []}

def get_duration_seconds(path: str) -> float:
    info = ffprobe_json(path)
    for s in info.get("streams", []):
        if s.get("codec_type") == "video":
            try:
                return float(s.get("duration")) if s.get("duration") else 0.0
            except Exception:
                continue
    return 0.0

def try_python_asr_to_srt(src: str, out_srt: str) -> bool:
    """Try to run ASR via Python libs (openai-whisper or faster_whisper). Returns True on success."""
    # openai-whisper
    try:
        import whisper  # type: ignore
        model = whisper.load_model("small")
        result = model.transcribe(src, language="zh", task="transcribe")
        segments = result.get("segments", [])
        with open(out_srt, "w", encoding="utf-8") as f:
            for i, seg in enumerate(segments, 1):
                start = float(seg.get("start", 0.0))
                end = float(seg.get("end", 0.0))
                def fmt(t: float) -> str:
                    h = int(t // 3600); m = int((t % 3600) // 60); s = int(t % 60); ms = int((t - int(t)) * 1000)
                    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
                text = str(seg.get("text", "")).strip()
                if not text:
                    continue
                f.write(f"{i}\n{fmt(start)} --> {fmt(end)}\n{text}\n\n")
        return True
    except Exception:
        pass
    # faster-whisper
    try:
        from faster_whisper import WhisperModel  # type: ignore
        model = WhisperModel("small", device="cpu")
        segments, _ = model.transcribe(src, language="zh")
        def fmt(t: float) -> str:
            h = int(t // 3600); m = int((t % 3600) // 60); s = int(t % 60); ms = int((t - int(t)) * 1000)
            return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
        with open(out_srt, "w", encoding="utf-8") as f:
            for i, seg in enumerate(segments, 1):
                text = (seg.text or "").strip()
                if not text:
                    continue
                f.write(f"{i}\n{fmt(seg.start)} --> {fmt(seg.end)}\n{text}\n\n")
        return True
    except Exception:
        return False

def parse_srt_simple(path: str) -> List[Dict[str, str]]:
    """Parse SRT to list of dicts with start, end, text (mm:ss granularity)."""
    if not os.path.exists(path):
        return []
    raw = Path(path).read_text(encoding="utf-8", errors="ignore")
    items: List[Dict[str, str]] = []
    for block in [b.strip() for b in raw.split("\n\n") if b.strip()]:
        lines = [l.strip() for l in block.splitlines() if l.strip()]
        timeline = next((l for l in lines if "-->" in l), "")
        if not timeline:
            continue
        try:
            start, end = [t.strip().replace(",", ":")[0:8] for t in timeline.split("-->")]
        except Exception:
            continue
        txt_lines = []
        seen = False
        for l in lines:
            if seen:
                txt_lines.append(l)
            if l == timeline:
                seen = True
        text = " ".join(txt_lines).strip()
        items.append({"start": start if len(start)==8 else f"00:{start}", "end": end if len(end)==8 else f"00:{end}", "text": text})
    return items

def score_text_heuristic(t: str) -> float:
    score = 0.0
    if "ï¼" in t or "!" in t:
        score += 2
    if "ï¼Ÿ" in t or "?" in t:
        score += 1.2
    if any(ch.isdigit() for ch in t):
        score += 1.0
    for w in ["äº®ç‚¹","å…³é”®","æ€»ç»“","æ­¥éª¤","å› æ­¤","æ‰€ä»¥","æ¡ˆä¾‹","æ³¨æ„","åšæ³•","æŠ€å·§","æ­ç§˜","æå‡","å¯¹æ¯”","æ¨è"]:
        if w in t:
            score += 0.8
    score += min(len(t) / 20.0, 1.5)
    return score

def select_windows_from_srt(srt_items: List[Dict[str, str]], min_sec: int, max_sec: int, top_k: int = 3) -> List[Dict[str, str]]:
    def to_s(ts: str) -> int:
        parts = list(map(int, ts.split(":")))
        if len(parts) == 2:
            return parts[0]*60 + parts[1]
        return parts[0]*3600 + parts[1]*60 + parts[2]
    n = len(srt_items)
    windows: List[Tuple[float,int,int]] = []
    for i in range(n):
        start_s = to_s(srt_items[i]["start"]) 
        acc = 0.0
        j = i
        while j < n:
            acc += score_text_heuristic(srt_items[j]["text"]) 
            end_s = to_s(srt_items[j]["end"]) 
            dur = end_s - start_s
            if dur >= min_sec:
                windows.append((acc, i, j))
            if dur >= max_sec:
                break
            j += 1
    windows.sort(key=lambda x: x[0], reverse=True)
    chosen: List[Tuple[int,int]] = []
    for _, i, j in windows:
        if len(chosen) >= top_k:
            break
        ok = True
        for ci, cj in chosen:
            if not (j < ci or i > cj):
                ok = False
                break
        if ok:
            chosen.append((i, j))
    clips: List[Dict[str,str]] = []
    for i, j in chosen:
        clips.append({"start": srt_items[i]["start"], "end": srt_items[j]["end"], "reason": "highlight"})
    return clips

# --- Gemini Mock Function with Error Handling ---
def run_gemini(prompt: str, max_retries: int = 3) -> str:
    """Mock Gemini API call with retry logic"""
    logger.info(f"Mock Gemini call with prompt length: {len(prompt)}")
    
    for attempt in range(max_retries):
        try:
            if "åˆ‡ç‰‡ä¸“å®¶" in prompt:
                return '''{"clips": [{"start": "00:15","end": "00:48","reason": "ä»‹ç»äº†é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡å’ŒæŠ€æœ¯é€‰å‹ã€‚"},{"start": "01:10","end": "01:55","reason": "æ¼”ç¤ºäº†å…³é”®åŠŸèƒ½å¹¶è§£é‡Šäº†å…¶ç”¨æˆ·ä»·å€¼ã€‚"}]}'''
            else:
                return '''{"title": "AIå¦‚ä½•é¢ è¦†æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Ÿ","hashtags": ["#AI", "#ç§‘æŠ€æ”¹å˜ç”Ÿæ´»"],"desc": "è¿™ä¸ªè§†é¢‘çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œè®©ä½ ä¸‰åˆ†é’Ÿçœ‹æ‡‚äººå·¥æ™ºèƒ½çš„çœŸæ­£å¨åŠ›ï¼"}'''
        except Exception as e:
            logger.error(f"Gemini call attempt {attempt + 1} failed: {str(e)}")
            if attempt == max_retries - 1:
                raise HTTPException(status_code=500, detail="AIæœåŠ¡è°ƒç”¨å¤±è´¥")
            time.sleep(1)  # Wait before retry

# --- API Endpoints ---
@app.get("/")
async def root():
    """Health check endpoint"""
    return {"message": "AI Video Clipper API is running", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """è¯¦ç»†å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy", 
        "timestamp": int(time.time()),
        "services": {
            "api": "running",
            "smart_clipping": "available"
        }
    }



@app.post("/segment")
async def segment(req: SegmentReq):
    """AIæ™ºèƒ½åˆ‡ç‰‡åˆ†æ"""
    try:
        logger.info(f"Processing segment request with duration: {req.min_sec}-{req.max_sec}s")
        
        prompt = SEGMENT_PROMPT.format(
            txt=req.transcript, 
            min_sec=req.min_sec, 
            max_sec=req.max_sec
        )
        
        result = run_gemini(prompt)
        parsed_result = json.loads(result[result.find("{"):result.rfind("}")+1])
        
        logger.info(f"Generated {len(parsed_result.get('clips', []))} clips")
        return parsed_result
        
    except json.JSONDecodeError:
        logger.error("Failed to parse Gemini response as JSON")
        raise HTTPException(status_code=500, detail="AIåˆ†æç»“æœæ ¼å¼é”™è¯¯")
    except Exception as e:
        logger.error(f"Segment processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ‡ç‰‡åˆ†æå¤±è´¥: {str(e)}")

@app.post("/captions")
async def captions(req: CaptionsReq):
    """AIæ–‡æ¡ˆç”Ÿæˆ"""
    try:
        logger.info(f"Generating captions for topic: {req.topic}")
        
        prompt = CAPTIONS_PROMPT.format(
            topic=req.topic, 
            clip_text=req.clip_text, 
            transcript=req.transcript
        )
        
        result = run_gemini(prompt)
        parsed_result = json.loads(result[result.find("{"):result.rfind("}")+1])
        
        logger.info(f"Generated captions: {parsed_result.get('title', '')}")
        return parsed_result
        
    except json.JSONDecodeError:
        logger.error("Failed to parse captions response as JSON")
        raise HTTPException(status_code=500, detail="æ–‡æ¡ˆç”Ÿæˆç»“æœæ ¼å¼é”™è¯¯")
    except Exception as e:
        logger.error(f"Caption generation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/cut916")
async def cut916(req: CutReq):
    """ç”Ÿæˆ9:16ç«–å±è§†é¢‘"""
    try:
        logger.info(f"Cutting video: {req.src} ({req.start} - {req.end})")
        
        # Normalize time format
        start_time = normalize_time(req.start)
        end_time = normalize_time(req.end)
        
        # Robust 9:16 pipeline using expressions (no FOAR option):
        # - If input is wider than 9:16, scale height to 1920 and width proportional; else scale width to 1080
        # - Then center crop to exactly 1080x1920; set pixel format and SAR for compatibility
        vf_filters = (
            "scale="
            "if(gte(iw/ih\,1080/1920)\,-2\,1080):"
            "if(gte(iw/ih\,1080/1920)\,1920\,-2),"
            "crop=1080:1920,format=yuv420p,setsar=1:1"
        )

        cmd = [
            "ffmpeg", "-y",
            # Use output seeking (place -ss/-to after -i) to keep A/V sync and audio reliably
            "-i", req.src,
            "-ss", start_time,
            "-to", end_time,
            "-vf", vf_filters,
            "-r", str(VIDEO_FPS),
            "-pix_fmt", "yuv420p",
            # Explicitly map first video and (optional) first audio stream to avoid accidental drops
            "-map", "0:v:0",
            "-map", "0:a?",
            "-c:v", "libx264",
            "-preset", "veryfast",
            "-crf", str(VIDEO_CRF),
            "-c:a", "aac",
            "-b:a", AUDIO_BITRATE,
            "-movflags", "+faststart",
            req.out
        ]
        
        result = safe_run_ffmpeg(cmd)
        result["out"] = req.out
        
        logger.info(f"Successfully created 9:16 video: {req.out}")
        return result
        
    except Exception as e:
        logger.error(f"9:16 video cut error: {str(e)}")
        # Clean up partial output file if it exists
        if os.path.exists(req.out):
            os.remove(req.out)
        raise HTTPException(status_code=500, detail=f"9:16è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/burnsub")
async def burnsub(req: BurnSubReq):
    """çƒ§å½•å­—å¹•åˆ°è§†é¢‘"""
    try:
        logger.info(f"Burning subtitles: {req.srt} -> {req.src}")
        
        cmd = [
            "ffmpeg", "-y",
            "-i", req.src,
            "-vf", f"subtitles={req.srt}:force_style='Fontsize=28'",
            "-c:a", "copy",
            req.out
        ]
        
        result = safe_run_ffmpeg(cmd)
        result["out"] = req.out
        
        logger.info(f"Successfully burned subtitles: {req.out}")
        return result
        
    except Exception as e:
        logger.error(f"Subtitle burning error: {str(e)}")
        # Clean up partial output file if it exists
        if os.path.exists(req.out):
            os.remove(req.out)
        raise HTTPException(status_code=500, detail=f"å­—å¹•çƒ§å½•å¤±è´¥: {str(e)}")

@app.post("/upload")
async def upload(req: UploadReq):
    """æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ åˆ°äº‘å­˜å‚¨"""
    try:
        if not os.path.exists(req.path):
            logger.error(f"File not found: {req.path}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        file_name = os.path.basename(req.path)
        file_size = os.path.getsize(req.path)
        
        if not validate_file_size(file_size):
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°è¶…å‡ºé™åˆ¶")
        
        # Mock upload to cloud storage
        mock_url = f"{UPLOAD_BASE_URL}/{req.bucket}/{file_name}"
        logger.info(f"Mock upload: {req.path} -> {mock_url}")
        
        return {
            "url": mock_url,
            "key": file_name,
            "size": file_size,
            "bucket": req.bucket
        }
        
    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")


# --- New: Intro-style auto highlights from URL ---
class URLIntroReq(BaseModel):
    url: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    want_asr: bool = True
    top_k: int = 3
    output: str = str(Path("output_data") / "intro_916.mp4")
    smart_mode: bool = True  # å¯ç”¨æ™ºèƒ½é€‰æ®µæ¨¡å¼

class VideoAnalysisReq(BaseModel):
    url: str
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v

class ASRTranscribeReq(BaseModel):
    url: str
    language: Optional[str] = None  # è¯­è¨€ä»£ç ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹
    subtitle_format: str = "srt"    # å­—å¹•æ ¼å¼: srt, vtt, txt, json, none
    task: str = "transcribe"        # transcribe æˆ– translate
    model_size: str = "base"        # tiny, base, small, medium, large
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('subtitle_format')
    @classmethod
    def validate_subtitle_format(cls, v):
        valid_formats = ['srt', 'vtt', 'txt', 'json', 'none']
        if v not in valid_formats:
            raise ValueError(f'å­—å¹•æ ¼å¼å¿…é¡»æ˜¯: {", ".join(valid_formats)}')
        return v
    
    @field_validator('task')
    @classmethod
    def validate_task(cls, v):
        if v not in ['transcribe', 'translate']:
            raise ValueError('ä»»åŠ¡ç±»å‹å¿…é¡»æ˜¯: transcribe æˆ– translate')
        return v
    
    @field_validator('model_size')
    @classmethod
    def validate_model_size(cls, v):
        valid_sizes = ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']
        if v not in valid_sizes:
            raise ValueError(f'æ¨¡å‹å¤§å°å¿…é¡»æ˜¯: {", ".join(valid_sizes)}')
        return v

class AudioExtractionReq(BaseModel):
    url: str
    sample_rate: int = 16000
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('sample_rate')
    @classmethod
    def validate_sample_rate(cls, v):
        if v not in [8000, 16000, 22050, 44100, 48000]:
            raise ValueError('é‡‡æ ·ç‡å¿…é¡»æ˜¯: 8000, 16000, 22050, 44100, 48000')
        return v

class SemanticAnalysisReq(BaseModel):
    text: str
    include_keywords: bool = True
    include_sentiment: bool = True
    include_topics: bool = True
    include_quality: bool = True
    
    @field_validator('text')
    @classmethod
    def validate_text(cls, v):
        if not v.strip():
            raise ValueError('æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()

class ASRSmartClippingReq(BaseModel):
    url: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    count: int = 1
    model_size: str = "base"
    language: Optional[str] = None
    output_prefix: str = "asr_smart_clip"
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('min_sec', 'max_sec')
    @classmethod
    def validate_duration(cls, v):
        if v < 5 or v > 300:
            raise ValueError('æ—¶é•¿å¿…é¡»åœ¨5-300ç§’ä¹‹é—´')
        return v
    
    @field_validator('count')
    @classmethod
    def validate_count(cls, v):
        if v < 1 or v > 5:
            raise ValueError('ç‰‡æ®µæ•°é‡å¿…é¡»åœ¨1-5ä¹‹é—´')
        return v

class PhotoRankingReq(BaseModel):
    photos: List[str]
    top_k: int = 15
    
    @field_validator('photos')
    @classmethod
    def validate_photos(cls, v):
        if not v:
            raise ValueError('ç…§ç‰‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        return v
    
    @field_validator('top_k')
    @classmethod
    def validate_top_k(cls, v):
        if v < 1 or v > 50:
            raise ValueError('é€‰æ‹©æ•°é‡å¿…é¡»åœ¨1-50ä¹‹é—´')
        return v

class StorylineReq(BaseModel):
    transcript_mmss: List[Dict]
    notes: str
    city: str = ""
    date: str = ""
    style: str = "æ²»æ„ˆ"
    
    @field_validator('transcript_mmss')
    @classmethod
    def validate_transcript(cls, v):
        if not v:
            raise ValueError('è½¬å½•æ–‡æœ¬ä¸èƒ½ä¸ºç©º')
        return v
    
    @field_validator('style')
    @classmethod
    def validate_style(cls, v):
        valid_styles = ['æ²»æ„ˆ', 'ä¸“ä¸š', 'è¸©é›·']
        if v not in valid_styles:
            raise ValueError(f'é£æ ¼å¿…é¡»æ˜¯: {", ".join(valid_styles)}')
        return v

class XHSDraftReq(BaseModel):
    storyline: Dict
    brand_tone: str = "æ²»æ„ˆ"
    constraints: Optional[Dict] = None
    
    @field_validator('storyline')
    @classmethod
    def validate_storyline(cls, v):
        if not v:
            raise ValueError('æ•…äº‹çº¿æ•°æ®ä¸èƒ½ä¸ºç©º')
        return v

class SubtitleReq(BaseModel):
    clips: List[Dict]
    transcript_mmss: List[Dict]
    style: str = "å£è¯­"
    
    @field_validator('style')
    @classmethod
    def validate_style(cls, v):
        valid_styles = ['å£è¯­', 'ä¹¦é¢', 'å¯çˆ±']
        if v not in valid_styles:
            raise ValueError(f'å­—å¹•é£æ ¼å¿…é¡»æ˜¯: {", ".join(valid_styles)}')
        return v

class CoverReq(BaseModel):
    clips: List[Dict]
    photos_topk: List[Dict]
    title: str = ""

class ExportReq(BaseModel):
    pipeline_result: Dict
    export_format: str = "zip"
    include_source: bool = False
    
    @field_validator('export_format')
    @classmethod
    def validate_format(cls, v):
        valid_formats = ['json', 'zip', 'folder']
        if v not in valid_formats:
            raise ValueError(f'å¯¼å‡ºæ ¼å¼å¿…é¡»æ˜¯: {", ".join(valid_formats)}')
        return v

class XHSPipelineReq(BaseModel):
    video_url: str
    photos: List[str] = []
    notes: str = ""
    city: str = ""
    style: str = "æ²»æ„ˆ"
    model_size: str = "base"
    export_format: str = "zip"
    
    @field_validator('video_url')
    @classmethod
    def validate_video_url(cls, v):
        if not v.strip():
            raise ValueError('è§†é¢‘URLä¸èƒ½ä¸ºç©º')
        return v.strip()

# ProåŠŸèƒ½APIæ¨¡å‹
class AdvancedPhotoRankingReq(BaseModel):
    photos: List[str]
    top_k: int = 15
    context: Optional[Dict] = None
    use_clip: bool = True
    use_aesthetic_model: bool = True
    
    @field_validator('photos')
    @classmethod
    def validate_photos(cls, v):
        if not v:
            raise ValueError('ç…§ç‰‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        return v

class SemanticHighlightsReq(BaseModel):
    transcript_segments: List[Dict]
    context: Optional[Dict] = None
    min_score: float = 0.3
    max_highlights: int = 10
    
    @field_validator('transcript_segments')
    @classmethod
    def validate_segments(cls, v):
        if not v:
            raise ValueError('è½¬å½•ç‰‡æ®µä¸èƒ½ä¸ºç©º')
        return v

class PersonalizedWritingReq(BaseModel):
    user_id: str
    content_data: Dict
    style_override: Optional[str] = None
    learn_from_history: bool = True
    
    @field_validator('user_id')
    @classmethod
    def validate_user_id(cls, v):
        if not v.strip():
            raise ValueError('ç”¨æˆ·IDä¸èƒ½ä¸ºç©º')
        return v.strip()

class UserStyleLearningReq(BaseModel):
    user_id: str
    content_samples: List[Dict]
    
    @field_validator('content_samples')
    @classmethod
    def validate_samples(cls, v):
        if not v:
            raise ValueError('å†…å®¹æ ·æœ¬ä¸èƒ½ä¸ºç©º')
        return v

class SmartCoverDesignReq(BaseModel):
    clips: List[Dict]
    photos: List[Dict] = []
    title: str
    style: str = "æ²»æ„ˆ"
    template: str = "minimal"
    
    @field_validator('title')
    @classmethod
    def validate_title(cls, v):
        if not v.strip():
            raise ValueError('æ ‡é¢˜ä¸èƒ½ä¸ºç©º')
        return v.strip()

class AudioProcessingReq(BaseModel):
    video_path: str
    style: str = "æ²»æ„ˆ"
    enhance_speech: bool = True
    add_bgm: bool = True
    bgm_volume: float = 0.3
    
    @field_validator('video_path')
    @classmethod
    def validate_video_path(cls, v):
        if not v.strip():
            raise ValueError('è§†é¢‘è·¯å¾„ä¸èƒ½ä¸ºç©º')
        return v.strip()
    
    @field_validator('bgm_volume')
    @classmethod
    def validate_bgm_volume(cls, v):
        if v < 0 or v > 1:
            raise ValueError('BGMéŸ³é‡å¿…é¡»åœ¨0-1ä¹‹é—´')
        return v

class XHSProPipelineReq(BaseModel):
    video_url: str
    photos: List[str] = []
    notes: str = ""
    city: str = ""
    style: str = "æ²»æ„ˆ"
    user_id: Optional[str] = None
    model_size: str = "base"

# æ–°å¢APIæ¨¡å‹
class XHSPublishReq(BaseModel):
    title: str
    content: str
    images: List[str]
    tags: List[str]
    location: Optional[str] = None
    privacy: str = "public"

class ImageDecorateReq(BaseModel):
    image_path: str
    decorations: Dict[str, Any]

class SmartCoverReq(BaseModel):
    images: List[str]
    title: str
    subtitle: str = ""
    layout: str = "grid_3x3"
    theme: str = "pink_gradient"
    platform: str = "xiaohongshu"
    custom_config: Optional[Dict[str, Any]] = None
    export_format: str = "zip"
    
    # ProåŠŸèƒ½å¼€å…³
    use_advanced_photo_ranking: bool = True
    use_semantic_highlights: bool = True
    use_personalized_writing: bool = False
    use_smart_cover: bool = True
    use_audio_enhancement: bool = True

class LLMContentReq(BaseModel):
    theme: str
    photo_descriptions: List[str] = []
    highlights: str = ""
    feeling: str = ""
    style: str = "æ´»æ³¼"
    length: str = "medium"
    custom_requirements: str = ""
    content_type: str = "xiaohongshu"  # xiaohongshu, pro, general

class ProContentReq(BaseModel):
    topic: str
    content_type: str = "complete"  # title, description, hashtags, complete
    style: str = "professional"
    target_audience: str = "general"

class AdvancedCollageReq(BaseModel):
    images: List[str]  # å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    title: str
    layout_type: str = "dynamic"  # dynamic, grid, magazine, mosaic, creative
    style: str = "modern"  # modern, vintage, artistic, minimal
    color_scheme: str = "auto"  # auto, warm, cool, monochrome, vibrant
    canvas_size: List[int] = [800, 800]
    add_effects: bool = True
    add_text_overlay: bool = True
    extra_text: Optional[str] = ""
    text_position: str = "bottom"  # bottom | center

class XiaohongshuCollageReq(BaseModel):
    images: List[str]
    title: str
    subtitle: str = ""
    layout: str = "magazine_style"
    color_scheme: str = "xiaohongshu_pink"
    width: int = 2160
    height: int = 2160
    quality: int = 95
    custom_texts: List[Dict[str, Any]] = []
    # æ–°å¢ï¼šæ ‡é¢˜æ’ç‰ˆæ§åˆ¶
    title_position: str = "center_overlay"  # center_overlay | top
    font_path: Optional[str] = None
    overlay_texts: List[Dict[str, Any]] = []  # é¢å¤–æ–‡æ¡ˆå—ï¼Œé”šç‚¹å®šä½

class EditableTextReq(BaseModel):
    collage_id: str
    text_id: str
    new_text: str
    font_size: int = 80
    color: str = "#2C2C2C"
    position: List[int] = [100, 100]


@app.post("/analyze_video")
async def analyze_video(req: VideoAnalysisReq):
    """åˆ†æè§†é¢‘å†…å®¹ï¼Œè¿”å›æ™ºèƒ½åŒ–åˆ†æç»“æœ"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½è§†é¢‘
        ts = int(time.time())
        dl_path = str(Path("input_data/downloads") / f"analysis_{ts}.mp4")
        
        if req.url.startswith("file:"):
            # æœ¬åœ°æ–‡ä»¶
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            dl_path = local_path
        else:
            # URLä¸‹è½½ (ç®€åŒ–ç‰ˆï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ä¸‹è½½é€»è¾‘)
            from urllib.request import urlretrieve
            urlretrieve(req.url, dl_path)
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†æ
        logger.info(f"Starting intelligent video analysis for: {dl_path}")
        analysis_result = analyze_video_intelligence(dl_path)
        
        # è·å–æ™ºèƒ½ç‰‡æ®µæ¨è
        smart_segments = get_smart_segments(dl_path, 15, 30, count=5)
        
        return {
            "status": "success",
            "video_path": dl_path,
            "analysis": analysis_result,
            "recommended_segments": smart_segments,
            "message": f"æˆåŠŸåˆ†æè§†é¢‘ï¼Œå‘ç° {len(smart_segments)} ä¸ªæ¨èç‰‡æ®µ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Video analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")


@app.post("/asr/transcribe")
async def asr_transcribe(req: ASRTranscribeReq):
    """è‡ªåŠ¨è¯­éŸ³è¯†åˆ« - è½¬å½•è§†é¢‘/éŸ³é¢‘"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.url.startswith("file:"):
            # æœ¬åœ°æ–‡ä»¶
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            # URLä¸‹è½½
            input_path = str(Path("input_data/downloads") / f"asr_input_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸ¤ å¼€å§‹ASRè½¬å½•: {input_path}")
        
        # è·å–ASRæœåŠ¡
        asr_service = get_asr_service(model_size=req.model_size)
        
        # è½¬å½•è§†é¢‘
        result = asr_service.transcribe_video(
            input_path,
            language=req.language,
            task=req.task,
            cleanup_audio=True
        )
        
        # ç”Ÿæˆå­—å¹•æ–‡ä»¶
        subtitle_file = None
        if req.subtitle_format != "none":
            video_stem = Path(input_path).stem
            subtitle_path = f"output_data/{video_stem}_asr_{ts}.{req.subtitle_format}"
            
            subtitle_file = asr_service.generate_subtitles(
                result,
                format=req.subtitle_format,
                output_path=subtitle_path
            )
        
        return {
            "status": "success",
            "input_path": input_path,
            "language": result["language"],
            "language_probability": result["language_probability"],
            "duration": result["duration"],
            "full_text": result["full_text"],
            "word_count": result["word_count"],
            "segment_count": result["segment_count"],
            "processing_time": result["processing_time"],
            "subtitle_file": subtitle_file,
            "subtitle_format": req.subtitle_format if req.subtitle_format != "none" else None,
            "segments": result["segments"][:10] if len(result["segments"]) > 10 else result["segments"],  # é™åˆ¶è¿”å›çš„æ®µè½æ•°é‡
            "message": f"è½¬å½•å®Œæˆ - æ£€æµ‹è¯­è¨€: {result['language']}, æ–‡æœ¬é•¿åº¦: {result['word_count']}è¯"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ASRè½¬å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")


@app.post("/asr/extract_audio")
async def extract_audio(req: AudioExtractionReq):
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    try:
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        if req.url.startswith("file:"):
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            # URLä¸‹è½½
            ts = int(time.time())
            input_path = str(Path("input_data/downloads") / f"audio_extract_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸµ å¼€å§‹æå–éŸ³é¢‘: {input_path}")
        
        # è·å–ASRæœåŠ¡
        asr_service = get_asr_service()
        
        # æå–éŸ³é¢‘
        video_stem = Path(input_path).stem
        audio_path = f"output_data/{video_stem}_audio_{int(time.time())}.wav"
        
        extracted_audio = asr_service.extract_audio_from_video(
            input_path,
            audio_path=audio_path,
            sample_rate=req.sample_rate
        )
        
        # è·å–éŸ³é¢‘ä¿¡æ¯
        audio_info = {}
        try:
            import subprocess
            result = subprocess.run([
                "ffprobe", "-v", "quiet", "-print_format", "json", "-show_format",
                "-show_streams", extracted_audio
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                import json
                probe_data = json.loads(result.stdout)
                if 'format' in probe_data:
                    audio_info = {
                        'duration': float(probe_data['format'].get('duration', 0)),
                        'size': int(probe_data['format'].get('size', 0)),
                        'bit_rate': probe_data['format'].get('bit_rate'),
                    }
                if 'streams' in probe_data and probe_data['streams']:
                    stream = probe_data['streams'][0]
                    audio_info.update({
                        'sample_rate': stream.get('sample_rate'),
                        'channels': stream.get('channels'),
                        'codec': stream.get('codec_name')
                    })
        except Exception as e:
            logger.warning(f"è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            "status": "success",
            "input_path": input_path,
            "audio_path": extracted_audio,
            "sample_rate": req.sample_rate,
            "audio_info": audio_info,
            "message": f"éŸ³é¢‘æå–å®Œæˆ: {extracted_audio}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘æå–å¤±è´¥: {str(e)}")


@app.get("/asr/info")
async def asr_info():
    """è·å–ASRæœåŠ¡ä¿¡æ¯"""
    try:
        asr_service = get_asr_service()
        info = asr_service.get_model_info()
        
        return {
            "status": "success",
            "asr_info": info,
            "available_models": ["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"],
            "supported_languages": info["supported_languages"],
            "subtitle_formats": info["subtitle_formats"],
            "message": "ASRæœåŠ¡ä¿¡æ¯è·å–æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–ASRä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ASRä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/semantic/analyze")
async def semantic_analyze(req: SemanticAnalysisReq):
    """è¯­ä¹‰åˆ†æ - åˆ†ææ–‡æœ¬çš„å…³é”®è¯ã€æƒ…æ„Ÿã€ä¸»é¢˜ç­‰"""
    try:
        logger.info(f"å¼€å§‹è¯­ä¹‰åˆ†æï¼Œæ–‡æœ¬é•¿åº¦: {len(req.text)}")
        
        analyzer = get_semantic_analyzer()
        result = {}
        
        # å…³é”®è¯æå–
        if req.include_keywords:
            result['keywords'] = analyzer.extract_keywords(req.text, top_k=10)
        
        # æƒ…æ„Ÿåˆ†æ
        if req.include_sentiment:
            result['sentiment'] = analyzer.analyze_sentiment(req.text)
        
        # ä¸»é¢˜ç›¸å…³æ€§
        if req.include_topics:
            result['topic_relevance'] = analyzer.analyze_topic_relevance(req.text)
        
        # å†…å®¹è´¨é‡è¯„åˆ†
        if req.include_quality:
            result['quality_score'] = analyzer.calculate_content_quality_score(req.text)
        
        return {
            "status": "success",
            "text_length": len(req.text),
            "word_count": len(req.text.split()),
            "analysis": result,
            "message": "è¯­ä¹‰åˆ†æå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­ä¹‰åˆ†æå¤±è´¥: {str(e)}")


@app.post("/smart_clipping/asr_enhanced")
async def asr_enhanced_smart_clipping(req: ASRSmartClippingReq):
    """ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡ - ç»“åˆè¯­éŸ³è¯†åˆ«å’Œè¯­ä¹‰åˆ†æçš„æ™ºèƒ½é€‰æ®µ"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.url.startswith("file:"):
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"asr_smart_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸ¯ å¼€å§‹ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡: {input_path}")
        
        # 1. è·å–ASRè½¬å½•ç»“æœ
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(
            input_path,
            language=req.language,
            cleanup_audio=True
        )
        
        logger.info(f"ASRè½¬å½•å®Œæˆ - è¯­è¨€: {transcription_result['language']}, æ–‡æœ¬é•¿åº¦: {transcription_result['word_count']}è¯")
        
        # 2. æ‰§è¡ŒASRå¢å¼ºæ™ºèƒ½é€‰æ®µ
        asr_engine = get_asr_smart_engine()
        selected_segments = asr_engine.select_best_segments_with_asr(
            input_path,
            transcription_result,
            req.min_sec,
            req.max_sec,
            req.count
        )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ™ºèƒ½ç‰‡æ®µ")
        
        # 3. ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        
        for i, segment in enumerate(selected_segments):
            output_filename = f"{req.output_prefix}_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ä½¿ç”¨pad-firstç­–ç•¥ç”Ÿæˆ9:16è§†é¢‘
            fade_out_start = max(0.1, duration - 0.25)
            vf_filters = (
                "scale=1080:1920:force_original_aspect_ratio=decrease,"
                "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
            )
            
            cmd = [
                "ffmpeg", "-y", "-hwaccel", "none",
                "-i", input_path,
                "-ss", start_time, "-t", f"{duration:.2f}",
                "-vf", vf_filters,
                "-pix_fmt", "yuv420p",
                "-map", "0:v:0", "-map", "0:a?",
                "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                "-shortest", "-movflags", "+faststart",
                output_path
            ]
            
            safe_run_ffmpeg(cmd)
            
            # è·å–ç”Ÿæˆçš„è§†é¢‘ä¿¡æ¯
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "duration": duration,
                "file_size": file_size,
                "visual_score": segment['visual_score'],
                "content_score": segment['content_score'],
                "total_score": segment['total_score'],
                "selection_type": segment['type']
            })
        
        # 4. æ„å»ºå“åº”
        return {
            "status": "success",
            "input_path": input_path,
            "transcription": {
                "language": transcription_result["language"],
                "language_probability": transcription_result["language_probability"],
                "duration": transcription_result["duration"],
                "word_count": transcription_result["word_count"],
                "segment_count": transcription_result["segment_count"]
            },
            "selected_segments": len(selected_segments),
            "generated_clips": generated_clips,
            "processing_summary": {
                "asr_enhanced": True,
                "semantic_analysis": True,
                "visual_analysis": True,
                "selection_algorithm": "asr_smart_clipping"
            },
            "message": f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å®Œæˆ - ç”Ÿæˆäº† {len(generated_clips)} ä¸ªé«˜è´¨é‡ç‰‡æ®µ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å¤±è´¥: {str(e)}")


@app.post("/pro/photo_rank_advanced")
async def advanced_photo_rank(req: AdvancedPhotoRankingReq):
    """é«˜çº§ç…§ç‰‡é€‰ä¼˜æ’åºï¼ˆProç‰ˆæœ¬ï¼‰"""
    try:
        logger.info(f"å¼€å§‹é«˜çº§ç…§ç‰‡é€‰ä¼˜ï¼Œå…± {len(req.photos)} å¼ ç…§ç‰‡")
        
        advanced_service = get_advanced_photo_service()
        ranked_photos = advanced_service.rank_photos_advanced(
            req.photos, req.top_k, req.context
        )
        
        return {
            "status": "success",
            "input_count": len(req.photos),
            "output_count": len(ranked_photos),
            "ranked_photos": ranked_photos,
            "features_used": {
                "clip_analysis": req.use_clip,
                "aesthetic_model": req.use_aesthetic_model,
                "duplicate_detection": True,
                "subject_consistency": True
            },
            "message": f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å®Œæˆï¼Œè¿”å›å‰ {len(ranked_photos)} å¼ "
        }
        
    except Exception as e:
        logger.error(f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {str(e)}")


@app.post("/pro/semantic_highlights")
async def detect_semantic_highlights(req: SemanticHighlightsReq):
    """è¯­ä¹‰é«˜å…‰æ£€æµ‹"""
    try:
        logger.info(f"å¼€å§‹è¯­ä¹‰é«˜å…‰æ£€æµ‹ï¼Œå…± {len(req.transcript_segments)} ä¸ªç‰‡æ®µ")
        
        detector = get_semantic_highlight_detector()
        highlights = detector.detect_highlights(req.transcript_segments, req.context)
        
        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„é«˜å…‰
        filtered_highlights = [
            h for h in highlights 
            if h.get('highlight_score', 0) >= req.min_score
        ][:req.max_highlights]
        
        return {
            "status": "success",
            "total_segments": len(req.transcript_segments),
            "highlights_found": len(highlights),
            "highlights_returned": len(filtered_highlights),
            "highlights": filtered_highlights,
            "analysis_summary": {
                "avg_score": np.mean([h.get('highlight_score', 0) for h in filtered_highlights]) if filtered_highlights else 0,
                "highlight_types": list(set(h.get('highlight_type', 'unknown') for h in filtered_highlights)),
                "min_score_threshold": req.min_score
            },
            "message": f"æ£€æµ‹åˆ° {len(filtered_highlights)} ä¸ªé«˜å…‰æ—¶åˆ»"
        }
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰é«˜å…‰æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­ä¹‰é«˜å…‰æ£€æµ‹å¤±è´¥: {str(e)}")


@app.post("/pro/user_style_learning")
async def learn_user_style(req: UserStyleLearningReq):
    """å­¦ä¹ ç”¨æˆ·å†™ä½œé£æ ¼"""
    try:
        logger.info(f"å¼€å§‹å­¦ä¹ ç”¨æˆ· {req.user_id} çš„å†™ä½œé£æ ¼")
        
        writing_service = get_personalized_writing_service()
        user_profile = writing_service.learn_user_style(req.user_id, req.content_samples)
        
        return {
            "status": "success",
            "user_id": req.user_id,
            "samples_analyzed": len(req.content_samples),
            "confidence_score": user_profile.get('confidence_score', 0),
            "learned_features": {
                "vocabulary_patterns": len(user_profile.get('vocabulary', {}).get('signature_words', [])),
                "emoji_preferences": len(user_profile.get('emoji', {}).get('favorite_emojis', [])),
                "tone_analysis": user_profile.get('tone', {}).get('primary_tone', 'unknown'),
                "topic_preferences": len(user_profile.get('topics', {}).get('preferred_topics', []))
            },
            "message": f"ç”¨æˆ·é£æ ¼å­¦ä¹ å®Œæˆï¼Œç½®ä¿¡åº¦: {user_profile.get('confidence_score', 0):.2f}"
        }
        
    except Exception as e:
        logger.error(f"ç”¨æˆ·é£æ ¼å­¦ä¹ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”¨æˆ·é£æ ¼å­¦ä¹ å¤±è´¥: {str(e)}")


@app.post("/pro/personalized_writing")
async def generate_personalized_content(req: PersonalizedWritingReq):
    """ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹"""
    try:
        logger.info(f"å¼€å§‹ä¸ºç”¨æˆ· {req.user_id} ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹")
        
        writing_service = get_personalized_writing_service()
        personalized_content = writing_service.generate_personalized_content(
            req.user_id, req.content_data, req.style_override
        )
        
        return {
            "status": "success",
            "user_id": req.user_id,
            "personalized_content": personalized_content,
            "personalization_confidence": personalized_content.get('personalization_confidence', 0),
            "features_applied": personalized_content.get('metadata', {}).get('personalization_features', []),
            "message": "ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/pro/smart_cover")
async def generate_smart_cover(req: SmartCoverDesignReq):
    """æ™ºèƒ½å°é¢è®¾è®¡"""
    try:
        logger.info(f"å¼€å§‹æ™ºèƒ½å°é¢è®¾è®¡ - æ ‡é¢˜: {req.title[:20]}...")
        
        cover_designer = get_smart_cover_designer()
        cover_result = cover_designer.generate_smart_cover(
            req.clips, req.photos, req.title, req.style
        )
        
        return {
            "status": "success",
            "cover_result": cover_result,
            "design_features": {
                "frame_analysis": bool(cover_result.get('source_frame')),
                "color_extraction": bool(cover_result.get('color_palette')),
                "text_layout": bool(cover_result.get('text_layout')),
                "smart_positioning": True
            },
            "message": "æ™ºèƒ½å°é¢è®¾è®¡å®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½å°é¢è®¾è®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½å°é¢è®¾è®¡å¤±è´¥: {str(e)}")


@app.post("/pro/audio_processing")
async def process_video_audio(req: AudioProcessingReq):
    """éŸ³é¢‘å¤„ç†ï¼ˆé™å™ªã€BGMåŒ¹é…ï¼‰"""
    try:
        logger.info(f"å¼€å§‹éŸ³é¢‘å¤„ç† - é£æ ¼: {req.style}")
        
        audio_service = get_audio_processing_service()
        processing_result = audio_service.process_video_audio(
            req.video_path, req.style, req.enhance_speech, req.add_bgm
        )
        
        return {
            "status": "success",
            "processing_result": processing_result,
            "audio_enhancements": {
                "noise_reduction": req.enhance_speech,
                "bgm_added": req.add_bgm,
                "beat_alignment": processing_result.get('bgm_info', {}).get('tempo', 0) > 0,
                "final_optimization": processing_result.get('processing_steps', {}).get('final_optimization', False)
            },
            "message": "éŸ³é¢‘å¤„ç†å®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/photo_rank")
async def photo_rank(req: PhotoRankingReq):
    """ç…§ç‰‡é€‰ä¼˜æ’åº"""
    try:
        logger.info(f"å¼€å§‹ç…§ç‰‡é€‰ä¼˜ï¼Œå…± {len(req.photos)} å¼ ç…§ç‰‡")
        
        photo_service = get_photo_ranking_service()
        ranked_photos = photo_service.rank_photos(req.photos, req.top_k)
        
        return {
            "status": "success",
            "input_count": len(req.photos),
            "output_count": len(ranked_photos),
            "ranked_photos": ranked_photos,
            "message": f"ç…§ç‰‡é€‰ä¼˜å®Œæˆï¼Œè¿”å›å‰ {len(ranked_photos)} å¼ "
        }
        
    except Exception as e:
        logger.error(f"ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/storyline")
async def generate_storyline(req: StorylineReq):
    """ç”Ÿæˆæ—…è¡Œæ•…äº‹çº¿"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆæ•…äº‹çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}")
        
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            req.transcript_mmss, req.notes, req.city, req.date, req.style
        )
        
        return {
            "status": "success",
            "storyline": storyline,
            "section_count": len(storyline.get('sections', [])),
            "tip_count": len(storyline.get('tips', [])),
            "poi_count": len(storyline.get('pois', [])),
            "message": "æ•…äº‹çº¿ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ•…äº‹çº¿ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ•…äº‹çº¿ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/draft")
async def generate_xhs_draft(req: XHSDraftReq):
    """ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ - è°ƒæ€§: {req.brand_tone}")
        
        draft_gen = get_draft_generator()
        draft = draft_gen.generate_draft(req.storyline, req.brand_tone, req.constraints)
        
        return {
            "status": "success",
            "draft": draft,
            "metadata": draft.get('metadata', {}),
            "message": "å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/subtitles")
async def generate_subtitles(req: SubtitleReq):
    """ç”Ÿæˆå­—å¹•æ–‡ä»¶"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå­—å¹• - é£æ ¼: {req.style}")
        
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(req.clips, req.transcript_mmss, req.style)
        
        return {
            "status": "success",
            "subtitles": subtitles,
            "message": "å­—å¹•ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å­—å¹•ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å­—å¹•ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/cover")
async def suggest_cover(req: CoverReq):
    """ç”Ÿæˆå°é¢å»ºè®®"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå°é¢å»ºè®® - æ ‡é¢˜: {req.title[:20]}...")
        
        cover_service = get_cover_service()
        cover_suggestions = cover_service.suggest_cover(req.clips, req.photos_topk, req.title)
        
        return {
            "status": "success",
            "cover_suggestions": cover_suggestions,
            "message": "å°é¢å»ºè®®ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å°é¢å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°é¢å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/export")
async def export_content(req: ExportReq):
    """å¯¼å‡ºå°çº¢ä¹¦å†…å®¹äº§ç‰©"""
    try:
        logger.info(f"å¼€å§‹å¯¼å‡ºå†…å®¹ - æ ¼å¼: {req.export_format}")
        
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            req.pipeline_result, req.export_format, req.include_source
        )
        
        return {
            "status": "success",
            "export_result": export_result,
            "message": "å†…å®¹å¯¼å‡ºå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å†…å®¹å¯¼å‡ºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å†…å®¹å¯¼å‡ºå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/pipeline")
async def xiaohongshu_pipeline(req: XHSPipelineReq):
    """å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿å®Œæ•´æµæ°´çº¿"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ¬ å¼€å§‹å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿æµæ°´çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}")
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.video_url.startswith("file:"):
            local_path = req.video_url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"xhs_pipeline_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.video_url, input_path)
        
        pipeline_result = {
            'source_video': input_path,
            'processing_id': f'xhs_{ts}',
            'created_at': datetime.now().isoformat()
        }
        
        # 1. ASRè¯­éŸ³è¯†åˆ«
        logger.info("æ­¥éª¤1: ASRè¯­éŸ³è¯†åˆ«...")
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(input_path, cleanup_audio=True)
        
        # è½¬æ¢ä¸ºå¸¦æ—¶é—´æˆ³æ ¼å¼
        transcript_mmss = []
        for segment in transcription_result.get('segments', []):
            transcript_mmss.append({
                'start': segment.get('start', 0),
                'end': segment.get('end', 0),
                'text': segment.get('text', ''),
                'timestamp': f"{int(segment.get('start', 0)//60):02d}:{int(segment.get('start', 0)%60):02d}"
            })
        
        pipeline_result['transcription'] = transcription_result
        pipeline_result['transcript_mmss'] = transcript_mmss
        
        # 2. æ™ºèƒ½é€‰æ®µ
        logger.info("æ­¥éª¤2: ASRå¢å¼ºæ™ºèƒ½é€‰æ®µ...")
        asr_engine = get_asr_smart_engine()
        selected_segments = asr_engine.select_best_segments_with_asr(
            input_path, transcription_result, 15, 30, 2  # ç”Ÿæˆ2ä¸ª15-30ç§’ç‰‡æ®µ
        )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘ç‰‡æ®µ")
        
        # ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        for i, segment in enumerate(selected_segments):
            output_filename = f"xhs_clip_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ä½¿ç”¨pad-firstç­–ç•¥ç”Ÿæˆ9:16è§†é¢‘
            fade_out_start = max(0.1, duration - 0.25)
            vf_filters = (
                "scale=1080:1920:force_original_aspect_ratio=decrease,"
                "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
            )
            
            cmd = [
                "ffmpeg", "-y", "-hwaccel", "none",
                "-i", input_path,
                "-ss", start_time, "-t", f"{duration:.2f}",
                "-vf", vf_filters,
                "-pix_fmt", "yuv420p",
                "-map", "0:v:0", "-map", "0:a?",
                "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                "-shortest", "-movflags", "+faststart",
                output_path
            ]
            
            safe_run_ffmpeg(cmd)
            
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "start_time_seconds": segment['start_time'],
                "end_time_seconds": segment['end_time'],
                "duration": duration,
                "file_size": file_size
            })
        
        pipeline_result['clips'] = generated_clips
        
        # 3. ç…§ç‰‡é€‰ä¼˜ï¼ˆå¦‚æœæœ‰ç…§ç‰‡ï¼‰
        if req.photos:
            logger.info("æ­¥éª¤3: ç…§ç‰‡é€‰ä¼˜...")
            photo_service = get_photo_ranking_service()
            ranked_photos = photo_service.rank_photos(req.photos, 10)
            pipeline_result['photos_ranked'] = ranked_photos
        else:
            pipeline_result['photos_ranked'] = []
        
        # 4. æ•…äº‹çº¿ç”Ÿæˆ
        logger.info("æ­¥éª¤4: ç”Ÿæˆæ•…äº‹çº¿...")
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            transcript_mmss, req.notes, req.city, "", req.style
        )
        pipeline_result['storyline'] = storyline
        
        # 5. å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆ
        logger.info("æ­¥éª¤5: ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ...")
        draft_gen = get_draft_generator()
        draft = draft_gen.generate_draft(storyline, req.style)
        pipeline_result['draft'] = draft
        
        # 6. å­—å¹•ç”Ÿæˆ
        logger.info("æ­¥éª¤6: ç”Ÿæˆå­—å¹•...")
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(generated_clips, transcript_mmss, "å¯çˆ±")
        pipeline_result['subtitles'] = subtitles
        
        # 7. å°é¢å»ºè®®
        logger.info("æ­¥éª¤7: ç”Ÿæˆå°é¢å»ºè®®...")
        cover_service = get_cover_service()
        cover_suggestions = cover_service.suggest_cover(
            generated_clips, pipeline_result['photos_ranked'], draft['title']
        )
        pipeline_result['cover'] = cover_suggestions
        
        # 8. å¯¼å‡ºäº§ç‰©
        logger.info("æ­¥éª¤8: å¯¼å‡ºå†…å®¹äº§ç‰©...")
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            pipeline_result, req.export_format, False
        )
        
        return {
            "status": "success",
            "pipeline_result": {
                'processing_id': pipeline_result['processing_id'],
                'transcription_summary': {
                    'language': transcription_result['language'],
                    'duration': transcription_result['duration'],
                    'word_count': transcription_result['word_count']
                },
                'clips_generated': len(generated_clips),
                'photos_ranked': len(pipeline_result['photos_ranked']),
                'storyline_sections': len(storyline.get('sections', [])),
                'draft_info': {
                    'title': draft['title'],
                    'hashtag_count': len(draft.get('hashtags', [])),
                    'word_count': draft.get('metadata', {}).get('word_count', 0)
                },
                'subtitle_files': len(subtitles.get('srt_files', [])),
                'export_info': export_result
            },
            "download_links": export_result.get('share_urls', {}),
            "message": f"ğŸ‰ å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿å®Œæˆï¼ç”Ÿæˆäº† {len(generated_clips)} ä¸ªè§†é¢‘ç‰‡æ®µå’Œå®Œæ•´æ–‡æ¡ˆ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦æµæ°´çº¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°çº¢ä¹¦æµæ°´çº¿å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/pipeline_pro")
async def xiaohongshu_pipeline_pro(req: XHSProPipelineReq):
    """å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿Proç‰ˆæµæ°´çº¿ï¼ˆåŒ…å«æ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼‰"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸš€ å¼€å§‹å°çº¢ä¹¦Proæµæ°´çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}, ç”¨æˆ·: {req.user_id or 'anonymous'}")
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.video_url.startswith("file:"):
            local_path = req.video_url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"xhs_pro_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.video_url, input_path)
        
        pipeline_result = {
            'source_video': input_path,
            'processing_id': f'xhs_pro_{ts}',
            'created_at': datetime.now().isoformat(),
            'pro_features_enabled': {
                'advanced_photo_ranking': req.use_advanced_photo_ranking,
                'semantic_highlights': req.use_semantic_highlights,
                'personalized_writing': req.use_personalized_writing,
                'smart_cover': req.use_smart_cover,
                'audio_enhancement': req.use_audio_enhancement
            }
        }
        
        # 1. ASRè¯­éŸ³è¯†åˆ«
        logger.info("æ­¥éª¤1: ASRè¯­éŸ³è¯†åˆ«...")
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(input_path, cleanup_audio=True)
        
        # è½¬æ¢ä¸ºå¸¦æ—¶é—´æˆ³æ ¼å¼
        transcript_mmss = []
        for segment in transcription_result.get('segments', []):
            transcript_mmss.append({
                'start': segment.get('start', 0),
                'end': segment.get('end', 0),
                'text': segment.get('text', ''),
                'timestamp': f"{int(segment.get('start', 0)//60):02d}:{int(segment.get('start', 0)%60):02d}"
            })
        
        pipeline_result['transcription'] = transcription_result
        pipeline_result['transcript_mmss'] = transcript_mmss
        
        # 2. ProåŠŸèƒ½ï¼šè¯­ä¹‰é«˜å…‰æ£€æµ‹
        if req.use_semantic_highlights:
            logger.info("æ­¥éª¤2Pro: è¯­ä¹‰é«˜å…‰æ£€æµ‹...")
            detector = get_semantic_highlight_detector()
            semantic_highlights = detector.detect_highlights(
                transcription_result.get('segments', []), 
                {'city': req.city, 'style': req.style}
            )
            pipeline_result['semantic_highlights'] = semantic_highlights
            logger.info(f"æ£€æµ‹åˆ° {len(semantic_highlights)} ä¸ªè¯­ä¹‰é«˜å…‰æ—¶åˆ»")
        
        # 3. æ™ºèƒ½é€‰æ®µï¼ˆç»“åˆè¯­ä¹‰é«˜å…‰ï¼‰
        logger.info("æ­¥éª¤3: ASRå¢å¼ºæ™ºèƒ½é€‰æ®µ...")
        asr_engine = get_asr_smart_engine()
        
        # å¦‚æœæœ‰è¯­ä¹‰é«˜å…‰ï¼Œä¼˜å…ˆé€‰æ‹©é«˜å…‰ç‰‡æ®µ
        if req.use_semantic_highlights and pipeline_result.get('semantic_highlights'):
            # åŸºäºè¯­ä¹‰é«˜å…‰é€‰æ‹©ç‰‡æ®µ
            highlight_segments = []
            for highlight in pipeline_result['semantic_highlights'][:3]:  # å–å‰3ä¸ªé«˜å…‰
                highlight_segments.append({
                    'start_time': highlight.get('start', 0),
                    'end_time': highlight.get('end', 0),
                    'duration': highlight.get('duration', 15),
                    'start_hms': f"{int(highlight.get('start', 0)//3600):02d}:{int((highlight.get('start', 0)%3600)//60):02d}:{int(highlight.get('start', 0)%60):02d}",
                    'reason': f"è¯­ä¹‰é«˜å…‰: {highlight.get('highlight_reason', 'ç²¾å½©å†…å®¹')}",
                    'highlight_score': highlight.get('highlight_score', 0.8)
                })
            selected_segments = highlight_segments
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ™ºèƒ½é€‰æ®µ
            selected_segments = asr_engine.select_best_segments_with_asr(
                input_path, transcription_result, 15, 30, 2
            )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘ç‰‡æ®µ")
        
        # ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        for i, segment in enumerate(selected_segments):
            output_filename = f"xhs_pro_clip_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ProåŠŸèƒ½ï¼šéŸ³é¢‘å¢å¼º
            if req.use_audio_enhancement:
                # å…ˆç”ŸæˆåŸºç¡€è§†é¢‘
                fade_out_start = max(0.1, duration - 0.25)
                vf_filters = (
                    "scale=1080:1920:force_original_aspect_ratio=decrease,"
                    "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                    f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
                )
                
                temp_video_path = f"output_data/temp_{output_filename}"
                
                cmd = [
                    "ffmpeg", "-y", "-hwaccel", "none",
                    "-i", input_path,
                    "-ss", start_time, "-t", f"{duration:.2f}",
                    "-vf", vf_filters,
                    "-pix_fmt", "yuv420p",
                    "-map", "0:v:0", "-map", "0:a?",
                    "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                    "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                    "-shortest", "-movflags", "+faststart",
                    temp_video_path
                ]
                
                safe_run_ffmpeg(cmd)
                
                # éŸ³é¢‘å¢å¼ºå¤„ç†
                try:
                    audio_service = get_audio_processing_service()
                    audio_result = audio_service.process_video_audio(
                        temp_video_path, req.style, True, True
                    )
                    
                    if audio_result.get('success') and audio_result.get('processed_video'):
                        # ä½¿ç”¨å¢å¼ºåçš„è§†é¢‘
                        Path(temp_video_path).unlink(missing_ok=True)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        Path(audio_result['processed_video']).rename(output_path)
                    else:
                        # éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸè§†é¢‘
                        Path(temp_video_path).rename(output_path)
                        
                except Exception as audio_error:
                    logger.warning(f"éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸéŸ³é¢‘: {audio_error}")
                    Path(temp_video_path).rename(output_path)
            else:
                # æ ‡å‡†è§†é¢‘ç”Ÿæˆ
                fade_out_start = max(0.1, duration - 0.25)
                vf_filters = (
                    "scale=1080:1920:force_original_aspect_ratio=decrease,"
                    "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                    f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
                )
                
                cmd = [
                    "ffmpeg", "-y", "-hwaccel", "none",
                    "-i", input_path,
                    "-ss", start_time, "-t", f"{duration:.2f}",
                    "-vf", vf_filters,
                    "-pix_fmt", "yuv420p",
                    "-map", "0:v:0", "-map", "0:a?",
                    "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                    "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                    "-shortest", "-movflags", "+faststart",
                    output_path
                ]
                
                safe_run_ffmpeg(cmd)
            
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "start_time_seconds": segment['start_time'],
                "end_time_seconds": segment['end_time'],
                "duration": duration,
                "file_size": file_size,
                "selection_reason": segment.get('reason', 'ASRæ™ºèƒ½é€‰æ®µ'),
                "highlight_score": segment.get('highlight_score', 0.7)
            })
        
        pipeline_result['clips'] = generated_clips
        
        # 4. ProåŠŸèƒ½ï¼šé«˜çº§ç…§ç‰‡é€‰ä¼˜
        if req.photos and req.use_advanced_photo_ranking:
            logger.info("æ­¥éª¤4Pro: é«˜çº§ç…§ç‰‡é€‰ä¼˜...")
            advanced_photo_service = get_advanced_photo_service()
            ranked_photos = advanced_photo_service.rank_photos_advanced(
                req.photos, 10, {'city': req.city, 'style': req.style}
            )
            pipeline_result['photos_ranked'] = ranked_photos
        elif req.photos:
            # ä½¿ç”¨åŸºç¡€ç…§ç‰‡é€‰ä¼˜
            logger.info("æ­¥éª¤4: åŸºç¡€ç…§ç‰‡é€‰ä¼˜...")
            photo_service = get_photo_ranking_service()
            ranked_photos = photo_service.rank_photos(req.photos, 10)
            pipeline_result['photos_ranked'] = ranked_photos
        else:
            pipeline_result['photos_ranked'] = []
        
        # 5. æ•…äº‹çº¿ç”Ÿæˆ
        logger.info("æ­¥éª¤5: ç”Ÿæˆæ•…äº‹çº¿...")
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            transcript_mmss, req.notes, req.city, "", req.style
        )
        pipeline_result['storyline'] = storyline
        
        # 6. ProåŠŸèƒ½ï¼šä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ
        if req.use_personalized_writing and req.user_id:
            logger.info("æ­¥éª¤6Pro: ä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ...")
            writing_service = get_personalized_writing_service()
            draft = writing_service.generate_personalized_content(
                req.user_id, 
                {'storyline': storyline, 'city': req.city, 'style': req.style},
                req.style
            )
            pipeline_result['draft'] = draft
        else:
            # ä½¿ç”¨æ ‡å‡†æ–‡æ¡ˆç”Ÿæˆ
            logger.info("æ­¥éª¤6: æ ‡å‡†æ–‡æ¡ˆç”Ÿæˆ...")
            draft_gen = get_draft_generator()
            draft = draft_gen.generate_draft(storyline, req.style)
            pipeline_result['draft'] = draft
        
        # 7. å­—å¹•ç”Ÿæˆ
        logger.info("æ­¥éª¤7: ç”Ÿæˆå­—å¹•...")
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(generated_clips, transcript_mmss, "å¯çˆ±")
        pipeline_result['subtitles'] = subtitles
        
        # 8. ProåŠŸèƒ½ï¼šæ™ºèƒ½å°é¢è®¾è®¡
        if req.use_smart_cover:
            logger.info("æ­¥éª¤8Pro: æ™ºèƒ½å°é¢è®¾è®¡...")
            cover_designer = get_smart_cover_designer()
            cover_suggestions = cover_designer.generate_smart_cover(
                generated_clips, pipeline_result['photos_ranked'], 
                pipeline_result['draft']['title'], req.style
            )
            pipeline_result['cover'] = cover_suggestions
        else:
            # ä½¿ç”¨åŸºç¡€å°é¢å»ºè®®
            logger.info("æ­¥éª¤8: åŸºç¡€å°é¢å»ºè®®...")
            cover_service = get_cover_service()
            cover_suggestions = cover_service.suggest_cover(
                generated_clips, pipeline_result['photos_ranked'], 
                pipeline_result['draft']['title']
            )
            pipeline_result['cover'] = cover_suggestions
        
        # 9. å¯¼å‡ºäº§ç‰©
        logger.info("æ­¥éª¤9: å¯¼å‡ºå†…å®¹äº§ç‰©...")
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            pipeline_result, req.export_format, False
        )
        
        # ç»Ÿè®¡ProåŠŸèƒ½ä½¿ç”¨æƒ…å†µ
        pro_features_used = []
        if req.use_advanced_photo_ranking and req.photos:
            pro_features_used.append("é«˜çº§ç…§ç‰‡é€‰ä¼˜")
        if req.use_semantic_highlights:
            pro_features_used.append("è¯­ä¹‰é«˜å…‰æ£€æµ‹")
        if req.use_personalized_writing and req.user_id:
            pro_features_used.append("ä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ")
        if req.use_smart_cover:
            pro_features_used.append("æ™ºèƒ½å°é¢è®¾è®¡")
        if req.use_audio_enhancement:
            pro_features_used.append("éŸ³é¢‘å¢å¼ºå¤„ç†")
        
        return {
            "status": "success",
            "pipeline_result": {
                'processing_id': pipeline_result['processing_id'],
                'pro_features_used': pro_features_used,
                'transcription_summary': {
                    'language': transcription_result['language'],
                    'duration': transcription_result['duration'],
                    'word_count': transcription_result['word_count']
                },
                'semantic_highlights_count': len(pipeline_result.get('semantic_highlights', [])),
                'clips_generated': len(generated_clips),
                'photos_ranked': len(pipeline_result['photos_ranked']),
                'storyline_sections': len(storyline.get('sections', [])),
                'draft_info': {
                    'title': pipeline_result['draft']['title'],
                    'hashtag_count': len(pipeline_result['draft'].get('hashtags', [])),
                    'word_count': pipeline_result['draft'].get('metadata', {}).get('word_count', 0),
                    'personalization_confidence': pipeline_result['draft'].get('personalization_confidence', 0)
                },
                'subtitle_files': len(subtitles.get('srt_files', [])),
                'cover_design': {
                    'success': pipeline_result['cover'].get('success', False),
                    'cover_path': pipeline_result['cover'].get('cover_path', ''),
                    'design_features': len(pro_features_used)
                },
                'export_info': export_result
            },
            "download_links": export_result.get('share_urls', {}),
            "message": f"ğŸ‰ å°çº¢ä¹¦Proä¸€é”®å‡ºç¨¿å®Œæˆï¼ç”Ÿæˆäº† {len(generated_clips)} ä¸ªè§†é¢‘ç‰‡æ®µï¼Œä½¿ç”¨äº† {len(pro_features_used)} ä¸ªProåŠŸèƒ½"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦Proæµæ°´çº¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°çº¢ä¹¦Proæµæ°´çº¿å¤±è´¥: {str(e)}")


@app.post("/auto_intro")
async def auto_intro(req: URLIntroReq):
    """Download video from URL, detect/extract subtitles or ASR, select highlights, cut 9:16, add simple fades, and concatenate into one intro video."""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)

        # 1) download via yt-dlp if platform URL, else urlretrieve
        ts = int(time.time())
        dl_path = str(Path("input_data/downloads") / f"dl_{ts}.mp4")
        try:
            import yt_dlp  # type: ignore
            ydl_opts = {
                'format': 'bv*+ba/b',
                'merge_output_format': 'mp4',
                'outtmpl': str(Path("input_data/downloads") / f"dl_{ts}.%(ext)s"),
                'writesubtitles': True,
                'subtitleslangs': ['zh.*','zh','zh-Hans','zh-Hant','en.*','en'],
                'subtitleformat': 'srt',
                'quiet': True,
                'noprogress': True,
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(req.url, download=True)
                # determine final filepath
                if 'requested_downloads' in info:
                    dl_path = info['requested_downloads'][0]['filepath']
                else:
                    # fallback: guess mp4 path
                    base = Path("input_data/downloads") / f"dl_{ts}"
                    if (base.with_suffix('.mp4')).exists():
                        dl_path = str(base.with_suffix('.mp4'))
                # normalize
                dl_path = str(Path(dl_path))
        except Exception:
            # Fallback 1: try system yt-dlp CLI
            try:
                cli_cmd = [
                    "yt-dlp",
                    "-f", "bv*+ba/b",
                    "--merge-output-format", "mp4",
                    "-o", str(Path("input_data/downloads") / f"dl_{ts}.%(ext)s"),
                    "--write-sub",
                    "--sub-lang", "zh.*,zh,zh-Hans,zh-Hant,en.*,en",
                    "--sub-format", "srt",
                    req.url,
                ]
                completed = subprocess.run(cli_cmd, capture_output=True, text=True)
                if completed.returncode != 0:
                    raise RuntimeError(completed.stderr or "yt-dlp CLI failed")
                # guess final mp4 path
                base = Path("input_data/downloads") / f"dl_{ts}"
                if (base.with_suffix('.mp4')).exists():
                    dl_path = str(base.with_suffix('.mp4'))
                else:
                    # try common containers
                    for ext in [".mkv", ".webm", ".mov"]:
                        cand = base.with_suffix(ext)
                        if cand.exists():
                            dl_path = str(cand)
                            break
                dl_path = str(Path(dl_path))
            except Exception:
                # Fallback 2: direct URL download (works only for direct media URLs)
                urlretrieve(req.url, dl_path)

        # 2) try extract subtitle stream to SRT
        srt_path = str(Path("output_data") / f"dl_{ts}.srt")
        # prefer subtitles downloaded by yt-dlp (scoped to current timestamp)
        possible_srts = list(Path("input_data/downloads").glob(f"dl_{ts}*.srt"))
        if possible_srts:
            # pick the largest srt as best
            best = max(possible_srts, key=lambda p: p.stat().st_size)
            try:
                Path(srt_path).write_text(best.read_text(encoding="utf-8", errors="ignore"), encoding="utf-8")
            except Exception:
                pass
        if not os.path.exists(srt_path):
            try:
                extract_cmd = [
                    "ffmpeg", "-y", "-i", dl_path,
                    "-map", "0:s:0", "-c:s", "srt", srt_path
                ]
                subprocess.run(extract_cmd, capture_output=True, text=True, timeout=45)
            except subprocess.TimeoutExpired:
                logger.warning("Subtitle extraction timed out; skipping embedded subs")
            except Exception:
                pass

        # 3) if no SRT and want_asr, run python ASR fallback
        if not os.path.exists(srt_path) and req.want_asr:
            ok = try_python_asr_to_srt(dl_path, srt_path)
            if not ok:
                srt_path = ""

        # 4) build clips
        clips: List[Dict[str, str]] = []
        if srt_path and os.path.exists(srt_path):
            items = parse_srt_simple(srt_path)
            clips = select_windows_from_srt(items, req.min_sec, req.max_sec, top_k=req.top_k)
        else:
            # fallback: split video into windows heuristically
            total = max(0, int(get_duration_seconds(dl_path)))
            if total == 0:
                raise HTTPException(status_code=400, detail="æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œä¸”æ— å­—å¹•å¯ä¾›åˆ†æ")
            start = 10
            while start + req.min_sec < total and len(clips) < req.top_k:
                end = min(total, start + req.max_sec)
                ss = f"{start//60:02d}:{start%60:02d}"
                ee = f"{end//60:02d}:{end%60:02d}"
                clips.append({"start": ss, "end": ee, "reason": "window"})
                start = end + 2

        if not clips:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨ç‰‡æ®µ")

        # 5) æ™ºèƒ½é€‰æ®µæˆ–ä¼ ç»Ÿé€‰æ®µ
        total = max(0, int(get_duration_seconds(dl_path)))
        if total == 0:
            raise HTTPException(status_code=400, detail="è§†é¢‘æ—¶é•¿ä¸å¯ç”¨")

        window_min = int(req.min_sec)
        window_max = int(req.max_sec)
        
        if req.smart_mode:
            # ä½¿ç”¨æ™ºèƒ½é€‰æ®µ
            logger.info("Using smart segment selection")
            try:
                smart_segments = get_smart_segments(dl_path, window_min, window_max, count=1)
                if smart_segments:
                    best_segment = smart_segments[0]
                    best_ss = best_segment['start_time']
                    best_dur = best_segment['duration']
                    logger.info(f"Smart selection: start={best_ss}s, duration={best_dur}s, score={best_segment['total_score']:.3f}")
                else:
                    raise ValueError("Smart selection failed")
            except Exception as e:
                logger.warning(f"Smart selection failed: {e}, falling back to traditional method")
                req.smart_mode = False  # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        
        if not req.smart_mode:
            # ä¼ ç»Ÿé€‰æ®µæ–¹æ³• (åŸæœ‰é€»è¾‘)
            scan_step = 1
            best_score = -1e9
            best_ss = 0
            best_dur = window_min
            max_scan = min(total - window_min, 15 * 60)  # up to 15 min scan
            for start_s in range(0, max(1, max_scan), scan_step):
                ss_hms = seconds_to_hms(start_s)
                # evaluate at min window first
                black_frac = black_fraction_in_segment(dl_path, ss_hms, float(window_min))
                if black_frac >= 0.5:
                    continue
                sil_frac = silence_fraction_in_segment(dl_path, ss_hms, float(window_min))
                # simple score: prefer less black/silence, earlier windows
                score = (1.0 - black_frac) * 1.5 + (1.0 - sil_frac) * 1.0 - 0.001 * start_s
                if score > best_score:
                    best_score = score
                    best_ss = start_s
                    best_dur = window_min
            if best_score < -1e8:
                # fallback to the first provided clip
                first = clips[0]
                best_ss = int(hms_to_seconds(normalize_time(first["start"])))
                best_dur = max(window_min, int(hms_to_seconds(normalize_time(first["end"])) - best_ss))

        start_time = seconds_to_hms(best_ss)
        dur_s = float(best_dur)
        fade_out_start = max(0.1, dur_s - 0.25)
        # pad-first to 1080x1920, avoiding black due to crop
        vf_pad = (
            "scale=1080:1920:force_original_aspect_ratio=decrease,"
            "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
            f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
        )

        out_final = req.output if req.output else str((Path("output_data") / f"intro_{ts}_916.mp4").resolve())
        cmd_single = [
            "ffmpeg", "-y", "-hwaccel", "none",
            "-i", dl_path,
            "-ss", start_time, "-t", f"{dur_s:.2f}",
            "-vf", vf_pad,
            # do not force frame rate; honor input
            "-pix_fmt", "yuv420p",
            "-map", "0:v:0", "-map", "0:a?",
            "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
            "-c:a", "aac", "-b:a", AUDIO_BITRATE,
            "-shortest", "-movflags", "+faststart",
            out_final
        ]
        safe_run_ffmpeg(cmd_single)

        # 7) optionally burn subtitles if we have srt and only if you want (requirement said: if has subs, do not add)
        # We skip burning here as per requirement to keep existing subtitles if present.

        return {"out": out_final, "parts": [], "clips": [{"start": seconds_to_hms(best_ss), "end": seconds_to_hms(best_ss + best_dur), "reason": "smart_window"}]}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Auto intro error: {e}")
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨ç®€ä»‹è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")


# æ–°å¢ï¼šæ™ºèƒ½å¤šç‰‡æ®µè§†é¢‘å‰ªè¾‘API
class MultiSegmentClippingReq(BaseModel):
    video_path: str
    subtitle_path: Optional[str] = None
    topic: str
    target_segments: int = 3
    segment_duration: float = 8.0
    total_duration: float = 24.0
    semantic_weight: float = 0.4
    visual_weight: float = 0.3
    audio_weight: float = 0.3
    include_intro: bool = True
    include_highlights: bool = True
    include_conclusion: bool = True

def parse_srt_file(srt_path: str) -> List[Dict]:
    """è§£æSRTå­—å¹•æ–‡ä»¶"""
    segments = []
    try:
        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç®€å•çš„SRTè§£æ
        blocks = content.strip().split('\n\n')
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                # æ—¶é—´æˆ³è¡Œ
                time_line = lines[1]
                if '-->' in time_line:
                    start_time, end_time = time_line.split(' --> ')
                    # æ–‡æœ¬å†…å®¹
                    text = ' '.join(lines[2:])
                    segments.append({
                        'start': start_time.strip(),
                        'end': end_time.strip(),
                        'text': text.strip()
                    })
    except Exception as e:
        logger.error(f"è§£æSRTæ–‡ä»¶å¤±è´¥: {e}")
    
    return segments

def time_to_seconds(time_str: str) -> float:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°"""
    try:
        # å¤„ç†æ ¼å¼å¦‚ "00:01:23,456"
        time_str = time_str.replace(',', '.')
        parts = time_str.split(':')
        if len(parts) == 3:
            h, m, s = parts
            return int(h) * 3600 + int(m) * 60 + float(s)
        elif len(parts) == 2:
            m, s = parts
            return int(m) * 60 + float(s)
        else:
            return float(parts[0])
    except:
        return 0.0

def analyze_video_segments_v2(video_path: str, subtitle_segments: List[Dict], total_duration: float, topic: str) -> List[Dict]:
    """åŸºäºæ ¸å¿ƒæ¦‚å¿µåˆ†æè§†é¢‘ç‰‡æ®µ"""
    segments = []
    
    # 1. æå–æ ¸å¿ƒæ¦‚å¿µ
    core_concepts = extract_video_core_concepts(subtitle_segments, topic)
    
    # 2. ä½¿ç”¨æ›´çµæ´»çš„çª—å£å¤§å°
    base_window_size = 15.0  # åŸºç¡€15ç§’çª—å£
    overlap = 5.0            # 5ç§’é‡å 
    
    current_time = 0.0
    segment_id = 0
    
    while current_time < total_duration - base_window_size:
        # åŠ¨æ€è°ƒæ•´çª—å£å¤§å°ï¼ˆ8-30ç§’ä¹‹é—´ï¼‰
        window_size = base_window_size
        end_time = min(current_time + window_size, total_duration)
        
        # è·å–è¯¥æ—¶é—´æ®µçš„å­—å¹•æ–‡æœ¬
        segment_text = ""
        subtitle_count = 0
        
        for sub in subtitle_segments:
            sub_start = time_to_seconds(sub['start'])
            sub_end = time_to_seconds(sub['end'])
            
            # æ›´å®½æ¾çš„æ—¶é—´åŒ¹é…
            if sub_start < end_time and sub_end > current_time:
                segment_text += sub['text'] + " "
                subtitle_count += 1
        
        # æ™ºèƒ½è¾¹ç•Œæ£€æµ‹ï¼šç¡®ä¿ä¸åœ¨å¥å­ä¸­é—´åˆ‡æ–­
        if subtitle_count >= 3:
            # å¯»æ‰¾æ›´å¥½çš„ç»“æŸç‚¹ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·åï¼‰
            for sub in subtitle_segments:
                sub_start = time_to_seconds(sub['start'])
                sub_end = time_to_seconds(sub['end'])
                
                if sub_start >= end_time and sub_start <= end_time + 5:  # 5ç§’ç¼“å†²
                    text = sub['text'].strip()
                    # å¦‚æœè¿™ä¸ªå­—å¹•ä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç»“å°¾ï¼Œå°±å»¶ä¼¸åˆ°è¿™é‡Œ
                    if text.endswith(('ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!')):
                        window_size = sub_end - current_time
                        end_time = sub_end
                        break
        
        # å¦‚æœæ–‡æœ¬å¤ªå°‘ï¼Œæ‰©å±•çª—å£
        if subtitle_count < 3 and end_time < total_duration - 10:
            window_size = min(window_size + 10, 35.0)  # å¢åŠ åˆ°35ç§’æœ€å¤§
            end_time = min(current_time + window_size, total_duration)
            
            # é‡æ–°è·å–æ–‡æœ¬
            segment_text = ""
            for sub in subtitle_segments:
                sub_start = time_to_seconds(sub['start'])
                sub_end = time_to_seconds(sub['end'])
                if sub_start < end_time and sub_end > current_time:
                    segment_text += sub['text'] + " "
        
        # è®¡ç®—è¯­ä¹‰è¯„åˆ†ï¼ˆåŸºäºæ ¸å¿ƒæ¦‚å¿µï¼‰
        semantic_analysis = calculate_semantic_score(segment_text, topic, core_concepts)
        
        # è®¡ç®—å…¶ä»–è¯„åˆ†
        visual_score = calculate_visual_score(current_time, end_time, video_path)
        audio_score = calculate_audio_score(current_time, end_time, video_path)
        position_weight = calculate_position_weight(current_time, total_duration)
        
        # ç»¼åˆè¯„åˆ† - æé«˜è¯­ä¹‰æƒé‡
        total_score = (
            semantic_analysis["total_score"] * 0.6 + 
            visual_score * 0.2 + 
            audio_score * 0.1 + 
            position_weight * 0.1
        )
        
        # æ‰¾å‡ºæœ€åŒ¹é…çš„æ ¸å¿ƒæ¦‚å¿µ
        best_concept = None
        best_concept_score = 0
        for concept_name, concept_data in semantic_analysis["concept_scores"].items():
            if concept_data["score"] > best_concept_score:
                best_concept = concept_name
                best_concept_score = concept_data["score"]
        
        segments.append({
            'id': segment_id,
            'start_time': current_time,
            'end_time': end_time,
            'duration': end_time - current_time,
            'text': segment_text.strip(),
            'semantic_analysis': semantic_analysis,
            'visual_score': visual_score,
            'audio_score': audio_score,
            'position_weight': position_weight,
            'total_score': total_score,
            'best_concept': best_concept,
            'best_concept_score': best_concept_score,
            'subtitle_count': subtitle_count
        })
        
        current_time += (window_size - overlap)
        segment_id += 1
    
    return segments

def analyze_video_content_structure(subtitle_segments: List[Dict]) -> Dict:
    """åˆ†æè§†é¢‘å†…å®¹ç»“æ„ï¼Œè‡ªåŠ¨æå–å…³é”®ä¸»é¢˜å’Œè®ºç‚¹"""
    full_text = " ".join([seg['text'] for seg in subtitle_segments])
    
    # 1. æ‰¾åˆ°å…³é”®é—®é¢˜å’Œå®šä¹‰
    key_questions = []
    definitions = []
    examples = []
    background = []
    conclusions = []
    
    for i, seg in enumerate(subtitle_segments):
        text = seg['text'].strip()
        start_time = time_to_seconds(seg['start'])
        
        # è¯†åˆ«å…³é”®é—®é¢˜
        if any(q in text for q in ['ä»€ä¹ˆæ˜¯', 'åˆ°åº•æ˜¯ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆåš', 'å¦‚ä½•']):
            key_questions.append({
                'text': text,
                'start_time': start_time,
                'type': 'question'
            })
        
        # è¯†åˆ«å®šä¹‰æ€§å†…å®¹
        if any(d in text for d in ['å®šä¹‰', 'æ¦‚å¿µ', 'æœ¬è´¨', 'æ ¸å¿ƒ', 'å°±æ˜¯']):
            definitions.append({
                'text': text,
                'start_time': start_time,
                'type': 'definition'
            })
        
        # è¯†åˆ«ä¾‹å­å’Œæ¡ˆä¾‹
        if any(e in text for e in ['æ¯”å¦‚', 'ä¾‹å­', 'æ¡ˆä¾‹', 'ä¸¾ä¾‹', 'æ¼”ç¤º']):
            examples.append({
                'text': text,
                'start_time': start_time,
                'type': 'example'
            })
        
        # è¯†åˆ«èƒŒæ™¯ä¿¡æ¯
        if any(b in text for b in ['èƒŒæ™¯', 'ç°å®', 'å‘ç”Ÿ', 'å˜åŒ–', 'é©å‘½', 'å†å²']):
            background.append({
                'text': text,
                'start_time': start_time,
                'type': 'background'
            })
    
    # 2. åŸºäºVibe Codingä¸»é¢˜çš„ç‰¹å®šåˆ†æ
    vibe_coding_analysis = {
        'introduction': [],
        'definition': [],
        'why_important': [],
        'how_it_works': [],
        'advantages': [],
        'context': [],
        'future': []
    }
    
    for seg in subtitle_segments:
        text = seg['text'].strip().lower()
        start_time = time_to_seconds(seg['start'])
        
        # å¼€åœºä»‹ç»
        if any(intro in text for intro in ['æœ€è¿‘', 'å‘ç”Ÿ', 'ä¸¤ä»¶äº‹', 'å‘Šè¯‰å¤§å®¶']):
            vibe_coding_analysis['introduction'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 0.9
            })
        
        # æ ¸å¿ƒå®šä¹‰
        if 'vibe coding' in text and any(def_word in text for def_word in ['ä»€ä¹ˆ', 'å®šä¹‰', 'æ¦‚å¿µ', 'åˆ°åº•æ˜¯']):
            vibe_coding_analysis['definition'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 1.0
            })
        
        # é‡è¦æ€§è¯´æ˜
        if any(imp in text for imp in ['ä»·å€¼', 'é‡è¦', 'ç€è¿·', 'å¿…é¡»', 'éœ€è¦']):
            vibe_coding_analysis['why_important'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 0.9
            })
        
        # å·¥ä½œåŸç†
        if any(how in text for how in ['æ€ä¹ˆ', 'å¦‚ä½•', 'æ–¹æ³•', 'æ–¹å¼', 'æ“ä½œ']):
            vibe_coding_analysis['how_it_works'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 0.8
            })
        
        # ä¼˜åŠ¿åˆ†æ
        if any(adv in text for adv in ['ä¼˜åŠ¿', 'å¥½å¤„', 'æ•ˆç‡', 'å¿«é€Ÿ', 'ç®€å•']):
            vibe_coding_analysis['advantages'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 0.8
            })
        
        # èƒŒæ™¯ä¸Šä¸‹æ–‡
        if any(ctx in text for ctx in ['è‡ªåŠ¨åŒ–', 'é©å‘½', 'å¤±ä¸š', 'å·¥å…·äºº', 'ç°å®']):
            vibe_coding_analysis['context'].append({
                'text': seg['text'],
                'start_time': start_time,
                'relevance': 0.7
            })
    
    return {
        'structure_analysis': {
            'key_questions': key_questions,
            'definitions': definitions,
            'examples': examples,
            'background': background
        },
        'vibe_coding_analysis': vibe_coding_analysis,
        'content_summary': generate_content_summary(vibe_coding_analysis)
    }

def generate_content_summary(analysis: Dict) -> Dict:
    """åŸºäºåˆ†æç”Ÿæˆå†…å®¹æ‘˜è¦"""
    summary = {
        'main_topics': [],
        'key_segments': [],
        'content_flow': []
    }
    
    # è¯†åˆ«ä¸»è¦è¯é¢˜
    for topic, segments in analysis.items():
        if segments and len(segments) > 0:
            avg_relevance = sum(s.get('relevance', 0) for s in segments) / len(segments)
            summary['main_topics'].append({
                'topic': topic,
                'segment_count': len(segments),
                'avg_relevance': avg_relevance,
                'description': get_topic_description(topic)
            })
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    summary['main_topics'].sort(key=lambda x: x['avg_relevance'], reverse=True)
    
    # è¯†åˆ«å…³é”®ç‰‡æ®µ
    all_segments = []
    for topic, segments in analysis.items():
        for seg in segments:
            seg['topic'] = topic
            all_segments.append(seg)
    
    # æŒ‰ç›¸å…³æ€§å’Œæ—¶é—´æ’åº
    all_segments.sort(key=lambda x: (x.get('relevance', 0), -x['start_time']), reverse=True)
    summary['key_segments'] = all_segments[:10]  # å–å‰10ä¸ªå…³é”®ç‰‡æ®µ
    
    return summary

def get_topic_description(topic: str) -> str:
    """è·å–è¯é¢˜æè¿°"""
    descriptions = {
        'introduction': 'è§†é¢‘å¼€åœºå¼•å…¥',
        'definition': 'Vibe Codingæ ¸å¿ƒå®šä¹‰',
        'why_important': 'é‡è¦æ€§å’Œä»·å€¼é˜è¿°',
        'how_it_works': 'å·¥ä½œåŸç†å’Œæ–¹æ³•',
        'advantages': 'ä¼˜åŠ¿å’Œå¥½å¤„',
        'context': 'èƒŒæ™¯å’Œç°å®åˆ†æ',
        'future': 'æœªæ¥å‘å±•è¶‹åŠ¿'
    }
    return descriptions.get(topic, topic)

def extract_video_core_concepts(subtitle_segments: List[Dict], topic: str) -> Dict:
    """æå–è§†é¢‘æ ¸å¿ƒæ¦‚å¿µå’Œå…³é”®è®ºç‚¹"""
    # åˆå¹¶æ‰€æœ‰å­—å¹•æ–‡æœ¬
    full_text = " ".join([seg['text'] for seg in subtitle_segments])
    
    # åŸºäºä¸»é¢˜çš„æ ¸å¿ƒæ¦‚å¿µè¯†åˆ«
    if "vibe coding" in topic.lower() or "ç¼–ç¨‹" in topic:
        core_concepts = {
            "opening_hook": {
                "keywords": ["ä¸ºä»€ä¹ˆ", "æˆ‘ä»¬æ¯ä¸ªäºº", "ç«‹å³", "é©¬ä¸Š", "å¼€å§‹", "æ‰€è°“çš„", "vibe coding"],
                "weight": 1.0,
                "min_duration": 8.0,
                "description": "å¼€åœºå¼•å…¥ï¼šä¸ºä»€ä¹ˆè¦å¼€å§‹Vibe Coding"
            },
            "core_definition": {
                "keywords": ["vibe codingåˆ°åº•æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ", "æœ¬è´¨", "æ ¸å¿ƒ"],
                "weight": 1.0,
                "min_duration": 15.0,
                "description": "æ ¸å¿ƒå®šä¹‰ï¼šVibe Codingåˆ°åº•æ˜¯ä»€ä¹ˆ"
            },
            "value_proposition": {
                "keywords": ["ä»·å€¼", "ä¸ºä»€ä¹ˆ", "ç€è¿·", "é‡è¦", "æ„ä¹‰", "ä½œç”¨"],
                "weight": 1.0,
                "min_duration": 12.0,
                "description": "ä»·å€¼ä¸»å¼ ï¼šä¸ºä»€ä¹ˆVibe Codingé‡è¦"
            },
            "background_context": {
                "keywords": ["èƒŒæ™¯", "ç°å®", "å‘ç”Ÿ", "å˜åŒ–", "è¶‹åŠ¿", "é©å‘½", "è‡ªåŠ¨åŒ–"],
                "weight": 0.9,
                "min_duration": 10.0,
                "description": "èƒŒæ™¯åˆ†æï¼šæŠ€æœ¯å˜é©çš„ç°å®"
            },
            "practical_approach": {
                "keywords": ["æ€ä¹ˆåš", "æ–¹æ³•", "å®è·µ", "æ“ä½œ", "å…·ä½“", "æ­¥éª¤"],
                "weight": 0.8,
                "min_duration": 8.0,
                "description": "å®è·µæ–¹æ³•ï¼šå¦‚ä½•è¿›è¡ŒVibe Coding"
            },
            # é™ä½è¿™äº›è¾¹ç¼˜æ¦‚å¿µçš„æƒé‡
            "personal_story": {
                "keywords": ["ä¸€ä¸‡ç¾é‡‘", "èµåŠ©", "å–æ¶ˆ", "æ”¶ç›Š", "æ›´æ–°"],
                "weight": 0.3,  # å¤§å¹…é™ä½æƒé‡
                "min_duration": 5.0,
                "description": "ä¸ªäººæ•…äº‹ï¼ˆè¾¹ç¼˜å†…å®¹ï¼‰"
            }
        }
    else:
        # é€šç”¨æŠ€æœ¯åˆ†äº«çš„æ ¸å¿ƒæ¦‚å¿µ
        core_concepts = {
            "introduction": {
                "keywords": ["ä»‹ç»", "ä»€ä¹ˆæ˜¯", "æ¦‚å¿µ", "å®šä¹‰"],
                "weight": 1.0,
                "min_duration": 8.0,
                "description": "ä¸»é¢˜ä»‹ç»"
            },
            "main_points": {
                "keywords": ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦"],
                "weight": 1.0,
                "min_duration": 10.0,
                "description": "æ ¸å¿ƒè¦ç‚¹"
            },
            "examples": {
                "keywords": ["ä¾‹å­", "æ¡ˆä¾‹", "æ¯”å¦‚", "æ¼”ç¤º"],
                "weight": 0.8,
                "min_duration": 6.0,
                "description": "å®ä¾‹è¯´æ˜"
            }
        }
    
    return core_concepts

def calculate_semantic_score(text: str, topic: str, core_concepts: Dict) -> Dict:
    """åŸºäºæ ¸å¿ƒæ¦‚å¿µè®¡ç®—è¯­ä¹‰ç›¸å…³æ€§è¯„åˆ†"""
    if not text.strip():
        return {"total_score": 0.0, "concept_scores": {}}
    
    text_lower = text.lower()
    concept_scores = {}
    
    for concept_name, concept_info in core_concepts.items():
        score = 0.0
        matched_keywords = []
        
        # å…³é”®è¯åŒ¹é…
        for keyword in concept_info["keywords"]:
            if keyword.lower() in text_lower:
                score += 0.2
                matched_keywords.append(keyword)
        
        # æ–‡æœ¬é•¿åº¦å¥–åŠ±
        if len(text) > 30:
            score += 0.1
        
        # æƒé‡è°ƒæ•´
        weighted_score = score * concept_info["weight"]
        
        concept_scores[concept_name] = {
            "score": min(weighted_score, 1.0),
            "matched_keywords": matched_keywords,
            "weight": concept_info["weight"],
            "min_duration": concept_info["min_duration"]
        }
    
    # è®¡ç®—æ€»åˆ†
    total_score = sum(cs["score"] for cs in concept_scores.values())
    
    return {
        "total_score": min(total_score, 1.0),
        "concept_scores": concept_scores
    }

def calculate_visual_score(start_time: float, end_time: float, video_path: str) -> float:
    """è®¡ç®—è§†è§‰è´¨é‡è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„è¯„åˆ†é€»è¾‘
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯åˆ†æç”»é¢è´¨é‡
    
    # é¿å¼€è§†é¢‘å¼€å¤´å’Œç»“å°¾çš„ä½è´¨é‡éƒ¨åˆ†
    if start_time < 5.0 or end_time > 1400:  # 24åˆ†é’Ÿè§†é¢‘çš„æœ€å1åˆ†é’Ÿ
        return 0.6
    
    # ä¸­é—´éƒ¨åˆ†é€šå¸¸è´¨é‡è¾ƒå¥½
    return 0.8

def calculate_audio_score(start_time: float, end_time: float, video_path: str) -> float:
    """è®¡ç®—éŸ³é¢‘è´¨é‡è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # ç®€åŒ–çš„éŸ³é¢‘è´¨é‡è¯„ä¼°
    # å‡è®¾ä¸­é—´éƒ¨åˆ†éŸ³é¢‘è´¨é‡æ›´ç¨³å®š
    if 60 < start_time < 1200:  # 1-20åˆ†é’Ÿä¹‹é—´
        return 0.9
    else:
        return 0.7

def calculate_position_weight(current_time: float, total_duration: float) -> float:
    """è®¡ç®—ä½ç½®æƒé‡"""
    position_ratio = current_time / total_duration
    
    # å¼€å¤´éƒ¨åˆ†æƒé‡è¾ƒé«˜
    if position_ratio < 0.1:
        return 0.9
    # ç»“å°¾éƒ¨åˆ†æƒé‡è¾ƒé«˜
    elif position_ratio > 0.8:
        return 0.8
    # ä¸­é—´éƒ¨åˆ†æƒé‡ä¸­ç­‰
    else:
        return 0.6

def intelligent_segment_selection_v2(
    segments: List[Dict], 
    topic: str,
    core_concepts: Dict
) -> List[Dict]:
    """åŸºäºæ ¸å¿ƒæ¦‚å¿µçš„æ™ºèƒ½ç‰‡æ®µé€‰æ‹©"""
    
    selected = []
    concept_coverage = {}
    
    # 1. ä¸ºæ¯ä¸ªæ ¸å¿ƒæ¦‚å¿µæ‰¾åˆ°æœ€ä½³ç‰‡æ®µ
    for concept_name, concept_info in core_concepts.items():
        best_segments_for_concept = []
        
        # æ‰¾åˆ°æ‰€æœ‰ä¸è¯¥æ¦‚å¿µç›¸å…³çš„ç‰‡æ®µ
        for segment in segments:
            concept_score = segment['semantic_analysis']['concept_scores'].get(concept_name, {}).get('score', 0)
            if concept_score > 0.3:  # æœ€ä½ç›¸å…³æ€§é˜ˆå€¼
                segment_with_concept = segment.copy()
                segment_with_concept['concept_relevance'] = concept_score
                segment_with_concept['target_concept'] = concept_name
                best_segments_for_concept.append(segment_with_concept)
        
        # æŒ‰æ¦‚å¿µç›¸å…³æ€§æ’åº
        best_segments_for_concept.sort(key=lambda x: x['concept_relevance'], reverse=True)
        
        # ä¸ºè¿™ä¸ªæ¦‚å¿µé€‰æ‹©æœ€ä½³ç‰‡æ®µï¼ˆå¯èƒ½é€‰æ‹©å¤šä¸ªï¼‰
        concept_segments_selected = 0
        max_segments_per_concept = 2 if concept_info['weight'] >= 0.9 else 1
        
        for segment in best_segments_for_concept:
            if concept_segments_selected >= max_segments_per_concept:
                break
                
            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰ç‰‡æ®µé‡å 
            overlaps = False
            for selected_seg in selected:
                if (segment['start_time'] < selected_seg['end_time'] and 
                    segment['end_time'] > selected_seg['start_time']):
                    overlaps = True
                    break
            
            if not overlaps:
                # åŠ¨æ€è°ƒæ•´ç‰‡æ®µæ—¶é•¿
                min_duration = concept_info['min_duration']
                actual_duration = max(min_duration, segment['duration'])
                actual_duration = min(actual_duration, 30.0)  # æœ€é•¿30ç§’
                
                selected.append({
                    'start_time': segment['start_time'],
                    'end_time': segment['start_time'] + actual_duration,
                    'duration': actual_duration,
                    'score': segment['total_score'],
                    'concept_relevance': segment['concept_relevance'],
                    'type': concept_name,
                    'description': concept_info['description'],
                    'text': segment['text'][:200] + "..." if len(segment['text']) > 200 else segment['text'],
                    'matched_keywords': segment['semantic_analysis']['concept_scores'][concept_name].get('matched_keywords', [])
                })
                concept_segments_selected += 1
                concept_coverage[concept_name] = concept_segments_selected
    
    # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„æ ¸å¿ƒå†…å®¹ï¼Œæ·»åŠ é«˜åˆ†ç‰‡æ®µ
    if len(selected) < 2:
        high_score_segments = sorted(segments, key=lambda x: x['total_score'], reverse=True)
        
        for segment in high_score_segments[:5]:  # æ£€æŸ¥å‰5ä¸ªé«˜åˆ†ç‰‡æ®µ
            overlaps = False
            for selected_seg in selected:
                if (segment['start_time'] < selected_seg['end_time'] and 
                    segment['end_time'] > selected_seg['start_time']):
                    overlaps = True
                    break
            
            if not overlaps:
                selected.append({
                    'start_time': segment['start_time'],
                    'end_time': segment['start_time'] + min(segment['duration'], 20.0),
                    'duration': min(segment['duration'], 20.0),
                    'score': segment['total_score'],
                    'concept_relevance': segment.get('best_concept_score', 0),
                    'type': 'high_score',
                    'description': 'é«˜è´¨é‡å†…å®¹ç‰‡æ®µ',
                    'text': segment['text'][:200] + "..." if len(segment['text']) > 200 else segment['text'],
                    'matched_keywords': []
                })
                
                if len(selected) >= 3:  # æœ€å¤š3ä¸ªç‰‡æ®µ
                    break
    
    # 3. æŒ‰æ—¶é—´é¡ºåºæ’åº
    selected.sort(key=lambda x: x['start_time'])
    
    # 4. é™åˆ¶æ€»æ—¶é•¿ï¼ˆå¦‚æœå¤ªé•¿å°±ç¼©çŸ­ç‰‡æ®µï¼‰
    total_duration = sum(seg['duration'] for seg in selected)
    if total_duration > 90:  # æœ€é•¿90ç§’
        # ç­‰æ¯”ä¾‹ç¼©çŸ­æ¯ä¸ªç‰‡æ®µ
        scale_factor = 90 / total_duration
        for seg in selected:
            seg['duration'] *= scale_factor
            seg['end_time'] = seg['start_time'] + seg['duration']
    
    return selected

def generate_dynamic_concepts_from_analysis(content_analysis: Dict, topic: str) -> Dict:
    """åŸºäºå†…å®¹åˆ†æåŠ¨æ€ç”Ÿæˆæ ¸å¿ƒæ¦‚å¿µ"""
    analysis = content_analysis['vibe_coding_analysis']
    concepts = {}
    
    # åŸºäºå®é™…åˆ†æç»“æœç”Ÿæˆæ¦‚å¿µ
    for topic_name, segments in analysis.items():
        if segments:  # åªæœ‰å½“å®é™…æ‰¾åˆ°ç›¸å…³å†…å®¹æ—¶æ‰ç”Ÿæˆæ¦‚å¿µ
            avg_relevance = sum(s.get('relevance', 0) for s in segments) / len(segments)
            
            concepts[topic_name] = {
                'weight': min(avg_relevance + 0.1, 1.0),  # åŸºäºå®é™…ç›¸å…³æ€§è®¾ç½®æƒé‡
                'min_duration': get_dynamic_duration(topic_name, len(segments)),
                'description': get_topic_description(topic_name),
                'segment_count': len(segments),
                'avg_relevance': avg_relevance
            }
    
    return concepts

def get_dynamic_duration(topic_name: str, segment_count: int) -> float:
    """æ ¹æ®è¯é¢˜å’Œç‰‡æ®µæ•°é‡åŠ¨æ€å†³å®šæœ€å°æ—¶é•¿"""
    base_durations = {
        'definition': 20.0,      # å®šä¹‰éœ€è¦æ›´é•¿æ—¶é—´
        'why_important': 15.0,   # é‡è¦æ€§è¯´æ˜
        'introduction': 12.0,    # å¼€åœºå¼•å…¥
        'how_it_works': 18.0,    # å·¥ä½œåŸç†
        'advantages': 10.0,      # ä¼˜åŠ¿è¯´æ˜
        'context': 15.0,         # èƒŒæ™¯åˆ†æ
        'future': 8.0            # æœªæ¥å±•æœ›
    }
    
    base_duration = base_durations.get(topic_name, 10.0)
    
    # å¦‚æœç‰‡æ®µå¾ˆå¤šï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´æ¥å®Œæ•´è¡¨è¾¾
    if segment_count > 3:
        base_duration *= 1.2
    
    return min(base_duration, 25.0)  # æœ€é•¿25ç§’

def intelligent_segment_selection_v3(
    segments: List[Dict], 
    content_analysis: Dict,
    core_concepts: Dict
) -> List[Dict]:
    """åŸºäºå†…å®¹åˆ†æçš„é«˜çº§æ™ºèƒ½ç‰‡æ®µé€‰æ‹©"""
    
    selected = []
    key_segments = content_analysis['content_summary']['key_segments']
    
    # 1. ä¼˜å…ˆé€‰æ‹©å…³é”®ç‰‡æ®µä¸­ç›¸å…³æ€§æœ€é«˜çš„
    processed_time_ranges = []
    
    for key_seg in key_segments[:5]:  # æ£€æŸ¥å‰5ä¸ªå…³é”®ç‰‡æ®µ
        start_time = key_seg['start_time']
        topic = key_seg['topic']
        relevance = key_seg.get('relevance', 0)
        
        # åªé€‰æ‹©é«˜ç›¸å…³æ€§çš„ç‰‡æ®µ
        if relevance < 0.7:
            continue
            
        # æ‰¾åˆ°åŒ…å«è¿™ä¸ªå…³é”®ç‰‡æ®µçš„è§†é¢‘æ®µ
        best_video_segment = None
        best_overlap = 0
        
        for video_seg in segments:
            # æ£€æŸ¥é‡å 
            overlap_start = max(start_time, video_seg['start_time'])
            overlap_end = min(start_time + 10, video_seg['end_time'])  # å‡è®¾å…³é”®ç‰‡æ®µ10ç§’
            overlap = max(0, overlap_end - overlap_start)
            
            if overlap > best_overlap:
                best_overlap = overlap
                best_video_segment = video_seg
        
        if best_video_segment and best_overlap > 3:  # è‡³å°‘3ç§’é‡å 
            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰ç‰‡æ®µé‡å 
            overlaps_existing = False
            for existing in selected:
                if (best_video_segment['start_time'] < existing['end_time'] and 
                    best_video_segment['end_time'] > existing['start_time']):
                    overlaps_existing = True
                    break
            
            if not overlaps_existing:
                # æ ¹æ®è¯é¢˜é‡è¦æ€§è°ƒæ•´æ—¶é•¿
                concept_info = core_concepts.get(topic, {})
                min_duration = concept_info.get('min_duration', 15.0)
                
                # ç¡®ä¿ç‰‡æ®µè¶³å¤Ÿé•¿æ¥å®Œæ•´è¡¨è¾¾
                actual_duration = max(min_duration, best_video_segment['duration'])
                actual_duration = min(actual_duration, 30.0)  # æœ€é•¿30ç§’
                
                selected.append({
                    'start_time': best_video_segment['start_time'],
                    'end_time': best_video_segment['start_time'] + actual_duration,
                    'duration': actual_duration,
                    'score': best_video_segment['total_score'],
                    'relevance': relevance,
                    'type': topic,
                    'description': concept_info.get('description', topic),
                    'text': key_seg['text'][:300] + "..." if len(key_seg['text']) > 300 else key_seg['text'],
                    'key_segment_match': True
                })
                
                processed_time_ranges.append((best_video_segment['start_time'], 
                                            best_video_segment['start_time'] + actual_duration))
    
    # 2. å¦‚æœé€‰æ‹©çš„ç‰‡æ®µä¸å¤Ÿï¼Œä»é«˜åˆ†ç‰‡æ®µä¸­è¡¥å……
    if len(selected) < 2:
        high_score_segments = sorted(segments, key=lambda x: x['total_score'], reverse=True)
        
        for segment in high_score_segments[:8]:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„æ—¶é—´èŒƒå›´é‡å 
            overlaps = False
            for start_range, end_range in processed_time_ranges:
                if (segment['start_time'] < end_range and segment['end_time'] > start_range):
                    overlaps = True
                    break
            
            if not overlaps:
                selected.append({
                    'start_time': segment['start_time'],
                    'end_time': segment['start_time'] + min(segment['duration'], 20.0),
                    'duration': min(segment['duration'], 20.0),
                    'score': segment['total_score'],
                    'relevance': segment.get('best_concept_score', 0.5),
                    'type': 'high_quality',
                    'description': 'é«˜è´¨é‡è¡¥å……å†…å®¹',
                    'text': segment['text'][:300] + "..." if len(segment['text']) > 300 else segment['text'],
                    'key_segment_match': False
                })
                
                processed_time_ranges.append((segment['start_time'], 
                                            segment['start_time'] + min(segment['duration'], 20.0)))
                
                if len(selected) >= 3:
                    break
    
    # 3. æŒ‰æ—¶é—´é¡ºåºæ’åº
    selected.sort(key=lambda x: x['start_time'])
    
    # 4. ä¼˜åŒ–æ€»æ—¶é•¿
    total_duration = sum(seg['duration'] for seg in selected)
    if total_duration > 60:  # å¦‚æœè¶…è¿‡60ç§’ï¼Œé€‚å½“ç¼©çŸ­
        scale_factor = 60 / total_duration
        for seg in selected:
            seg['duration'] *= scale_factor
            seg['end_time'] = seg['start_time'] + seg['duration']
    
    return selected

def combine_segments_to_video(video_path: str, segments: List[Dict], output_path: str) -> None:
    """å°†å¤šä¸ªç‰‡æ®µç»„åˆæˆä¸€ä¸ªè§†é¢‘"""
    try:
        # åˆ›å»ºä¸´æ—¶ç‰‡æ®µæ–‡ä»¶åˆ—è¡¨
        temp_files = []
        concat_list_path = f"temp_concat_{int(time.time())}.txt"
        
        # ç”Ÿæˆæ¯ä¸ªç‰‡æ®µ
        for i, segment in enumerate(segments):
            temp_file = f"temp_segment_{int(time.time())}_{i}.mp4"
            
            # æå–ç‰‡æ®µ
            cmd = [
                "ffmpeg", "-y", "-hwaccel", "none",
                "-i", video_path,
                "-ss", str(segment['start_time']),
                "-t", str(segment['duration']),
                "-vf", "scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1",
                "-pix_fmt", "yuv420p",
                "-c:v", "libx264", "-preset", "veryfast", "-crf", "23",
                "-c:a", "aac", "-b:a", "128k",
                "-movflags", "+faststart",
                temp_file
            ]
            
            safe_run_ffmpeg(cmd)
            temp_files.append(temp_file)
        
        # åˆ›å»ºconcatæ–‡ä»¶åˆ—è¡¨
        with open(concat_list_path, 'w') as f:
            for temp_file in temp_files:
                f.write(f"file '{os.path.abspath(temp_file)}'\n")
        
        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        concat_cmd = [
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", concat_list_path,
            "-c", "copy",
            "-movflags", "+faststart",
            output_path
        ]
        
        safe_run_ffmpeg(concat_cmd)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        if os.path.exists(concat_list_path):
            os.remove(concat_list_path)
            
    except Exception as e:
        logger.error(f"åˆå¹¶è§†é¢‘ç‰‡æ®µå¤±è´¥: {e}")
        raise

def calculate_content_coverage(segments: List[Dict], subtitle_text: List[Dict]) -> float:
    """è®¡ç®—å†…å®¹è¦†ç›–åº¦"""
    if not subtitle_text:
        return 0.5
    
    total_subtitle_duration = sum(
        time_to_seconds(sub['end']) - time_to_seconds(sub['start']) 
        for sub in subtitle_text
    )
    
    selected_duration = sum(seg['duration'] for seg in segments)
    
    return min(selected_duration / total_subtitle_duration * 5, 1.0)  # ä¹˜ä»¥5å› ä¸ºæˆ‘ä»¬åªé€‰æ‹©äº†å¾ˆå°ä¸€éƒ¨åˆ†

def calculate_visual_quality(segments: List[Dict]) -> float:
    """è®¡ç®—è§†è§‰è´¨é‡è¯„åˆ†"""
    if not segments:
        return 0.0
    return sum(seg.get('visual_score', 0.8) for seg in segments) / len(segments)

def calculate_audio_quality(segments: List[Dict]) -> float:
    """è®¡ç®—éŸ³é¢‘è´¨é‡è¯„åˆ†"""
    if not segments:
        return 0.0
    return sum(seg.get('audio_score', 0.8) for seg in segments) / len(segments)

@app.post("/video/multi_segment_clipping")
async def multi_segment_intelligent_clipping(req: MultiSegmentClippingReq):
    """æ™ºèƒ½å¤šç‰‡æ®µè§†é¢‘å‰ªè¾‘ - ä»é•¿è§†é¢‘ä¸­é€‰æ‹©å¤šä¸ªç²¾å½©ç‰‡æ®µç»„åˆ"""
    try:
        logger.info(f"å¼€å§‹æ™ºèƒ½å¤šç‰‡æ®µå‰ªè¾‘: {req.video_path}")
        
        # 1. æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(req.video_path):
            raise HTTPException(status_code=404, detail=f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {req.video_path}")
        
        # 2. åˆ†æè§†é¢‘åŸºæœ¬ä¿¡æ¯
        try:
            result = subprocess.run([
                "ffprobe", "-v", "quiet", "-print_format", "json", 
                "-show_format", req.video_path
            ], capture_output=True, text=True, check=True)
            
            video_info = json.loads(result.stdout)
            total_duration = float(video_info['format']['duration'])
            
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail="æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
        
        if total_duration < 60:
            raise HTTPException(status_code=400, detail="è§†é¢‘æ—¶é•¿å¤ªçŸ­ï¼Œä¸é€‚åˆå¤šç‰‡æ®µå‰ªè¾‘")
        
        logger.info(f"è§†é¢‘æ€»æ—¶é•¿: {total_duration:.1f}ç§’")
        
        # 3. è¯»å–å­—å¹•æ–‡ä»¶
        subtitle_segments = []
        if req.subtitle_path and os.path.exists(req.subtitle_path):
            subtitle_segments = parse_srt_file(req.subtitle_path)
            logger.info(f"è¯»å–åˆ°{len(subtitle_segments)}ä¸ªå­—å¹•ç‰‡æ®µ")
        else:
            logger.warning("æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç®€åŒ–åˆ†æ")
        
        # 4. å…ˆè¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æ
        content_analysis = analyze_video_content_structure(subtitle_segments)
        logger.info(f"å†…å®¹åˆ†æå®Œæˆï¼šè¯†åˆ«å‡º{len(content_analysis['content_summary']['main_topics'])}ä¸ªä¸»è¦è¯é¢˜")
        
        # æ‰“å°åˆ†ææ‘˜è¦
        for topic in content_analysis['content_summary']['main_topics'][:3]:
            logger.info(f"è¯é¢˜: {topic['description']} - ç›¸å…³æ€§: {topic['avg_relevance']:.2f} - ç‰‡æ®µæ•°: {topic['segment_count']}")
        
        # 5. åŸºäºå†…å®¹åˆ†æç»“æœè¿›è¡Œè§†é¢‘ç‰‡æ®µåˆ†æ
        segments = analyze_video_segments_v2(req.video_path, subtitle_segments, total_duration, req.topic)
        logger.info(f"åˆ†æäº†{len(segments)}ä¸ªè§†é¢‘ç‰‡æ®µ")
        
        # 6. åŸºäºå†…å®¹åˆ†æåŠ¨æ€ç”Ÿæˆæ ¸å¿ƒæ¦‚å¿µ
        core_concepts = generate_dynamic_concepts_from_analysis(content_analysis, req.topic)
        logger.info(f"åŸºäºå†…å®¹åˆ†æç”Ÿæˆ{len(core_concepts)}ä¸ªæ ¸å¿ƒæ¦‚å¿µ")
        
        # 7. åŸºäºåˆ†æç»“æœæ™ºèƒ½é€‰æ‹©ç‰‡æ®µ
        selected_segments = intelligent_segment_selection_v3(
            segments, 
            content_analysis,
            core_concepts
        )
        
        logger.info(f"é€‰æ‹©äº†{len(selected_segments)}ä¸ªç‰‡æ®µè¿›è¡Œç»„åˆ")
        
        # 6. ç”Ÿæˆç»„åˆè§†é¢‘
        timestamp = int(time.time())
        output_path = f"output_data/multi_clip_{timestamp}.mp4"
        
        combine_segments_to_video(req.video_path, selected_segments, output_path)
        
        logger.info(f"å¤šç‰‡æ®µè§†é¢‘ç”Ÿæˆå®Œæˆ: {output_path}")
        
        return {
            "status": "success",
            "output_video": output_path,
            "selected_segments": [
                {
                    "start_time": seg['start_time'],
                    "end_time": seg['end_time'],
                    "duration": seg['duration'],
                    "type": seg['type'],
                    "score": round(seg['score'], 3),
                    "preview_text": seg['text']
                }
                for seg in selected_segments
            ],
            "analysis": {
                "total_video_duration": round(total_duration, 1),
                "analyzed_segments": len(segments),
                "selected_segments": len(selected_segments),
                "total_output_duration": sum(seg['duration'] for seg in selected_segments),
                "selection_strategy": {
                    "semantic_weight": req.semantic_weight,
                    "visual_weight": req.visual_weight,
                    "audio_weight": req.audio_weight,
                    "include_intro": req.include_intro,
                    "include_highlights": req.include_highlights,
                    "include_conclusion": req.include_conclusion
                }
            },
            "quality_metrics": {
                "content_coverage": round(calculate_content_coverage(selected_segments, subtitle_segments), 3),
                "visual_quality": round(calculate_visual_quality(selected_segments), 3),
                "audio_quality": round(calculate_audio_quality(selected_segments), 3),
                "overall_score": round(sum(seg['score'] for seg in selected_segments) / len(selected_segments), 3) if selected_segments else 0
            },
            "message": f"æˆåŠŸç”Ÿæˆ{len(selected_segments)}ä¸ªç‰‡æ®µç»„åˆçš„æ™ºèƒ½å‰ªè¾‘è§†é¢‘"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½å¤šç‰‡æ®µå‰ªè¾‘å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½å¤šç‰‡æ®µå‰ªè¾‘å¤±è´¥: {str(e)}")


# å°çº¢ä¹¦å‘å¸ƒç›¸å…³API
@app.post("/xiaohongshu/authorize")
async def xiaohongshu_authorize(auth_code: str):
    """å°çº¢ä¹¦OAuthæˆæƒ"""
    try:
        publisher = get_xiaohongshu_publisher()
        result = await publisher.authorize(auth_code)
        return result
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦æˆæƒå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æˆæƒå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/publish")
async def xiaohongshu_publish(request: XHSPublishReq):
    """å‘å¸ƒå†…å®¹åˆ°å°çº¢ä¹¦"""
    try:
        publisher = get_xiaohongshu_publisher()
        result = await publisher.publish_note(
            title=request.title,
            content=request.content,
            images=request.images,
            tags=request.tags,
            location=request.location,
            privacy=request.privacy
        )
        return result
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦å‘å¸ƒå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å‘å¸ƒå¤±è´¥: {str(e)}")


@app.get("/xiaohongshu/profile")
async def xiaohongshu_get_profile():
    """è·å–ç”¨æˆ·èµ„æ–™"""
    try:
        publisher = get_xiaohongshu_publisher()
        result = await publisher.get_user_profile()
        return result
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·èµ„æ–™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@app.get("/xiaohongshu/note/{note_id}/stats")
async def xiaohongshu_get_note_stats(note_id: str):
    """è·å–ç¬”è®°ç»Ÿè®¡æ•°æ®"""
    try:
        publisher = get_xiaohongshu_publisher()
        result = await publisher.get_note_stats(note_id)
        return result
    except Exception as e:
        logger.error(f"è·å–ç¬”è®°ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


# å›¾ç‰‡è£…é¥°ç›¸å…³API
@app.post("/image/decorate")
async def decorate_image(request: ImageDecorateReq):
    """è£…é¥°å›¾ç‰‡"""
    try:
        decorator = get_image_decorator()
        result = await decorator.decorate_image(
            image_path=request.image_path,
            decorations=request.decorations
        )
        return result
    except Exception as e:
        logger.error(f"å›¾ç‰‡è£…é¥°å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è£…é¥°å¤±è´¥: {str(e)}")


@app.post("/image/decorations/smart")
async def generate_smart_decorations(theme: str, content_type: str, mood: str = "vibrant"):
    """æ™ºèƒ½ç”Ÿæˆè£…é¥°é…ç½®"""
    try:
        decorator = get_image_decorator()
        result = await decorator.generate_smart_decorations(
            theme=theme,
            content_type=content_type,
            mood=mood
        )
        return result
    except Exception as e:
        logger.error(f"æ™ºèƒ½è£…é¥°ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


# æ–‡ä»¶ä¸Šä¼ ç›¸å…³API
@app.post("/upload/photos")
async def upload_photos(files: List[UploadFile] = File(...)):
    """ä¸Šä¼ å¤šå¼ ç…§ç‰‡"""
    try:
        if len(files) > 20:
            raise HTTPException(status_code=400, detail="æœ€å¤šåªèƒ½ä¸Šä¼ 20å¼ ç…§ç‰‡")
        
        uploaded_files = []
        upload_dir = Path("output_data/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not file.content_type or not file.content_type.startswith('image/'):
                raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ {file.filename} ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼")
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = int(time.time())
            file_extension = Path(file.filename).suffix if file.filename else '.jpg'
            unique_filename = f"photo_{timestamp}_{len(uploaded_files)}{file_extension}"
            file_path = upload_dir / unique_filename
            
            # ä¿å­˜æ–‡ä»¶
            content = await file.read()
            with open(file_path, "wb") as f:
                f.write(content)
            
            uploaded_files.append({
                "original_name": file.filename,
                "saved_path": str(file_path),
                "url": f"/static/uploads/{unique_filename}",
                "size": len(content)
            })
            
            logger.info(f"ä¸Šä¼ ç…§ç‰‡æˆåŠŸ: {file.filename} -> {file_path}")
        
        return {
            "status": "success",
            "message": f"æˆåŠŸä¸Šä¼ {len(uploaded_files)}å¼ ç…§ç‰‡",
            "files": uploaded_files
        }
        
    except Exception as e:
        logger.error(f"ç…§ç‰‡ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.post("/upload/video")
async def upload_video(file: UploadFile = File(...)):
    """ä¸Šä¼ è§†é¢‘æ–‡ä»¶"""
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.content_type or not file.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ æœ‰æ•ˆçš„è§†é¢‘æ–‡ä»¶")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶500MB)
        content = await file.read()
        if len(content) > 500 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="è§†é¢‘æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡500MB")
        
        upload_dir = Path("input_data/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        timestamp = int(time.time())
        file_extension = Path(file.filename).suffix if file.filename else '.mp4'
        unique_filename = f"video_{timestamp}{file_extension}"
        file_path = upload_dir / unique_filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as f:
            f.write(content)
        
        logger.info(f"ä¸Šä¼ è§†é¢‘æˆåŠŸ: {file.filename} -> {file_path}")
        
        return {
            "status": "success",
            "message": "è§†é¢‘ä¸Šä¼ æˆåŠŸ",
            "file": {
                "original_name": file.filename,
                "saved_path": str(file_path),
                "size": len(content)
            }
        }
        
    except Exception as e:
        logger.error(f"è§†é¢‘ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

# LLMæ™ºèƒ½å†…å®¹ç”Ÿæˆç›¸å…³API
@app.post("/llm/generate_content")
async def generate_llm_content(request: LLMContentReq):
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ™ºèƒ½å†…å®¹"""
    try:
        llm_service = get_llm_service()
        
        if request.content_type == "xiaohongshu":
            result = await llm_service.generate_xiaohongshu_content(
                theme=request.theme,
                photo_descriptions=request.photo_descriptions,
                highlights=request.highlights,
                feeling=request.feeling,
                style=request.style,
                length=request.length,
                custom_requirements=request.custom_requirements
            )
        else:
            # å…¶ä»–ç±»å‹å†…å®¹ç”Ÿæˆ
            result = await llm_service.generate_pro_content(
                topic=request.theme,
                content_type="complete",
                style=request.style,
                target_audience="general"
            )
        
        logger.info(f"LLMå†…å®¹ç”ŸæˆæˆåŠŸ: {request.theme}")
        return result
        
    except Exception as e:
        logger.error(f"LLMå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/llm/generate_pro_content")
async def generate_pro_llm_content(request: ProContentReq):
    """ç”ŸæˆProåŠŸèƒ½ä¸“ä¸šå†…å®¹"""
    try:
        llm_service = get_llm_service()
        
        result = await llm_service.generate_pro_content(
            topic=request.topic,
            content_type=request.content_type,
            style=request.style,
            target_audience=request.target_audience
        )
        
        logger.info(f"Proå†…å®¹ç”ŸæˆæˆåŠŸ: {request.topic}")
        return result
        
    except Exception as e:
        logger.error(f"Proå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Proå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/llm/status")
async def get_llm_status():
    """è·å–LLMæœåŠ¡çŠ¶æ€"""
    try:
        llm_service = get_llm_service()
        
        return {
            "status": "active" if llm_service.current_api else "fallback",
            "current_api": llm_service.current_api,
            "available_apis": [
                {
                    "name": api_name,
                    "enabled": config["enabled"],
                    "model": config["model"]
                }
                for api_name, config in llm_service.apis.items()
                if config["enabled"]
            ],
            "message": "LLMæœåŠ¡æ­£å¸¸è¿è¡Œ" if llm_service.current_api else "æœªé…ç½®LLM APIï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å¼"
        }
        
    except Exception as e:
        logger.error(f"è·å–LLMçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

# é«˜çº§æ‹¼å›¾ç”Ÿæˆç›¸å…³API
@app.post("/collage/generate_advanced")
async def generate_advanced_collage(request: AdvancedCollageReq):
    """ç”Ÿæˆé«˜çº§æ‹¼å›¾æ•ˆæœ"""
    try:
        collage_generator = get_advanced_collage_generator()
        
        result = await collage_generator.generate_advanced_collage(
            images=request.images,
            title=request.title,
            layout_type=request.layout_type,
            style=request.style,
            color_scheme=request.color_scheme,
            canvas_size=tuple(request.canvas_size),
            add_effects=request.add_effects,
            add_text_overlay=request.add_text_overlay,
            extra_text=request.extra_text or "",
            text_position=request.text_position
        )
        
        logger.info(f"é«˜çº§æ‹¼å›¾ç”ŸæˆæˆåŠŸ: {request.title}")
        return result
        
    except Exception as e:
        logger.error(f"é«˜çº§æ‹¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‹¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/collage/layouts")
async def get_collage_layouts():
    """è·å–å¯ç”¨çš„æ‹¼å›¾å¸ƒå±€"""
    return {
        "layouts": [
            {"id": "dynamic", "name": "æ™ºèƒ½åŠ¨æ€", "description": "æ ¹æ®å›¾ç‰‡æ•°é‡å’Œæ¯”ä¾‹æ™ºèƒ½æ’åˆ—"},
            {"id": "grid", "name": "ç½‘æ ¼å¸ƒå±€", "description": "è§„æ•´çš„ç½‘æ ¼æ’åˆ—"},
            {"id": "magazine", "name": "æ‚å¿—é£æ ¼", "description": "ä¸»å›¾+è¾…åŠ©å›¾çš„æ‚å¿—å¸ƒå±€"},
            {"id": "mosaic", "name": "é©¬èµ›å…‹æ‹¼å›¾", "description": "ä¸è§„åˆ™å¤§å°å’Œä½ç½®çš„è‰ºæœ¯æ‹¼è´´"},
            {"id": "creative", "name": "åˆ›æ„å¸ƒå±€", "description": "åœ†å½¢ã€å¤šè¾¹å½¢ç­‰ç‰¹æ®Šå½¢çŠ¶"},
            {"id": "treemap", "name": "æ ‘åœ°å›¾", "description": "æŒ‰æƒé‡åŒºåŸŸåˆ‡åˆ†çš„æ··åˆå°ºå¯¸æ‹¼è´´"}
        ],
        "styles": [
            {"id": "modern", "name": "ç°ä»£ç®€çº¦", "description": "ç®€æ´ç°ä»£çš„è®¾è®¡é£æ ¼"},
            {"id": "vintage", "name": "å¤å¤æ€€æ—§", "description": "æ¸©æš–çš„å¤å¤è‰²è°ƒå’Œæ•ˆæœ"},
            {"id": "artistic", "name": "è‰ºæœ¯é£æ ¼", "description": "å¼ºè°ƒè‰ºæœ¯æ„Ÿçš„è§†è§‰æ•ˆæœ"},
            {"id": "minimal", "name": "æç®€ä¸»ä¹‰", "description": "æœ€ç®€åŒ–çš„è®¾è®¡å…ƒç´ "}
        ],
        "color_schemes": [
            {"id": "auto", "name": "è‡ªåŠ¨é€‰æ‹©", "description": "æ ¹æ®å›¾ç‰‡å†…å®¹è‡ªåŠ¨é€‰æ‹©é…è‰²"},
            {"id": "warm", "name": "æš–è‰²è°ƒ", "description": "æ¸©æš–çš„æ©™çº¢è‰²ç³»"},
            {"id": "cool", "name": "å†·è‰²è°ƒ", "description": "æ¸…çˆ½çš„è“ç»¿è‰²ç³»"},
            {"id": "monochrome", "name": "å•è‰²è°ƒ", "description": "é»‘ç™½ç°çš„ç»å…¸æ­é…"},
            {"id": "vibrant", "name": "é²œè‰³è‰²å½©", "description": "é«˜é¥±å’Œåº¦çš„é²œè‰³é…è‰²"}
        ]
    }

# æ™ºèƒ½å°é¢ç”Ÿæˆç›¸å…³API
@app.post("/cover/generate")
async def generate_smart_cover(request: SmartCoverReq):
    """ç”Ÿæˆæ™ºèƒ½å°é¢"""
    try:
        generator = get_smart_cover_generator()
        result = await generator.generate_cover(
            images=request.images,
            title=request.title,
            subtitle=request.subtitle,
            layout=request.layout,
            theme=request.theme,
            platform=request.platform,
            custom_config=request.custom_config
        )
        return result
    except Exception as e:
        logger.error(f"æ™ºèƒ½å°é¢ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.get("/cover/templates")
async def get_cover_templates():
    """è·å–å°é¢æ¨¡æ¿åˆ—è¡¨"""
    try:
        generator = get_smart_cover_generator()
        templates = {
            "layouts": list(generator.layout_templates.keys()),
            "themes": list(generator.color_themes.keys()),
            "platforms": list(generator.cover_sizes.keys())
        }
        return {
            "status": "success",
            "data": templates,
            "message": "æ¨¡æ¿åˆ—è¡¨è·å–æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–æ¨¡æ¿åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")

# å°çº¢ä¹¦çº§åˆ«æ‹¼å›¾ç”ŸæˆAPI
@app.post("/xiaohongshu/generate_collage")
async def generate_xiaohongshu_collage(request: XiaohongshuCollageReq):
    """ç”Ÿæˆå°çº¢ä¹¦çº§åˆ«çš„é«˜è´¨é‡æ‹¼å›¾"""
    try:
        xiaohongshu_generator = get_xiaohongshu_collage_generator()
        
        # åˆ›å»ºé…ç½®
        config = CollageConfig(
            width=request.width,
            height=request.height,
            quality=request.quality,
            title_position=request.title_position if request.title_position in ["center_overlay", "top"] else "center_overlay",
            font_path_override=request.font_path
        )
        
        # å¤„ç†è‡ªå®šä¹‰æ–‡æ¡ˆ
        custom_texts = []
        for text_data in request.custom_texts:
            if isinstance(text_data, dict):
                text_config = TextConfig(
                    text=text_data.get("text", ""),
                    font_size=text_data.get("font_size", 80),
                    color=text_data.get("color", "#2C2C2C"),
                    position=tuple(text_data.get("position", [100, 100])),
                    max_width=text_data.get("max_width", 800),
                    font_weight=text_data.get("font_weight", "normal"),
                    shadow=text_data.get("shadow", True),
                    background=text_data.get("background", False),
                    background_color=text_data.get("background_color", "#FFFFFF"),
                    background_opacity=text_data.get("background_opacity", 180)
                )
                custom_texts.append(text_config)
        
        result = await xiaohongshu_generator.generate_xiaohongshu_collage(
            images=request.images,
            title=request.title,
            subtitle=request.subtitle,
            layout=request.layout,
            color_scheme=request.color_scheme,
            custom_texts=custom_texts,
            config=config,
            overlay_texts=request.overlay_texts
        )
        
        logger.info(f"å°çº¢ä¹¦æ‹¼å›¾ç”ŸæˆæˆåŠŸ: {request.title}")
        return result
        
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦æ‹¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‹¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/xiaohongshu/layouts")
async def get_xiaohongshu_layouts():
    """è·å–å°çº¢ä¹¦æ‹¼å›¾å¸ƒå±€é€‰é¡¹"""
    return {
        "layouts": [
            {"id": "magazine_style", "name": "æ‚å¿—é£æ ¼", "description": "ä¸“ä¸šæ‚å¿—çº§åˆ«çš„ä¸å¯¹ç§°å¸ƒå±€"},
            {"id": "grid_modern", "name": "ç°ä»£ç½‘æ ¼", "description": "æ•´é½çš„ç°ä»£ç½‘æ ¼å¸ƒå±€"},
            {"id": "story_flow", "name": "æ•…äº‹æµ", "description": "è®²è¿°æ•…äº‹çš„æµç•…å¸ƒå±€"},
            {"id": "featured_main", "name": "ä¸»å›¾ç‰¹è‰²", "description": "çªå‡ºä¸»å›¾çš„ç‰¹è‰²å¸ƒå±€"},
            {"id": "artistic_collage", "name": "è‰ºæœ¯æ‹¼è´´", "description": "å¯Œæœ‰è‰ºæœ¯æ„Ÿçš„æ‹¼è´´æ•ˆæœ"},
            {"id": "minimal_clean", "name": "ç®€çº¦æ¸…æ–°", "description": "ç®€çº¦æ¸…æ–°çš„å°çº¢ä¹¦é£æ ¼"},
            {"id": "scrapbook", "name": "æ‰‹å¸æ‹¼è´´", "description": "éšæœºæ—‹è½¬/å æ”¾/èƒ¶å¸¦/æ’•è¾¹æ›´æœ‰æ‹¼è´´æ„Ÿ"}
        ],
        "color_schemes": [
            {"id": "xiaohongshu_pink", "name": "å°çº¢ä¹¦ç²‰", "description": "ç»å…¸å°çº¢ä¹¦ç²‰è‰²ä¸»é¢˜"},
            {"id": "fresh_mint", "name": "æ¸…æ–°è–„è·", "description": "æ¸…æ–°çš„è–„è·ç»¿ä¸»é¢˜"},
            {"id": "warm_sunset", "name": "æ¸©æš–å¤•é˜³", "description": "æ¸©æš–çš„å¤•é˜³æ©™è‰²ä¸»é¢˜"},
            {"id": "elegant_gray", "name": "ä¼˜é›…ç°è°ƒ", "description": "ä¼˜é›…çš„ç°è‰²ä¸»é¢˜"}
        ],
        "quality_options": [
            {"value": 85, "name": "æ ‡å‡†è´¨é‡", "description": "é€‚åˆå¿«é€Ÿåˆ†äº«"},
            {"value": 95, "name": "é«˜è´¨é‡", "description": "æ¨èç”¨äºå°çº¢ä¹¦å‘å¸ƒ"},
            {"value": 98, "name": "è¶…é«˜è´¨é‡", "description": "æœ€ä½³è´¨é‡ï¼Œæ–‡ä»¶è¾ƒå¤§"}
        ],
        "size_presets": [
            {"width": 1080, "height": 1080, "name": "å°çº¢ä¹¦æ ‡å‡†", "description": "1080x1080 æ ‡å‡†æ­£æ–¹å½¢"},
            {"width": 2160, "height": 2160, "name": "4Ké«˜æ¸…", "description": "2160x2160 è¶…é«˜æ¸…"},
            {"width": 1080, "height": 1350, "name": "å°çº¢ä¹¦ç«–ç‰ˆ", "description": "1080x1350 ç«–ç‰ˆæ¯”ä¾‹"}
        ]
    }

@app.post("/xiaohongshu/edit_text")
async def edit_collage_text(request: EditableTextReq):
    """ç¼–è¾‘æ‹¼å›¾ä¸­çš„æ–‡æ¡ˆ"""
    try:
        # è¿™é‡Œå¯ä»¥å®ç°æ–‡æ¡ˆç¼–è¾‘åŠŸèƒ½
        # ç›®å‰è¿”å›æˆåŠŸå“åº”ï¼Œå®é™…å®ç°éœ€è¦å­˜å‚¨å’Œæ›´æ–°æ‹¼å›¾æ•°æ®
        return {
            "success": True,
            "message": "æ–‡æ¡ˆç¼–è¾‘æˆåŠŸ",
            "collage_id": request.collage_id,
            "text_id": request.text_id,
            "new_text": request.new_text
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡ˆç¼–è¾‘å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç¼–è¾‘å¤±è´¥: {str(e)}")


# å•é¡µæ¸²æŸ“ï¼ˆå•å›¾æˆ–æ‹¼å›¾ï¼‰
class PageRenderReq(BaseModel):
    images: List[str]
    mode: str = "single"  # single | collage
    layout: str = "scrapbook"  # å½“ mode=collage æ—¶ç”Ÿæ•ˆ
    title: str = ""
    subtitle: str = ""
    title_position: str = "center_overlay"
    overlay_texts: List[Dict[str, Any]] = []
    width: int = 1080
    height: int = 1080
    quality: int = 95

@app.post("/xiaohongshu/render_page")
async def render_xhs_page(request: PageRenderReq):
    try:
        xh = get_xiaohongshu_collage_generator()
        cfg = CollageConfig(width=request.width, height=request.height, quality=request.quality,
                            title_position=request.title_position)
        if request.mode == "single":
            from PIL import Image as PILImage
            img_paths = request.images
            if not img_paths:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘å›¾ç‰‡")
            canvas = PILImage.new('RGB', (cfg.width, cfg.height), '#FFFFFF')
            p = Path(img_paths[0])
            if not p.exists():
                p = Path("output_data")/img_paths[0]
            im = PILImage.open(p)
            if im.mode != 'RGB':
                im = im.convert('RGB')
            max_w = int(cfg.width*0.82)
            max_h = int(cfg.height*0.68)
            ratio = min(max_w/im.width, max_h/im.height)
            new_size = (max(1,int(im.width*ratio)), max(1,int(im.height*ratio)))
            im = im.resize(new_size, PILImage.Resampling.LANCZOS)
            x = (cfg.width - im.width)//2
            y = (cfg.height - im.height)//2 + 50
            canvas.paste(im, (x,y))
            # æ ‡é¢˜ä¸æ–‡æ¡ˆ
            xh.config_font_override = None
            colors = xh.color_schemes['xiaohongshu_pink']
            canvas = xh._add_main_texts(canvas, request.title, request.subtitle, colors, cfg)
            if request.overlay_texts:
                canvas = xh._draw_text_blocks(canvas, request.overlay_texts, colors, cfg)
            out_path, b64 = await xh._save_high_quality_image(canvas, cfg)
            return {"success": True, "image_path": str(out_path), "base64_data": b64, "mode": request.mode}
        else:
            res = await xh.generate_xiaohongshu_collage(
                images=request.images,
                title=request.title,
                subtitle=request.subtitle,
                layout=request.layout,
                color_scheme='xiaohongshu_pink',
                custom_texts=[],
                config=cfg,
                overlay_texts=request.overlay_texts
            )
            return res
    except Exception as e:
        logger.error(f"æ¸²æŸ“é¡µé¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸²æŸ“å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    from core.config import API_HOST, API_PORT
    
    logger.info(f"Starting API server on {API_HOST}:{API_PORT}")
    uvicorn.run(app, host=API_HOST, port=API_PORT)
