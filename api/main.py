import json
import os
import subprocess
import time
from pathlib import Path
from datetime import datetime
from urllib.request import urlretrieve
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from fastapi import FastAPI, Request, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, field_validator
from loguru import logger
from starlette.middleware.base import BaseHTTPMiddleware
import uuid

# Import configuration and validation functions
from core.config import (
    SEGMENT_PROMPT, CAPTIONS_PROMPT, GEMINI_API_BASE, CUT_API_BASE,
    UPLOAD_BUCKET, UPLOAD_BASE_URL, VIDEO_FPS, VIDEO_CRF, AUDIO_BITRATE,
    validate_video_extension, validate_file_size, MIN_CLIP_DURATION, MAX_CLIP_DURATION
)
from core.settings import settings
from core.smart_clipping import get_smart_segments, analyze_video_intelligence
from core.whisper_asr import get_asr_service, transcribe_video_file
from core.semantic_analysis import get_semantic_analyzer, analyze_transcription_semantics
from core.asr_smart_clipping import get_asr_smart_engine, select_segments_with_asr
from core.xiaohongshu_pipeline import (
    get_photo_ranking_service, get_storyline_generator, get_draft_generator
)
from core.subtitle_service import get_subtitle_generator, get_cover_service
from core.export_service import get_export_service
from core.advanced_photo_ranking import get_advanced_photo_service
from core.semantic_highlights import get_semantic_highlight_detector
from core.personalized_writing import get_personalized_writing_service
from core.smart_cover_design import get_smart_cover_designer
from core.audio_processing import get_audio_processing_service
from routers.tasks import router as tasks_router

# Setup logging
logger.add("logs/api.log", rotation="10 MB", level="INFO")
logger.add("logs/api.jsonl", rotation="10 MB", level="INFO", serialize=True)

app = FastAPI(
    title="AI Video Clipper API",
    description="æ™ºèƒ½çŸ­è§†é¢‘è‡ªåŠ¨åˆ‡ç‰‡å’Œæ–‡æ¡ˆç”ŸæˆæœåŠ¡",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request ID middleware for tracing
class RequestIDMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        req_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        request.state.request_id = req_id
        response = await call_next(request)
        response.headers["X-Request-ID"] = req_id
        return response

app.add_middleware(RequestIDMiddleware)

# Create logs directory if it doesn't exist
Path("logs").mkdir(exist_ok=True)

# Routers
app.include_router(tasks_router)

# Exception handler for validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    logger.error(f"Validation error: {exc.errors()}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "message": "è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥"},
    )

# --- Pydantic Models with Validation ---
class SegmentReq(BaseModel):
    transcript: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    
    @field_validator('transcript')
    @classmethod
    def validate_transcript(cls, v):
        if not v.strip():
            raise ValueError('å­—å¹•å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()
    
    @field_validator('min_sec', 'max_sec')
    @classmethod
    def validate_duration(cls, v):
        if v <= 0:
            raise ValueError('æ—¶é•¿å¿…é¡»å¤§äº0')
        return v

class CaptionsReq(BaseModel):
    topic: str = "AI and Technology"
    transcript: str
    clip_text: str
    
    @field_validator('topic', 'transcript', 'clip_text')
    @classmethod
    def validate_text_fields(cls, v):
        if not v.strip():
            raise ValueError('æ–‡æœ¬å­—æ®µä¸èƒ½ä¸ºç©º')
        return v.strip()

class CutReq(BaseModel):
    src: str
    start: str
    end: str
    out: str
    
    @field_validator('src')
    @classmethod
    def validate_src_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æºæ–‡ä»¶ä¸å­˜åœ¨: {v}')
        if not validate_video_extension(v):
            raise ValueError(f'ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼: {v}')
        return v
    
    @field_validator('start', 'end')
    @classmethod
    def validate_time_format(cls, v):
        import re
        if not re.match(r'^\d{2}:\d{2}(:\d{2})?$', v):
            raise ValueError(f'æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º mm:ss æˆ– hh:mm:ss: {v}')
        return v

class BurnSubReq(BaseModel):
    src: str
    srt: str
    out: str
    
    @field_validator('src')
    @classmethod
    def validate_src_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æºæ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v
    
    @field_validator('srt')
    @classmethod
    def validate_srt_file(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v

class UploadReq(BaseModel):
    path: str
    bucket: str = UPLOAD_BUCKET
    
    @field_validator('path')
    @classmethod
    def validate_path(cls, v):
        if not os.path.exists(v):
            raise ValueError(f'æ–‡ä»¶ä¸å­˜åœ¨: {v}')
        return v

# --- Utility Functions ---
def normalize_time(time_str: str) -> str:
    """Normalize time format to hh:mm:ss"""
    parts = time_str.split(':')
    if len(parts) == 2:
        return f"00:{time_str}"
    return time_str

def hms_to_seconds(hms: str) -> float:
    parts = list(map(int, hms.split(':')))
    if len(parts) == 2:
        return parts[0] * 60 + parts[1]
    return parts[0] * 3600 + parts[1] * 60 + parts[2]

def seconds_to_hms(t: float) -> str:
    h = int(t // 3600)
    m = int((t % 3600) // 60)
    s = int(t % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def safe_run_ffmpeg(cmd: List[str], timeout: int = 300) -> Dict[str, Any]:
    """Safely run ffmpeg command with timeout and error handling"""
    try:
        logger.info(f"Running FFmpeg command: {' '.join(cmd)}")
        start_time = time.time()
        
        process = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            timeout=timeout
        )
        
        duration = time.time() - start_time
        logger.info(f"FFmpeg completed in {duration:.2f}s with return code: {process.returncode}")
        
        if process.returncode != 0:
            logger.error(f"FFmpeg failed: {process.stderr}")
            raise HTTPException(
                status_code=500, 
                detail=f"è§†é¢‘å¤„ç†å¤±è´¥: {process.stderr}"
            )
        
        return {
            "code": process.returncode,
            "stdout": process.stdout,
            "stderr": process.stderr,
            "duration": duration
        }
        
    except subprocess.TimeoutExpired:
        logger.error("FFmpeg command timed out")
        raise HTTPException(status_code=504, detail="è§†é¢‘å¤„ç†è¶…æ—¶")
    except Exception as e:
        logger.error(f"FFmpeg execution error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘å¤„ç†å¼‚å¸¸: {str(e)}")

def black_fraction_in_segment(path: str, start_hms: str, duration_s: float) -> float:
    """Approximate fraction of time that is black in the given segment using ffmpeg blackdetect.
    Use a center ROI to ignore letterboxing borders.
    """
    try:
        # Limit runtime to short duration analysis
        cmd = [
            "ffmpeg", "-hide_banner", "-ss", start_hms, "-t", f"{max(0.5, duration_s):.2f}",
            "-i", path,
            "-vf", "crop=in_w*0.9:in_h*0.9:(in_w-out_w)/2:(in_h-out_h)/2,blackdetect=d=0.3:pic_th=0.98",
            "-an", "-f", "null", "-"
        ]
        completed = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", timeout=60)
        out = (completed.stdout or "") + "\n" + (completed.stderr or "")
        import re
        # Parse last blackdetect line: black_start:.. black_end:.. black_duration:..
        matches = re.findall(r"black_duration:([0-9]+\.?[0-9]*)", out)
        if not matches:
            return 0.0
        total_black = sum(float(x) for x in matches)
        return min(max(total_black / max(duration_s, 0.001), 0.0), 1.0)
    except Exception:
        return 0.0

def silence_fraction_in_segment(path: str, start_hms: str, duration_s: float) -> float:
    """Approximate fraction of time that is silence in the given segment using silencedetect."""
    try:
        cmd = [
            "ffmpeg", "-hide_banner", "-ss", start_hms, "-t", f"{max(0.5, duration_s):.2f}",
            "-i", path, "-af", "silencedetect=noise=-35dB:d=0.3", "-f", "null", "-"
        ]
        completed = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", timeout=60)
        out = (completed.stdout or "") + "\n" + (completed.stderr or "")
        import re
        sil_starts = [float(x) for x in re.findall(r"silence_start:([0-9]+\.?[0-9]*)", out)]
        sil_ends = [float(x) for x in re.findall(r"silence_end:([0-9]+\.?[0-9]*)", out)]
        # Pair up starts and ends; if unmatched end, ignore
        total_sil = 0.0
        for i, s in enumerate(sil_starts):
            if i < len(sil_ends):
                total_sil += max(0.0, sil_ends[i] - s)
        return min(max(total_sil / max(duration_s, 0.001), 0.0), 1.0)
    except Exception:
        return 0.0

def ffprobe_json(path: str, select_streams: str = "") -> Dict[str, Any]:
    """Return ffprobe json info for given path (optionally select streams)."""
    probe_cmd = [
        "ffprobe", "-v", "error", "-print_format", "json", "-show_streams"
    ]
    if select_streams:
        probe_cmd += ["-select_streams", select_streams]
    probe_cmd += [path]
    try:
        completed = subprocess.run(probe_cmd, capture_output=True, text=True, encoding="utf-8")
        if completed.returncode != 0:
            return {"streams": []}
        return json.loads(completed.stdout or "{}")
    except Exception:
        return {"streams": []}

def get_duration_seconds(path: str) -> float:
    info = ffprobe_json(path)
    for s in info.get("streams", []):
        if s.get("codec_type") == "video":
            try:
                return float(s.get("duration")) if s.get("duration") else 0.0
            except Exception:
                continue
    return 0.0

def try_python_asr_to_srt(src: str, out_srt: str) -> bool:
    """Try to run ASR via Python libs (openai-whisper or faster_whisper). Returns True on success."""
    # openai-whisper
    try:
        import whisper  # type: ignore
        model = whisper.load_model("small")
        result = model.transcribe(src, language="zh", task="transcribe")
        segments = result.get("segments", [])
        with open(out_srt, "w", encoding="utf-8") as f:
            for i, seg in enumerate(segments, 1):
                start = float(seg.get("start", 0.0))
                end = float(seg.get("end", 0.0))
                def fmt(t: float) -> str:
                    h = int(t // 3600); m = int((t % 3600) // 60); s = int(t % 60); ms = int((t - int(t)) * 1000)
                    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
                text = str(seg.get("text", "")).strip()
                if not text:
                    continue
                f.write(f"{i}\n{fmt(start)} --> {fmt(end)}\n{text}\n\n")
        return True
    except Exception:
        pass
    # faster-whisper
    try:
        from faster_whisper import WhisperModel  # type: ignore
        model = WhisperModel("small", device="cpu")
        segments, _ = model.transcribe(src, language="zh")
        def fmt(t: float) -> str:
            h = int(t // 3600); m = int((t % 3600) // 60); s = int(t % 60); ms = int((t - int(t)) * 1000)
            return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
        with open(out_srt, "w", encoding="utf-8") as f:
            for i, seg in enumerate(segments, 1):
                text = (seg.text or "").strip()
                if not text:
                    continue
                f.write(f"{i}\n{fmt(seg.start)} --> {fmt(seg.end)}\n{text}\n\n")
        return True
    except Exception:
        return False

def parse_srt_simple(path: str) -> List[Dict[str, str]]:
    """Parse SRT to list of dicts with start, end, text (mm:ss granularity)."""
    if not os.path.exists(path):
        return []
    raw = Path(path).read_text(encoding="utf-8", errors="ignore")
    items: List[Dict[str, str]] = []
    for block in [b.strip() for b in raw.split("\n\n") if b.strip()]:
        lines = [l.strip() for l in block.splitlines() if l.strip()]
        timeline = next((l for l in lines if "-->" in l), "")
        if not timeline:
            continue
        try:
            start, end = [t.strip().replace(",", ":")[0:8] for t in timeline.split("-->")]
        except Exception:
            continue
        txt_lines = []
        seen = False
        for l in lines:
            if seen:
                txt_lines.append(l)
            if l == timeline:
                seen = True
        text = " ".join(txt_lines).strip()
        items.append({"start": start if len(start)==8 else f"00:{start}", "end": end if len(end)==8 else f"00:{end}", "text": text})
    return items

def score_text_heuristic(t: str) -> float:
    score = 0.0
    if "ï¼" in t or "!" in t:
        score += 2
    if "ï¼Ÿ" in t or "?" in t:
        score += 1.2
    if any(ch.isdigit() for ch in t):
        score += 1.0
    for w in ["äº®ç‚¹","å…³é”®","æ€»ç»“","æ­¥éª¤","å› æ­¤","æ‰€ä»¥","æ¡ˆä¾‹","æ³¨æ„","åšæ³•","æŠ€å·§","æ­ç§˜","æå‡","å¯¹æ¯”","æ¨è"]:
        if w in t:
            score += 0.8
    score += min(len(t) / 20.0, 1.5)
    return score

def select_windows_from_srt(srt_items: List[Dict[str, str]], min_sec: int, max_sec: int, top_k: int = 3) -> List[Dict[str, str]]:
    def to_s(ts: str) -> int:
        parts = list(map(int, ts.split(":")))
        if len(parts) == 2:
            return parts[0]*60 + parts[1]
        return parts[0]*3600 + parts[1]*60 + parts[2]
    n = len(srt_items)
    windows: List[Tuple[float,int,int]] = []
    for i in range(n):
        start_s = to_s(srt_items[i]["start"]) 
        acc = 0.0
        j = i
        while j < n:
            acc += score_text_heuristic(srt_items[j]["text"]) 
            end_s = to_s(srt_items[j]["end"]) 
            dur = end_s - start_s
            if dur >= min_sec:
                windows.append((acc, i, j))
            if dur >= max_sec:
                break
            j += 1
    windows.sort(key=lambda x: x[0], reverse=True)
    chosen: List[Tuple[int,int]] = []
    for _, i, j in windows:
        if len(chosen) >= top_k:
            break
        ok = True
        for ci, cj in chosen:
            if not (j < ci or i > cj):
                ok = False
                break
        if ok:
            chosen.append((i, j))
    clips: List[Dict[str,str]] = []
    for i, j in chosen:
        clips.append({"start": srt_items[i]["start"], "end": srt_items[j]["end"], "reason": "highlight"})
    return clips

# --- Gemini Mock Function with Error Handling ---
def run_gemini(prompt: str, max_retries: int = 3) -> str:
    """Mock Gemini API call with retry logic"""
    logger.info(f"Mock Gemini call with prompt length: {len(prompt)}")
    
    for attempt in range(max_retries):
        try:
            if "åˆ‡ç‰‡ä¸“å®¶" in prompt:
                return '''{"clips": [{"start": "00:15","end": "00:48","reason": "ä»‹ç»äº†é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡å’ŒæŠ€æœ¯é€‰å‹ã€‚"},{"start": "01:10","end": "01:55","reason": "æ¼”ç¤ºäº†å…³é”®åŠŸèƒ½å¹¶è§£é‡Šäº†å…¶ç”¨æˆ·ä»·å€¼ã€‚"}]}'''
            else:
                return '''{"title": "AIå¦‚ä½•é¢ è¦†æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Ÿ","hashtags": ["#AI", "#ç§‘æŠ€æ”¹å˜ç”Ÿæ´»"],"desc": "è¿™ä¸ªè§†é¢‘çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œè®©ä½ ä¸‰åˆ†é’Ÿçœ‹æ‡‚äººå·¥æ™ºèƒ½çš„çœŸæ­£å¨åŠ›ï¼"}'''
        except Exception as e:
            logger.error(f"Gemini call attempt {attempt + 1} failed: {str(e)}")
            if attempt == max_retries - 1:
                raise HTTPException(status_code=500, detail="AIæœåŠ¡è°ƒç”¨å¤±è´¥")
            time.sleep(1)  # Wait before retry

# --- API Endpoints ---
@app.get("/")
async def root():
    """Health check endpoint"""
    return {"message": "AI Video Clipper API is running", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """è¯¦ç»†å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy", 
        "timestamp": int(time.time()),
        "services": {
            "api": "running",
            "smart_clipping": "available"
        }
    }



@app.post("/segment")
async def segment(req: SegmentReq):
    """AIæ™ºèƒ½åˆ‡ç‰‡åˆ†æ"""
    try:
        logger.info(f"Processing segment request with duration: {req.min_sec}-{req.max_sec}s")
        
        prompt = SEGMENT_PROMPT.format(
            txt=req.transcript, 
            min_sec=req.min_sec, 
            max_sec=req.max_sec
        )
        
        result = run_gemini(prompt)
        parsed_result = json.loads(result[result.find("{"):result.rfind("}")+1])
        
        logger.info(f"Generated {len(parsed_result.get('clips', []))} clips")
        return parsed_result
        
    except json.JSONDecodeError:
        logger.error("Failed to parse Gemini response as JSON")
        raise HTTPException(status_code=500, detail="AIåˆ†æç»“æœæ ¼å¼é”™è¯¯")
    except Exception as e:
        logger.error(f"Segment processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ‡ç‰‡åˆ†æå¤±è´¥: {str(e)}")

@app.post("/captions")
async def captions(req: CaptionsReq):
    """AIæ–‡æ¡ˆç”Ÿæˆ"""
    try:
        logger.info(f"Generating captions for topic: {req.topic}")
        
        prompt = CAPTIONS_PROMPT.format(
            topic=req.topic, 
            clip_text=req.clip_text, 
            transcript=req.transcript
        )
        
        result = run_gemini(prompt)
        parsed_result = json.loads(result[result.find("{"):result.rfind("}")+1])
        
        logger.info(f"Generated captions: {parsed_result.get('title', '')}")
        return parsed_result
        
    except json.JSONDecodeError:
        logger.error("Failed to parse captions response as JSON")
        raise HTTPException(status_code=500, detail="æ–‡æ¡ˆç”Ÿæˆç»“æœæ ¼å¼é”™è¯¯")
    except Exception as e:
        logger.error(f"Caption generation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/cut916")
async def cut916(req: CutReq):
    """ç”Ÿæˆ9:16ç«–å±è§†é¢‘"""
    try:
        logger.info(f"Cutting video: {req.src} ({req.start} - {req.end})")
        
        # Normalize time format
        start_time = normalize_time(req.start)
        end_time = normalize_time(req.end)
        
        # Robust 9:16 pipeline using expressions (no FOAR option):
        # - If input is wider than 9:16, scale height to 1920 and width proportional; else scale width to 1080
        # - Then center crop to exactly 1080x1920; set pixel format and SAR for compatibility
        vf_filters = (
            "scale="
            "if(gte(iw/ih\,1080/1920)\,-2\,1080):"
            "if(gte(iw/ih\,1080/1920)\,1920\,-2),"
            "crop=1080:1920,format=yuv420p,setsar=1:1"
        )

        cmd = [
            "ffmpeg", "-y",
            # Use output seeking (place -ss/-to after -i) to keep A/V sync and audio reliably
            "-i", req.src,
            "-ss", start_time,
            "-to", end_time,
            "-vf", vf_filters,
            "-r", str(VIDEO_FPS),
            "-pix_fmt", "yuv420p",
            # Explicitly map first video and (optional) first audio stream to avoid accidental drops
            "-map", "0:v:0",
            "-map", "0:a?",
            "-c:v", "libx264",
            "-preset", "veryfast",
            "-crf", str(VIDEO_CRF),
            "-c:a", "aac",
            "-b:a", AUDIO_BITRATE,
            "-movflags", "+faststart",
            req.out
        ]
        
        result = safe_run_ffmpeg(cmd)
        result["out"] = req.out
        
        logger.info(f"Successfully created 9:16 video: {req.out}")
        return result
        
    except Exception as e:
        logger.error(f"9:16 video cut error: {str(e)}")
        # Clean up partial output file if it exists
        if os.path.exists(req.out):
            os.remove(req.out)
        raise HTTPException(status_code=500, detail=f"9:16è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/burnsub")
async def burnsub(req: BurnSubReq):
    """çƒ§å½•å­—å¹•åˆ°è§†é¢‘"""
    try:
        logger.info(f"Burning subtitles: {req.srt} -> {req.src}")
        
        cmd = [
            "ffmpeg", "-y",
            "-i", req.src,
            "-vf", f"subtitles={req.srt}:force_style='Fontsize=28'",
            "-c:a", "copy",
            req.out
        ]
        
        result = safe_run_ffmpeg(cmd)
        result["out"] = req.out
        
        logger.info(f"Successfully burned subtitles: {req.out}")
        return result
        
    except Exception as e:
        logger.error(f"Subtitle burning error: {str(e)}")
        # Clean up partial output file if it exists
        if os.path.exists(req.out):
            os.remove(req.out)
        raise HTTPException(status_code=500, detail=f"å­—å¹•çƒ§å½•å¤±è´¥: {str(e)}")

@app.post("/upload")
async def upload(req: UploadReq):
    """æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ åˆ°äº‘å­˜å‚¨"""
    try:
        if not os.path.exists(req.path):
            logger.error(f"File not found: {req.path}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        file_name = os.path.basename(req.path)
        file_size = os.path.getsize(req.path)
        
        if not validate_file_size(file_size):
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°è¶…å‡ºé™åˆ¶")
        
        # Mock upload to cloud storage
        mock_url = f"{UPLOAD_BASE_URL}/{req.bucket}/{file_name}"
        logger.info(f"Mock upload: {req.path} -> {mock_url}")
        
        return {
            "url": mock_url,
            "key": file_name,
            "size": file_size,
            "bucket": req.bucket
        }
        
    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")


# --- New: Intro-style auto highlights from URL ---
class URLIntroReq(BaseModel):
    url: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    want_asr: bool = True
    top_k: int = 3
    output: str = str(Path("output_data") / "intro_916.mp4")
    smart_mode: bool = True  # å¯ç”¨æ™ºèƒ½é€‰æ®µæ¨¡å¼

class VideoAnalysisReq(BaseModel):
    url: str
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v

class ASRTranscribeReq(BaseModel):
    url: str
    language: Optional[str] = None  # è¯­è¨€ä»£ç ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹
    subtitle_format: str = "srt"    # å­—å¹•æ ¼å¼: srt, vtt, txt, json, none
    task: str = "transcribe"        # transcribe æˆ– translate
    model_size: str = "base"        # tiny, base, small, medium, large
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('subtitle_format')
    @classmethod
    def validate_subtitle_format(cls, v):
        valid_formats = ['srt', 'vtt', 'txt', 'json', 'none']
        if v not in valid_formats:
            raise ValueError(f'å­—å¹•æ ¼å¼å¿…é¡»æ˜¯: {", ".join(valid_formats)}')
        return v
    
    @field_validator('task')
    @classmethod
    def validate_task(cls, v):
        if v not in ['transcribe', 'translate']:
            raise ValueError('ä»»åŠ¡ç±»å‹å¿…é¡»æ˜¯: transcribe æˆ– translate')
        return v
    
    @field_validator('model_size')
    @classmethod
    def validate_model_size(cls, v):
        valid_sizes = ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']
        if v not in valid_sizes:
            raise ValueError(f'æ¨¡å‹å¤§å°å¿…é¡»æ˜¯: {", ".join(valid_sizes)}')
        return v

class AudioExtractionReq(BaseModel):
    url: str
    sample_rate: int = 16000
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('sample_rate')
    @classmethod
    def validate_sample_rate(cls, v):
        if v not in [8000, 16000, 22050, 44100, 48000]:
            raise ValueError('é‡‡æ ·ç‡å¿…é¡»æ˜¯: 8000, 16000, 22050, 44100, 48000')
        return v

class SemanticAnalysisReq(BaseModel):
    text: str
    include_keywords: bool = True
    include_sentiment: bool = True
    include_topics: bool = True
    include_quality: bool = True
    
    @field_validator('text')
    @classmethod
    def validate_text(cls, v):
        if not v.strip():
            raise ValueError('æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()

class ASRSmartClippingReq(BaseModel):
    url: str
    min_sec: int = MIN_CLIP_DURATION
    max_sec: int = MAX_CLIP_DURATION
    count: int = 1
    model_size: str = "base"
    language: Optional[str] = None
    output_prefix: str = "asr_smart_clip"
    
    @field_validator('url')
    @classmethod
    def validate_url(cls, v):
        if not v.strip():
            raise ValueError('URLä¸èƒ½ä¸ºç©º')
        v = v.strip()
        if not v.startswith("http") and not v.startswith("file:"):
            raise ValueError('ä»…æ”¯æŒ http/https/file URL')
        return v
    
    @field_validator('min_sec', 'max_sec')
    @classmethod
    def validate_duration(cls, v):
        if v < 5 or v > 300:
            raise ValueError('æ—¶é•¿å¿…é¡»åœ¨5-300ç§’ä¹‹é—´')
        return v
    
    @field_validator('count')
    @classmethod
    def validate_count(cls, v):
        if v < 1 or v > 5:
            raise ValueError('ç‰‡æ®µæ•°é‡å¿…é¡»åœ¨1-5ä¹‹é—´')
        return v

class PhotoRankingReq(BaseModel):
    photos: List[str]
    top_k: int = 15
    
    @field_validator('photos')
    @classmethod
    def validate_photos(cls, v):
        if not v:
            raise ValueError('ç…§ç‰‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        return v
    
    @field_validator('top_k')
    @classmethod
    def validate_top_k(cls, v):
        if v < 1 or v > 50:
            raise ValueError('é€‰æ‹©æ•°é‡å¿…é¡»åœ¨1-50ä¹‹é—´')
        return v

class StorylineReq(BaseModel):
    transcript_mmss: List[Dict]
    notes: str
    city: str = ""
    date: str = ""
    style: str = "æ²»æ„ˆ"
    
    @field_validator('transcript_mmss')
    @classmethod
    def validate_transcript(cls, v):
        if not v:
            raise ValueError('è½¬å½•æ–‡æœ¬ä¸èƒ½ä¸ºç©º')
        return v
    
    @field_validator('style')
    @classmethod
    def validate_style(cls, v):
        valid_styles = ['æ²»æ„ˆ', 'ä¸“ä¸š', 'è¸©é›·']
        if v not in valid_styles:
            raise ValueError(f'é£æ ¼å¿…é¡»æ˜¯: {", ".join(valid_styles)}')
        return v

class XHSDraftReq(BaseModel):
    storyline: Dict
    brand_tone: str = "æ²»æ„ˆ"
    constraints: Optional[Dict] = None
    
    @field_validator('storyline')
    @classmethod
    def validate_storyline(cls, v):
        if not v:
            raise ValueError('æ•…äº‹çº¿æ•°æ®ä¸èƒ½ä¸ºç©º')
        return v

class SubtitleReq(BaseModel):
    clips: List[Dict]
    transcript_mmss: List[Dict]
    style: str = "å£è¯­"
    
    @field_validator('style')
    @classmethod
    def validate_style(cls, v):
        valid_styles = ['å£è¯­', 'ä¹¦é¢', 'å¯çˆ±']
        if v not in valid_styles:
            raise ValueError(f'å­—å¹•é£æ ¼å¿…é¡»æ˜¯: {", ".join(valid_styles)}')
        return v

class CoverReq(BaseModel):
    clips: List[Dict]
    photos_topk: List[Dict]
    title: str = ""

class ExportReq(BaseModel):
    pipeline_result: Dict
    export_format: str = "zip"
    include_source: bool = False
    
    @field_validator('export_format')
    @classmethod
    def validate_format(cls, v):
        valid_formats = ['json', 'zip', 'folder']
        if v not in valid_formats:
            raise ValueError(f'å¯¼å‡ºæ ¼å¼å¿…é¡»æ˜¯: {", ".join(valid_formats)}')
        return v

class XHSPipelineReq(BaseModel):
    video_url: str
    photos: List[str] = []
    notes: str = ""
    city: str = ""
    style: str = "æ²»æ„ˆ"
    model_size: str = "base"
    export_format: str = "zip"
    
    @field_validator('video_url')
    @classmethod
    def validate_video_url(cls, v):
        if not v.strip():
            raise ValueError('è§†é¢‘URLä¸èƒ½ä¸ºç©º')
        return v.strip()

# ProåŠŸèƒ½APIæ¨¡å‹
class AdvancedPhotoRankingReq(BaseModel):
    photos: List[str]
    top_k: int = 15
    context: Optional[Dict] = None
    use_clip: bool = True
    use_aesthetic_model: bool = True
    
    @field_validator('photos')
    @classmethod
    def validate_photos(cls, v):
        if not v:
            raise ValueError('ç…§ç‰‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        return v

class SemanticHighlightsReq(BaseModel):
    transcript_segments: List[Dict]
    context: Optional[Dict] = None
    min_score: float = 0.3
    max_highlights: int = 10
    
    @field_validator('transcript_segments')
    @classmethod
    def validate_segments(cls, v):
        if not v:
            raise ValueError('è½¬å½•ç‰‡æ®µä¸èƒ½ä¸ºç©º')
        return v

class PersonalizedWritingReq(BaseModel):
    user_id: str
    content_data: Dict
    style_override: Optional[str] = None
    learn_from_history: bool = True
    
    @field_validator('user_id')
    @classmethod
    def validate_user_id(cls, v):
        if not v.strip():
            raise ValueError('ç”¨æˆ·IDä¸èƒ½ä¸ºç©º')
        return v.strip()

class UserStyleLearningReq(BaseModel):
    user_id: str
    content_samples: List[Dict]
    
    @field_validator('content_samples')
    @classmethod
    def validate_samples(cls, v):
        if not v:
            raise ValueError('å†…å®¹æ ·æœ¬ä¸èƒ½ä¸ºç©º')
        return v

class SmartCoverDesignReq(BaseModel):
    clips: List[Dict]
    photos: List[Dict] = []
    title: str
    style: str = "æ²»æ„ˆ"
    template: str = "minimal"
    
    @field_validator('title')
    @classmethod
    def validate_title(cls, v):
        if not v.strip():
            raise ValueError('æ ‡é¢˜ä¸èƒ½ä¸ºç©º')
        return v.strip()

class AudioProcessingReq(BaseModel):
    video_path: str
    style: str = "æ²»æ„ˆ"
    enhance_speech: bool = True
    add_bgm: bool = True
    bgm_volume: float = 0.3
    
    @field_validator('video_path')
    @classmethod
    def validate_video_path(cls, v):
        if not v.strip():
            raise ValueError('è§†é¢‘è·¯å¾„ä¸èƒ½ä¸ºç©º')
        return v.strip()
    
    @field_validator('bgm_volume')
    @classmethod
    def validate_bgm_volume(cls, v):
        if v < 0 or v > 1:
            raise ValueError('BGMéŸ³é‡å¿…é¡»åœ¨0-1ä¹‹é—´')
        return v

class XHSProPipelineReq(BaseModel):
    video_url: str
    photos: List[str] = []
    notes: str = ""
    city: str = ""
    style: str = "æ²»æ„ˆ"
    user_id: Optional[str] = None
    model_size: str = "base"
    export_format: str = "zip"
    
    # ProåŠŸèƒ½å¼€å…³
    use_advanced_photo_ranking: bool = True
    use_semantic_highlights: bool = True
    use_personalized_writing: bool = False
    use_smart_cover: bool = True
    use_audio_enhancement: bool = True
    
    @field_validator('video_url')
    @classmethod
    def validate_video_url(cls, v):
        if not v.strip():
            raise ValueError('è§†é¢‘URLä¸èƒ½ä¸ºç©º')
        return v.strip()


@app.post("/analyze_video")
async def analyze_video(req: VideoAnalysisReq):
    """åˆ†æè§†é¢‘å†…å®¹ï¼Œè¿”å›æ™ºèƒ½åŒ–åˆ†æç»“æœ"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½è§†é¢‘
        ts = int(time.time())
        dl_path = str(Path("input_data/downloads") / f"analysis_{ts}.mp4")
        
        if req.url.startswith("file:"):
            # æœ¬åœ°æ–‡ä»¶
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            dl_path = local_path
        else:
            # URLä¸‹è½½ (ç®€åŒ–ç‰ˆï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ä¸‹è½½é€»è¾‘)
            from urllib.request import urlretrieve
            urlretrieve(req.url, dl_path)
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†æ
        logger.info(f"Starting intelligent video analysis for: {dl_path}")
        analysis_result = analyze_video_intelligence(dl_path)
        
        # è·å–æ™ºèƒ½ç‰‡æ®µæ¨è
        smart_segments = get_smart_segments(dl_path, 15, 30, count=5)
        
        return {
            "status": "success",
            "video_path": dl_path,
            "analysis": analysis_result,
            "recommended_segments": smart_segments,
            "message": f"æˆåŠŸåˆ†æè§†é¢‘ï¼Œå‘ç° {len(smart_segments)} ä¸ªæ¨èç‰‡æ®µ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Video analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")


@app.post("/asr/transcribe")
async def asr_transcribe(req: ASRTranscribeReq):
    """è‡ªåŠ¨è¯­éŸ³è¯†åˆ« - è½¬å½•è§†é¢‘/éŸ³é¢‘"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.url.startswith("file:"):
            # æœ¬åœ°æ–‡ä»¶
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            # URLä¸‹è½½
            input_path = str(Path("input_data/downloads") / f"asr_input_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸ¤ å¼€å§‹ASRè½¬å½•: {input_path}")
        
        # è·å–ASRæœåŠ¡
        asr_service = get_asr_service(model_size=req.model_size)
        
        # è½¬å½•è§†é¢‘
        result = asr_service.transcribe_video(
            input_path,
            language=req.language,
            task=req.task,
            cleanup_audio=True
        )
        
        # ç”Ÿæˆå­—å¹•æ–‡ä»¶
        subtitle_file = None
        if req.subtitle_format != "none":
            video_stem = Path(input_path).stem
            subtitle_path = f"output_data/{video_stem}_asr_{ts}.{req.subtitle_format}"
            
            subtitle_file = asr_service.generate_subtitles(
                result,
                format=req.subtitle_format,
                output_path=subtitle_path
            )
        
        return {
            "status": "success",
            "input_path": input_path,
            "language": result["language"],
            "language_probability": result["language_probability"],
            "duration": result["duration"],
            "full_text": result["full_text"],
            "word_count": result["word_count"],
            "segment_count": result["segment_count"],
            "processing_time": result["processing_time"],
            "subtitle_file": subtitle_file,
            "subtitle_format": req.subtitle_format if req.subtitle_format != "none" else None,
            "segments": result["segments"][:10] if len(result["segments"]) > 10 else result["segments"],  # é™åˆ¶è¿”å›çš„æ®µè½æ•°é‡
            "message": f"è½¬å½•å®Œæˆ - æ£€æµ‹è¯­è¨€: {result['language']}, æ–‡æœ¬é•¿åº¦: {result['word_count']}è¯"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ASRè½¬å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")


@app.post("/asr/extract_audio")
async def extract_audio(req: AudioExtractionReq):
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    try:
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        if req.url.startswith("file:"):
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            # URLä¸‹è½½
            ts = int(time.time())
            input_path = str(Path("input_data/downloads") / f"audio_extract_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸµ å¼€å§‹æå–éŸ³é¢‘: {input_path}")
        
        # è·å–ASRæœåŠ¡
        asr_service = get_asr_service()
        
        # æå–éŸ³é¢‘
        video_stem = Path(input_path).stem
        audio_path = f"output_data/{video_stem}_audio_{int(time.time())}.wav"
        
        extracted_audio = asr_service.extract_audio_from_video(
            input_path,
            audio_path=audio_path,
            sample_rate=req.sample_rate
        )
        
        # è·å–éŸ³é¢‘ä¿¡æ¯
        audio_info = {}
        try:
            import subprocess
            result = subprocess.run([
                "ffprobe", "-v", "quiet", "-print_format", "json", "-show_format",
                "-show_streams", extracted_audio
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                import json
                probe_data = json.loads(result.stdout)
                if 'format' in probe_data:
                    audio_info = {
                        'duration': float(probe_data['format'].get('duration', 0)),
                        'size': int(probe_data['format'].get('size', 0)),
                        'bit_rate': probe_data['format'].get('bit_rate'),
                    }
                if 'streams' in probe_data and probe_data['streams']:
                    stream = probe_data['streams'][0]
                    audio_info.update({
                        'sample_rate': stream.get('sample_rate'),
                        'channels': stream.get('channels'),
                        'codec': stream.get('codec_name')
                    })
        except Exception as e:
            logger.warning(f"è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            "status": "success",
            "input_path": input_path,
            "audio_path": extracted_audio,
            "sample_rate": req.sample_rate,
            "audio_info": audio_info,
            "message": f"éŸ³é¢‘æå–å®Œæˆ: {extracted_audio}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘æå–å¤±è´¥: {str(e)}")


@app.get("/asr/info")
async def asr_info():
    """è·å–ASRæœåŠ¡ä¿¡æ¯"""
    try:
        asr_service = get_asr_service()
        info = asr_service.get_model_info()
        
        return {
            "status": "success",
            "asr_info": info,
            "available_models": ["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"],
            "supported_languages": info["supported_languages"],
            "subtitle_formats": info["subtitle_formats"],
            "message": "ASRæœåŠ¡ä¿¡æ¯è·å–æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–ASRä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ASRä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/semantic/analyze")
async def semantic_analyze(req: SemanticAnalysisReq):
    """è¯­ä¹‰åˆ†æ - åˆ†ææ–‡æœ¬çš„å…³é”®è¯ã€æƒ…æ„Ÿã€ä¸»é¢˜ç­‰"""
    try:
        logger.info(f"å¼€å§‹è¯­ä¹‰åˆ†æï¼Œæ–‡æœ¬é•¿åº¦: {len(req.text)}")
        
        analyzer = get_semantic_analyzer()
        result = {}
        
        # å…³é”®è¯æå–
        if req.include_keywords:
            result['keywords'] = analyzer.extract_keywords(req.text, top_k=10)
        
        # æƒ…æ„Ÿåˆ†æ
        if req.include_sentiment:
            result['sentiment'] = analyzer.analyze_sentiment(req.text)
        
        # ä¸»é¢˜ç›¸å…³æ€§
        if req.include_topics:
            result['topic_relevance'] = analyzer.analyze_topic_relevance(req.text)
        
        # å†…å®¹è´¨é‡è¯„åˆ†
        if req.include_quality:
            result['quality_score'] = analyzer.calculate_content_quality_score(req.text)
        
        return {
            "status": "success",
            "text_length": len(req.text),
            "word_count": len(req.text.split()),
            "analysis": result,
            "message": "è¯­ä¹‰åˆ†æå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­ä¹‰åˆ†æå¤±è´¥: {str(e)}")


@app.post("/smart_clipping/asr_enhanced")
async def asr_enhanced_smart_clipping(req: ASRSmartClippingReq):
    """ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡ - ç»“åˆè¯­éŸ³è¯†åˆ«å’Œè¯­ä¹‰åˆ†æçš„æ™ºèƒ½é€‰æ®µ"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.url.startswith("file:"):
            local_path = req.url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"asr_smart_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.url, input_path)
        
        logger.info(f"ğŸ¯ å¼€å§‹ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡: {input_path}")
        
        # 1. è·å–ASRè½¬å½•ç»“æœ
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(
            input_path,
            language=req.language,
            cleanup_audio=True
        )
        
        logger.info(f"ASRè½¬å½•å®Œæˆ - è¯­è¨€: {transcription_result['language']}, æ–‡æœ¬é•¿åº¦: {transcription_result['word_count']}è¯")
        
        # 2. æ‰§è¡ŒASRå¢å¼ºæ™ºèƒ½é€‰æ®µ
        asr_engine = get_asr_smart_engine()
        selected_segments = asr_engine.select_best_segments_with_asr(
            input_path,
            transcription_result,
            req.min_sec,
            req.max_sec,
            req.count
        )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ™ºèƒ½ç‰‡æ®µ")
        
        # 3. ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        
        for i, segment in enumerate(selected_segments):
            output_filename = f"{req.output_prefix}_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ä½¿ç”¨pad-firstç­–ç•¥ç”Ÿæˆ9:16è§†é¢‘
            fade_out_start = max(0.1, duration - 0.25)
            vf_filters = (
                "scale=1080:1920:force_original_aspect_ratio=decrease,"
                "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
            )
            
            cmd = [
                "ffmpeg", "-y", "-hwaccel", "none",
                "-i", input_path,
                "-ss", start_time, "-t", f"{duration:.2f}",
                "-vf", vf_filters,
                "-pix_fmt", "yuv420p",
                "-map", "0:v:0", "-map", "0:a?",
                "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                "-shortest", "-movflags", "+faststart",
                output_path
            ]
            
            safe_run_ffmpeg(cmd)
            
            # è·å–ç”Ÿæˆçš„è§†é¢‘ä¿¡æ¯
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "duration": duration,
                "file_size": file_size,
                "visual_score": segment['visual_score'],
                "content_score": segment['content_score'],
                "total_score": segment['total_score'],
                "selection_type": segment['type']
            })
        
        # 4. æ„å»ºå“åº”
        return {
            "status": "success",
            "input_path": input_path,
            "transcription": {
                "language": transcription_result["language"],
                "language_probability": transcription_result["language_probability"],
                "duration": transcription_result["duration"],
                "word_count": transcription_result["word_count"],
                "segment_count": transcription_result["segment_count"]
            },
            "selected_segments": len(selected_segments),
            "generated_clips": generated_clips,
            "processing_summary": {
                "asr_enhanced": True,
                "semantic_analysis": True,
                "visual_analysis": True,
                "selection_algorithm": "asr_smart_clipping"
            },
            "message": f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å®Œæˆ - ç”Ÿæˆäº† {len(generated_clips)} ä¸ªé«˜è´¨é‡ç‰‡æ®µ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ASRå¢å¼ºæ™ºèƒ½åˆ‡ç‰‡å¤±è´¥: {str(e)}")


@app.post("/pro/photo_rank_advanced")
async def advanced_photo_rank(req: AdvancedPhotoRankingReq):
    """é«˜çº§ç…§ç‰‡é€‰ä¼˜æ’åºï¼ˆProç‰ˆæœ¬ï¼‰"""
    try:
        logger.info(f"å¼€å§‹é«˜çº§ç…§ç‰‡é€‰ä¼˜ï¼Œå…± {len(req.photos)} å¼ ç…§ç‰‡")
        
        advanced_service = get_advanced_photo_service()
        ranked_photos = advanced_service.rank_photos_advanced(
            req.photos, req.top_k, req.context
        )
        
        return {
            "status": "success",
            "input_count": len(req.photos),
            "output_count": len(ranked_photos),
            "ranked_photos": ranked_photos,
            "features_used": {
                "clip_analysis": req.use_clip,
                "aesthetic_model": req.use_aesthetic_model,
                "duplicate_detection": True,
                "subject_consistency": True
            },
            "message": f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å®Œæˆï¼Œè¿”å›å‰ {len(ranked_photos)} å¼ "
        }
        
    except Exception as e:
        logger.error(f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"é«˜çº§ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {str(e)}")


@app.post("/pro/semantic_highlights")
async def detect_semantic_highlights(req: SemanticHighlightsReq):
    """è¯­ä¹‰é«˜å…‰æ£€æµ‹"""
    try:
        logger.info(f"å¼€å§‹è¯­ä¹‰é«˜å…‰æ£€æµ‹ï¼Œå…± {len(req.transcript_segments)} ä¸ªç‰‡æ®µ")
        
        detector = get_semantic_highlight_detector()
        highlights = detector.detect_highlights(req.transcript_segments, req.context)
        
        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„é«˜å…‰
        filtered_highlights = [
            h for h in highlights 
            if h.get('highlight_score', 0) >= req.min_score
        ][:req.max_highlights]
        
        return {
            "status": "success",
            "total_segments": len(req.transcript_segments),
            "highlights_found": len(highlights),
            "highlights_returned": len(filtered_highlights),
            "highlights": filtered_highlights,
            "analysis_summary": {
                "avg_score": np.mean([h.get('highlight_score', 0) for h in filtered_highlights]) if filtered_highlights else 0,
                "highlight_types": list(set(h.get('highlight_type', 'unknown') for h in filtered_highlights)),
                "min_score_threshold": req.min_score
            },
            "message": f"æ£€æµ‹åˆ° {len(filtered_highlights)} ä¸ªé«˜å…‰æ—¶åˆ»"
        }
        
    except Exception as e:
        logger.error(f"è¯­ä¹‰é«˜å…‰æ£€æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è¯­ä¹‰é«˜å…‰æ£€æµ‹å¤±è´¥: {str(e)}")


@app.post("/pro/user_style_learning")
async def learn_user_style(req: UserStyleLearningReq):
    """å­¦ä¹ ç”¨æˆ·å†™ä½œé£æ ¼"""
    try:
        logger.info(f"å¼€å§‹å­¦ä¹ ç”¨æˆ· {req.user_id} çš„å†™ä½œé£æ ¼")
        
        writing_service = get_personalized_writing_service()
        user_profile = writing_service.learn_user_style(req.user_id, req.content_samples)
        
        return {
            "status": "success",
            "user_id": req.user_id,
            "samples_analyzed": len(req.content_samples),
            "confidence_score": user_profile.get('confidence_score', 0),
            "learned_features": {
                "vocabulary_patterns": len(user_profile.get('vocabulary', {}).get('signature_words', [])),
                "emoji_preferences": len(user_profile.get('emoji', {}).get('favorite_emojis', [])),
                "tone_analysis": user_profile.get('tone', {}).get('primary_tone', 'unknown'),
                "topic_preferences": len(user_profile.get('topics', {}).get('preferred_topics', []))
            },
            "message": f"ç”¨æˆ·é£æ ¼å­¦ä¹ å®Œæˆï¼Œç½®ä¿¡åº¦: {user_profile.get('confidence_score', 0):.2f}"
        }
        
    except Exception as e:
        logger.error(f"ç”¨æˆ·é£æ ¼å­¦ä¹ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”¨æˆ·é£æ ¼å­¦ä¹ å¤±è´¥: {str(e)}")


@app.post("/pro/personalized_writing")
async def generate_personalized_content(req: PersonalizedWritingReq):
    """ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹"""
    try:
        logger.info(f"å¼€å§‹ä¸ºç”¨æˆ· {req.user_id} ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹")
        
        writing_service = get_personalized_writing_service()
        personalized_content = writing_service.generate_personalized_content(
            req.user_id, req.content_data, req.style_override
        )
        
        return {
            "status": "success",
            "user_id": req.user_id,
            "personalized_content": personalized_content,
            "personalization_confidence": personalized_content.get('personalization_confidence', 0),
            "features_applied": personalized_content.get('metadata', {}).get('personalization_features', []),
            "message": "ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/pro/smart_cover")
async def generate_smart_cover(req: SmartCoverDesignReq):
    """æ™ºèƒ½å°é¢è®¾è®¡"""
    try:
        logger.info(f"å¼€å§‹æ™ºèƒ½å°é¢è®¾è®¡ - æ ‡é¢˜: {req.title[:20]}...")
        
        cover_designer = get_smart_cover_designer()
        cover_result = cover_designer.generate_smart_cover(
            req.clips, req.photos, req.title, req.style
        )
        
        return {
            "status": "success",
            "cover_result": cover_result,
            "design_features": {
                "frame_analysis": bool(cover_result.get('source_frame')),
                "color_extraction": bool(cover_result.get('color_palette')),
                "text_layout": bool(cover_result.get('text_layout')),
                "smart_positioning": True
            },
            "message": "æ™ºèƒ½å°é¢è®¾è®¡å®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½å°é¢è®¾è®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½å°é¢è®¾è®¡å¤±è´¥: {str(e)}")


@app.post("/pro/audio_processing")
async def process_video_audio(req: AudioProcessingReq):
    """éŸ³é¢‘å¤„ç†ï¼ˆé™å™ªã€BGMåŒ¹é…ï¼‰"""
    try:
        logger.info(f"å¼€å§‹éŸ³é¢‘å¤„ç† - é£æ ¼: {req.style}")
        
        audio_service = get_audio_processing_service()
        processing_result = audio_service.process_video_audio(
            req.video_path, req.style, req.enhance_speech, req.add_bgm
        )
        
        return {
            "status": "success",
            "processing_result": processing_result,
            "audio_enhancements": {
                "noise_reduction": req.enhance_speech,
                "bgm_added": req.add_bgm,
                "beat_alignment": processing_result.get('bgm_info', {}).get('tempo', 0) > 0,
                "final_optimization": processing_result.get('processing_steps', {}).get('final_optimization', False)
            },
            "message": "éŸ³é¢‘å¤„ç†å®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/photo_rank")
async def photo_rank(req: PhotoRankingReq):
    """ç…§ç‰‡é€‰ä¼˜æ’åº"""
    try:
        logger.info(f"å¼€å§‹ç…§ç‰‡é€‰ä¼˜ï¼Œå…± {len(req.photos)} å¼ ç…§ç‰‡")
        
        photo_service = get_photo_ranking_service()
        ranked_photos = photo_service.rank_photos(req.photos, req.top_k)
        
        return {
            "status": "success",
            "input_count": len(req.photos),
            "output_count": len(ranked_photos),
            "ranked_photos": ranked_photos,
            "message": f"ç…§ç‰‡é€‰ä¼˜å®Œæˆï¼Œè¿”å›å‰ {len(ranked_photos)} å¼ "
        }
        
    except Exception as e:
        logger.error(f"ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç…§ç‰‡é€‰ä¼˜å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/storyline")
async def generate_storyline(req: StorylineReq):
    """ç”Ÿæˆæ—…è¡Œæ•…äº‹çº¿"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆæ•…äº‹çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}")
        
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            req.transcript_mmss, req.notes, req.city, req.date, req.style
        )
        
        return {
            "status": "success",
            "storyline": storyline,
            "section_count": len(storyline.get('sections', [])),
            "tip_count": len(storyline.get('tips', [])),
            "poi_count": len(storyline.get('pois', [])),
            "message": "æ•…äº‹çº¿ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ•…äº‹çº¿ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ•…äº‹çº¿ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/draft")
async def generate_xhs_draft(req: XHSDraftReq):
    """ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ - è°ƒæ€§: {req.brand_tone}")
        
        draft_gen = get_draft_generator()
        draft = draft_gen.generate_draft(req.storyline, req.brand_tone, req.constraints)
        
        return {
            "status": "success",
            "draft": draft,
            "metadata": draft.get('metadata', {}),
            "message": "å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/subtitles")
async def generate_subtitles(req: SubtitleReq):
    """ç”Ÿæˆå­—å¹•æ–‡ä»¶"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå­—å¹• - é£æ ¼: {req.style}")
        
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(req.clips, req.transcript_mmss, req.style)
        
        return {
            "status": "success",
            "subtitles": subtitles,
            "message": "å­—å¹•ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å­—å¹•ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å­—å¹•ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/cover")
async def suggest_cover(req: CoverReq):
    """ç”Ÿæˆå°é¢å»ºè®®"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆå°é¢å»ºè®® - æ ‡é¢˜: {req.title[:20]}...")
        
        cover_service = get_cover_service()
        cover_suggestions = cover_service.suggest_cover(req.clips, req.photos_topk, req.title)
        
        return {
            "status": "success",
            "cover_suggestions": cover_suggestions,
            "message": "å°é¢å»ºè®®ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å°é¢å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°é¢å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/export")
async def export_content(req: ExportReq):
    """å¯¼å‡ºå°çº¢ä¹¦å†…å®¹äº§ç‰©"""
    try:
        logger.info(f"å¼€å§‹å¯¼å‡ºå†…å®¹ - æ ¼å¼: {req.export_format}")
        
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            req.pipeline_result, req.export_format, req.include_source
        )
        
        return {
            "status": "success",
            "export_result": export_result,
            "message": "å†…å®¹å¯¼å‡ºå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"å†…å®¹å¯¼å‡ºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å†…å®¹å¯¼å‡ºå¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/pipeline")
async def xiaohongshu_pipeline(req: XHSPipelineReq):
    """å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿å®Œæ•´æµæ°´çº¿"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ¬ å¼€å§‹å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿æµæ°´çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}")
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.video_url.startswith("file:"):
            local_path = req.video_url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"xhs_pipeline_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.video_url, input_path)
        
        pipeline_result = {
            'source_video': input_path,
            'processing_id': f'xhs_{ts}',
            'created_at': datetime.now().isoformat()
        }
        
        # 1. ASRè¯­éŸ³è¯†åˆ«
        logger.info("æ­¥éª¤1: ASRè¯­éŸ³è¯†åˆ«...")
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(input_path, cleanup_audio=True)
        
        # è½¬æ¢ä¸ºå¸¦æ—¶é—´æˆ³æ ¼å¼
        transcript_mmss = []
        for segment in transcription_result.get('segments', []):
            transcript_mmss.append({
                'start': segment.get('start', 0),
                'end': segment.get('end', 0),
                'text': segment.get('text', ''),
                'timestamp': f"{int(segment.get('start', 0)//60):02d}:{int(segment.get('start', 0)%60):02d}"
            })
        
        pipeline_result['transcription'] = transcription_result
        pipeline_result['transcript_mmss'] = transcript_mmss
        
        # 2. æ™ºèƒ½é€‰æ®µ
        logger.info("æ­¥éª¤2: ASRå¢å¼ºæ™ºèƒ½é€‰æ®µ...")
        asr_engine = get_asr_smart_engine()
        selected_segments = asr_engine.select_best_segments_with_asr(
            input_path, transcription_result, 15, 30, 2  # ç”Ÿæˆ2ä¸ª15-30ç§’ç‰‡æ®µ
        )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘ç‰‡æ®µ")
        
        # ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        for i, segment in enumerate(selected_segments):
            output_filename = f"xhs_clip_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ä½¿ç”¨pad-firstç­–ç•¥ç”Ÿæˆ9:16è§†é¢‘
            fade_out_start = max(0.1, duration - 0.25)
            vf_filters = (
                "scale=1080:1920:force_original_aspect_ratio=decrease,"
                "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
            )
            
            cmd = [
                "ffmpeg", "-y", "-hwaccel", "none",
                "-i", input_path,
                "-ss", start_time, "-t", f"{duration:.2f}",
                "-vf", vf_filters,
                "-pix_fmt", "yuv420p",
                "-map", "0:v:0", "-map", "0:a?",
                "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                "-shortest", "-movflags", "+faststart",
                output_path
            ]
            
            safe_run_ffmpeg(cmd)
            
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "start_time_seconds": segment['start_time'],
                "end_time_seconds": segment['end_time'],
                "duration": duration,
                "file_size": file_size
            })
        
        pipeline_result['clips'] = generated_clips
        
        # 3. ç…§ç‰‡é€‰ä¼˜ï¼ˆå¦‚æœæœ‰ç…§ç‰‡ï¼‰
        if req.photos:
            logger.info("æ­¥éª¤3: ç…§ç‰‡é€‰ä¼˜...")
            photo_service = get_photo_ranking_service()
            ranked_photos = photo_service.rank_photos(req.photos, 10)
            pipeline_result['photos_ranked'] = ranked_photos
        else:
            pipeline_result['photos_ranked'] = []
        
        # 4. æ•…äº‹çº¿ç”Ÿæˆ
        logger.info("æ­¥éª¤4: ç”Ÿæˆæ•…äº‹çº¿...")
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            transcript_mmss, req.notes, req.city, "", req.style
        )
        pipeline_result['storyline'] = storyline
        
        # 5. å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆ
        logger.info("æ­¥éª¤5: ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆ...")
        draft_gen = get_draft_generator()
        draft = draft_gen.generate_draft(storyline, req.style)
        pipeline_result['draft'] = draft
        
        # 6. å­—å¹•ç”Ÿæˆ
        logger.info("æ­¥éª¤6: ç”Ÿæˆå­—å¹•...")
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(generated_clips, transcript_mmss, "å¯çˆ±")
        pipeline_result['subtitles'] = subtitles
        
        # 7. å°é¢å»ºè®®
        logger.info("æ­¥éª¤7: ç”Ÿæˆå°é¢å»ºè®®...")
        cover_service = get_cover_service()
        cover_suggestions = cover_service.suggest_cover(
            generated_clips, pipeline_result['photos_ranked'], draft['title']
        )
        pipeline_result['cover'] = cover_suggestions
        
        # 8. å¯¼å‡ºäº§ç‰©
        logger.info("æ­¥éª¤8: å¯¼å‡ºå†…å®¹äº§ç‰©...")
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            pipeline_result, req.export_format, False
        )
        
        return {
            "status": "success",
            "pipeline_result": {
                'processing_id': pipeline_result['processing_id'],
                'transcription_summary': {
                    'language': transcription_result['language'],
                    'duration': transcription_result['duration'],
                    'word_count': transcription_result['word_count']
                },
                'clips_generated': len(generated_clips),
                'photos_ranked': len(pipeline_result['photos_ranked']),
                'storyline_sections': len(storyline.get('sections', [])),
                'draft_info': {
                    'title': draft['title'],
                    'hashtag_count': len(draft.get('hashtags', [])),
                    'word_count': draft.get('metadata', {}).get('word_count', 0)
                },
                'subtitle_files': len(subtitles.get('srt_files', [])),
                'export_info': export_result
            },
            "download_links": export_result.get('share_urls', {}),
            "message": f"ğŸ‰ å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿å®Œæˆï¼ç”Ÿæˆäº† {len(generated_clips)} ä¸ªè§†é¢‘ç‰‡æ®µå’Œå®Œæ•´æ–‡æ¡ˆ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦æµæ°´çº¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°çº¢ä¹¦æµæ°´çº¿å¤±è´¥: {str(e)}")


@app.post("/xiaohongshu/pipeline_pro")
async def xiaohongshu_pipeline_pro(req: XHSProPipelineReq):
    """å°çº¢ä¹¦ä¸€é”®å‡ºç¨¿Proç‰ˆæµæ°´çº¿ï¼ˆåŒ…å«æ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼‰"""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸš€ å¼€å§‹å°çº¢ä¹¦Proæµæ°´çº¿ - åŸå¸‚: {req.city}, é£æ ¼: {req.style}, ç”¨æˆ·: {req.user_id or 'anonymous'}")
        
        # å¤„ç†è¾“å…¥æ–‡ä»¶
        ts = int(time.time())
        if req.video_url.startswith("file:"):
            local_path = req.video_url.replace("file://", "")
            if not Path(local_path).exists():
                raise HTTPException(status_code=400, detail="æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")
            input_path = local_path
        else:
            input_path = str(Path("input_data/downloads") / f"xhs_pro_{ts}.mp4")
            from urllib.request import urlretrieve
            urlretrieve(req.video_url, input_path)
        
        pipeline_result = {
            'source_video': input_path,
            'processing_id': f'xhs_pro_{ts}',
            'created_at': datetime.now().isoformat(),
            'pro_features_enabled': {
                'advanced_photo_ranking': req.use_advanced_photo_ranking,
                'semantic_highlights': req.use_semantic_highlights,
                'personalized_writing': req.use_personalized_writing,
                'smart_cover': req.use_smart_cover,
                'audio_enhancement': req.use_audio_enhancement
            }
        }
        
        # 1. ASRè¯­éŸ³è¯†åˆ«
        logger.info("æ­¥éª¤1: ASRè¯­éŸ³è¯†åˆ«...")
        asr_service = get_asr_service(model_size=req.model_size)
        transcription_result = asr_service.transcribe_video(input_path, cleanup_audio=True)
        
        # è½¬æ¢ä¸ºå¸¦æ—¶é—´æˆ³æ ¼å¼
        transcript_mmss = []
        for segment in transcription_result.get('segments', []):
            transcript_mmss.append({
                'start': segment.get('start', 0),
                'end': segment.get('end', 0),
                'text': segment.get('text', ''),
                'timestamp': f"{int(segment.get('start', 0)//60):02d}:{int(segment.get('start', 0)%60):02d}"
            })
        
        pipeline_result['transcription'] = transcription_result
        pipeline_result['transcript_mmss'] = transcript_mmss
        
        # 2. ProåŠŸèƒ½ï¼šè¯­ä¹‰é«˜å…‰æ£€æµ‹
        if req.use_semantic_highlights:
            logger.info("æ­¥éª¤2Pro: è¯­ä¹‰é«˜å…‰æ£€æµ‹...")
            detector = get_semantic_highlight_detector()
            semantic_highlights = detector.detect_highlights(
                transcription_result.get('segments', []), 
                {'city': req.city, 'style': req.style}
            )
            pipeline_result['semantic_highlights'] = semantic_highlights
            logger.info(f"æ£€æµ‹åˆ° {len(semantic_highlights)} ä¸ªè¯­ä¹‰é«˜å…‰æ—¶åˆ»")
        
        # 3. æ™ºèƒ½é€‰æ®µï¼ˆç»“åˆè¯­ä¹‰é«˜å…‰ï¼‰
        logger.info("æ­¥éª¤3: ASRå¢å¼ºæ™ºèƒ½é€‰æ®µ...")
        asr_engine = get_asr_smart_engine()
        
        # å¦‚æœæœ‰è¯­ä¹‰é«˜å…‰ï¼Œä¼˜å…ˆé€‰æ‹©é«˜å…‰ç‰‡æ®µ
        if req.use_semantic_highlights and pipeline_result.get('semantic_highlights'):
            # åŸºäºè¯­ä¹‰é«˜å…‰é€‰æ‹©ç‰‡æ®µ
            highlight_segments = []
            for highlight in pipeline_result['semantic_highlights'][:3]:  # å–å‰3ä¸ªé«˜å…‰
                highlight_segments.append({
                    'start_time': highlight.get('start', 0),
                    'end_time': highlight.get('end', 0),
                    'duration': highlight.get('duration', 15),
                    'start_hms': f"{int(highlight.get('start', 0)//3600):02d}:{int((highlight.get('start', 0)%3600)//60):02d}:{int(highlight.get('start', 0)%60):02d}",
                    'reason': f"è¯­ä¹‰é«˜å…‰: {highlight.get('highlight_reason', 'ç²¾å½©å†…å®¹')}",
                    'highlight_score': highlight.get('highlight_score', 0.8)
                })
            selected_segments = highlight_segments
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ™ºèƒ½é€‰æ®µ
            selected_segments = asr_engine.select_best_segments_with_asr(
                input_path, transcription_result, 15, 30, 2
            )
        
        if not selected_segments:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘ç‰‡æ®µ")
        
        # ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        generated_clips = []
        for i, segment in enumerate(selected_segments):
            output_filename = f"xhs_pro_clip_{ts}_{i+1:02d}.mp4"
            output_path = f"output_data/{output_filename}"
            
            start_time = segment['start_hms']
            duration = segment['duration']
            
            # ProåŠŸèƒ½ï¼šéŸ³é¢‘å¢å¼º
            if req.use_audio_enhancement:
                # å…ˆç”ŸæˆåŸºç¡€è§†é¢‘
                fade_out_start = max(0.1, duration - 0.25)
                vf_filters = (
                    "scale=1080:1920:force_original_aspect_ratio=decrease,"
                    "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                    f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
                )
                
                temp_video_path = f"output_data/temp_{output_filename}"
                
                cmd = [
                    "ffmpeg", "-y", "-hwaccel", "none",
                    "-i", input_path,
                    "-ss", start_time, "-t", f"{duration:.2f}",
                    "-vf", vf_filters,
                    "-pix_fmt", "yuv420p",
                    "-map", "0:v:0", "-map", "0:a?",
                    "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                    "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                    "-shortest", "-movflags", "+faststart",
                    temp_video_path
                ]
                
                safe_run_ffmpeg(cmd)
                
                # éŸ³é¢‘å¢å¼ºå¤„ç†
                try:
                    audio_service = get_audio_processing_service()
                    audio_result = audio_service.process_video_audio(
                        temp_video_path, req.style, True, True
                    )
                    
                    if audio_result.get('success') and audio_result.get('processed_video'):
                        # ä½¿ç”¨å¢å¼ºåçš„è§†é¢‘
                        Path(temp_video_path).unlink(missing_ok=True)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        Path(audio_result['processed_video']).rename(output_path)
                    else:
                        # éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸè§†é¢‘
                        Path(temp_video_path).rename(output_path)
                        
                except Exception as audio_error:
                    logger.warning(f"éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸéŸ³é¢‘: {audio_error}")
                    Path(temp_video_path).rename(output_path)
            else:
                # æ ‡å‡†è§†é¢‘ç”Ÿæˆ
                fade_out_start = max(0.1, duration - 0.25)
                vf_filters = (
                    "scale=1080:1920:force_original_aspect_ratio=decrease,"
                    "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
                    f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
                )
                
                cmd = [
                    "ffmpeg", "-y", "-hwaccel", "none",
                    "-i", input_path,
                    "-ss", start_time, "-t", f"{duration:.2f}",
                    "-vf", vf_filters,
                    "-pix_fmt", "yuv420p",
                    "-map", "0:v:0", "-map", "0:a?",
                    "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
                    "-c:a", "aac", "-b:a", AUDIO_BITRATE,
                    "-shortest", "-movflags", "+faststart",
                    output_path
                ]
                
                safe_run_ffmpeg(cmd)
            
            file_size = Path(output_path).stat().st_size if Path(output_path).exists() else 0
            
            generated_clips.append({
                "clip_index": i + 1,
                "output_path": output_path,
                "start_time": start_time,
                "start_time_seconds": segment['start_time'],
                "end_time_seconds": segment['end_time'],
                "duration": duration,
                "file_size": file_size,
                "selection_reason": segment.get('reason', 'ASRæ™ºèƒ½é€‰æ®µ'),
                "highlight_score": segment.get('highlight_score', 0.7)
            })
        
        pipeline_result['clips'] = generated_clips
        
        # 4. ProåŠŸèƒ½ï¼šé«˜çº§ç…§ç‰‡é€‰ä¼˜
        if req.photos and req.use_advanced_photo_ranking:
            logger.info("æ­¥éª¤4Pro: é«˜çº§ç…§ç‰‡é€‰ä¼˜...")
            advanced_photo_service = get_advanced_photo_service()
            ranked_photos = advanced_photo_service.rank_photos_advanced(
                req.photos, 10, {'city': req.city, 'style': req.style}
            )
            pipeline_result['photos_ranked'] = ranked_photos
        elif req.photos:
            # ä½¿ç”¨åŸºç¡€ç…§ç‰‡é€‰ä¼˜
            logger.info("æ­¥éª¤4: åŸºç¡€ç…§ç‰‡é€‰ä¼˜...")
            photo_service = get_photo_ranking_service()
            ranked_photos = photo_service.rank_photos(req.photos, 10)
            pipeline_result['photos_ranked'] = ranked_photos
        else:
            pipeline_result['photos_ranked'] = []
        
        # 5. æ•…äº‹çº¿ç”Ÿæˆ
        logger.info("æ­¥éª¤5: ç”Ÿæˆæ•…äº‹çº¿...")
        storyline_gen = get_storyline_generator()
        storyline = storyline_gen.generate_storyline(
            transcript_mmss, req.notes, req.city, "", req.style
        )
        pipeline_result['storyline'] = storyline
        
        # 6. ProåŠŸèƒ½ï¼šä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ
        if req.use_personalized_writing and req.user_id:
            logger.info("æ­¥éª¤6Pro: ä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ...")
            writing_service = get_personalized_writing_service()
            draft = writing_service.generate_personalized_content(
                req.user_id, 
                {'storyline': storyline, 'city': req.city, 'style': req.style},
                req.style
            )
            pipeline_result['draft'] = draft
        else:
            # ä½¿ç”¨æ ‡å‡†æ–‡æ¡ˆç”Ÿæˆ
            logger.info("æ­¥éª¤6: æ ‡å‡†æ–‡æ¡ˆç”Ÿæˆ...")
            draft_gen = get_draft_generator()
            draft = draft_gen.generate_draft(storyline, req.style)
            pipeline_result['draft'] = draft
        
        # 7. å­—å¹•ç”Ÿæˆ
        logger.info("æ­¥éª¤7: ç”Ÿæˆå­—å¹•...")
        subtitle_gen = get_subtitle_generator()
        subtitles = subtitle_gen.generate_subtitles(generated_clips, transcript_mmss, "å¯çˆ±")
        pipeline_result['subtitles'] = subtitles
        
        # 8. ProåŠŸèƒ½ï¼šæ™ºèƒ½å°é¢è®¾è®¡
        if req.use_smart_cover:
            logger.info("æ­¥éª¤8Pro: æ™ºèƒ½å°é¢è®¾è®¡...")
            cover_designer = get_smart_cover_designer()
            cover_suggestions = cover_designer.generate_smart_cover(
                generated_clips, pipeline_result['photos_ranked'], 
                pipeline_result['draft']['title'], req.style
            )
            pipeline_result['cover'] = cover_suggestions
        else:
            # ä½¿ç”¨åŸºç¡€å°é¢å»ºè®®
            logger.info("æ­¥éª¤8: åŸºç¡€å°é¢å»ºè®®...")
            cover_service = get_cover_service()
            cover_suggestions = cover_service.suggest_cover(
                generated_clips, pipeline_result['photos_ranked'], 
                pipeline_result['draft']['title']
            )
            pipeline_result['cover'] = cover_suggestions
        
        # 9. å¯¼å‡ºäº§ç‰©
        logger.info("æ­¥éª¤9: å¯¼å‡ºå†…å®¹äº§ç‰©...")
        export_service = get_export_service()
        export_result = export_service.export_xiaohongshu_content(
            pipeline_result, req.export_format, False
        )
        
        # ç»Ÿè®¡ProåŠŸèƒ½ä½¿ç”¨æƒ…å†µ
        pro_features_used = []
        if req.use_advanced_photo_ranking and req.photos:
            pro_features_used.append("é«˜çº§ç…§ç‰‡é€‰ä¼˜")
        if req.use_semantic_highlights:
            pro_features_used.append("è¯­ä¹‰é«˜å…‰æ£€æµ‹")
        if req.use_personalized_writing and req.user_id:
            pro_features_used.append("ä¸ªæ€§åŒ–æ–‡æ¡ˆç”Ÿæˆ")
        if req.use_smart_cover:
            pro_features_used.append("æ™ºèƒ½å°é¢è®¾è®¡")
        if req.use_audio_enhancement:
            pro_features_used.append("éŸ³é¢‘å¢å¼ºå¤„ç†")
        
        return {
            "status": "success",
            "pipeline_result": {
                'processing_id': pipeline_result['processing_id'],
                'pro_features_used': pro_features_used,
                'transcription_summary': {
                    'language': transcription_result['language'],
                    'duration': transcription_result['duration'],
                    'word_count': transcription_result['word_count']
                },
                'semantic_highlights_count': len(pipeline_result.get('semantic_highlights', [])),
                'clips_generated': len(generated_clips),
                'photos_ranked': len(pipeline_result['photos_ranked']),
                'storyline_sections': len(storyline.get('sections', [])),
                'draft_info': {
                    'title': pipeline_result['draft']['title'],
                    'hashtag_count': len(pipeline_result['draft'].get('hashtags', [])),
                    'word_count': pipeline_result['draft'].get('metadata', {}).get('word_count', 0),
                    'personalization_confidence': pipeline_result['draft'].get('personalization_confidence', 0)
                },
                'subtitle_files': len(subtitles.get('srt_files', [])),
                'cover_design': {
                    'success': pipeline_result['cover'].get('success', False),
                    'cover_path': pipeline_result['cover'].get('cover_path', ''),
                    'design_features': len(pro_features_used)
                },
                'export_info': export_result
            },
            "download_links": export_result.get('share_urls', {}),
            "message": f"ğŸ‰ å°çº¢ä¹¦Proä¸€é”®å‡ºç¨¿å®Œæˆï¼ç”Ÿæˆäº† {len(generated_clips)} ä¸ªè§†é¢‘ç‰‡æ®µï¼Œä½¿ç”¨äº† {len(pro_features_used)} ä¸ªProåŠŸèƒ½"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°çº¢ä¹¦Proæµæ°´çº¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å°çº¢ä¹¦Proæµæ°´çº¿å¤±è´¥: {str(e)}")


@app.post("/auto_intro")
async def auto_intro(req: URLIntroReq):
    """Download video from URL, detect/extract subtitles or ASR, select highlights, cut 9:16, add simple fades, and concatenate into one intro video."""
    try:
        Path("input_data/downloads").mkdir(parents=True, exist_ok=True)
        Path("output_data").mkdir(parents=True, exist_ok=True)

        # 1) download via yt-dlp if platform URL, else urlretrieve
        ts = int(time.time())
        dl_path = str(Path("input_data/downloads") / f"dl_{ts}.mp4")
        try:
            import yt_dlp  # type: ignore
            ydl_opts = {
                'format': 'bv*+ba/b',
                'merge_output_format': 'mp4',
                'outtmpl': str(Path("input_data/downloads") / f"dl_{ts}.%(ext)s"),
                'writesubtitles': True,
                'subtitleslangs': ['zh.*','zh','zh-Hans','zh-Hant','en.*','en'],
                'subtitleformat': 'srt',
                'quiet': True,
                'noprogress': True,
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(req.url, download=True)
                # determine final filepath
                if 'requested_downloads' in info:
                    dl_path = info['requested_downloads'][0]['filepath']
                else:
                    # fallback: guess mp4 path
                    base = Path("input_data/downloads") / f"dl_{ts}"
                    if (base.with_suffix('.mp4')).exists():
                        dl_path = str(base.with_suffix('.mp4'))
                # normalize
                dl_path = str(Path(dl_path))
        except Exception:
            # Fallback 1: try system yt-dlp CLI
            try:
                cli_cmd = [
                    "yt-dlp",
                    "-f", "bv*+ba/b",
                    "--merge-output-format", "mp4",
                    "-o", str(Path("input_data/downloads") / f"dl_{ts}.%(ext)s"),
                    "--write-sub",
                    "--sub-lang", "zh.*,zh,zh-Hans,zh-Hant,en.*,en",
                    "--sub-format", "srt",
                    req.url,
                ]
                completed = subprocess.run(cli_cmd, capture_output=True, text=True)
                if completed.returncode != 0:
                    raise RuntimeError(completed.stderr or "yt-dlp CLI failed")
                # guess final mp4 path
                base = Path("input_data/downloads") / f"dl_{ts}"
                if (base.with_suffix('.mp4')).exists():
                    dl_path = str(base.with_suffix('.mp4'))
                else:
                    # try common containers
                    for ext in [".mkv", ".webm", ".mov"]:
                        cand = base.with_suffix(ext)
                        if cand.exists():
                            dl_path = str(cand)
                            break
                dl_path = str(Path(dl_path))
            except Exception:
                # Fallback 2: direct URL download (works only for direct media URLs)
                urlretrieve(req.url, dl_path)

        # 2) try extract subtitle stream to SRT
        srt_path = str(Path("output_data") / f"dl_{ts}.srt")
        # prefer subtitles downloaded by yt-dlp (scoped to current timestamp)
        possible_srts = list(Path("input_data/downloads").glob(f"dl_{ts}*.srt"))
        if possible_srts:
            # pick the largest srt as best
            best = max(possible_srts, key=lambda p: p.stat().st_size)
            try:
                Path(srt_path).write_text(best.read_text(encoding="utf-8", errors="ignore"), encoding="utf-8")
            except Exception:
                pass
        if not os.path.exists(srt_path):
            try:
                extract_cmd = [
                    "ffmpeg", "-y", "-i", dl_path,
                    "-map", "0:s:0", "-c:s", "srt", srt_path
                ]
                subprocess.run(extract_cmd, capture_output=True, text=True, timeout=45)
            except subprocess.TimeoutExpired:
                logger.warning("Subtitle extraction timed out; skipping embedded subs")
            except Exception:
                pass

        # 3) if no SRT and want_asr, run python ASR fallback
        if not os.path.exists(srt_path) and req.want_asr:
            ok = try_python_asr_to_srt(dl_path, srt_path)
            if not ok:
                srt_path = ""

        # 4) build clips
        clips: List[Dict[str, str]] = []
        if srt_path and os.path.exists(srt_path):
            items = parse_srt_simple(srt_path)
            clips = select_windows_from_srt(items, req.min_sec, req.max_sec, top_k=req.top_k)
        else:
            # fallback: split video into windows heuristically
            total = max(0, int(get_duration_seconds(dl_path)))
            if total == 0:
                raise HTTPException(status_code=400, detail="æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œä¸”æ— å­—å¹•å¯ä¾›åˆ†æ")
            start = 10
            while start + req.min_sec < total and len(clips) < req.top_k:
                end = min(total, start + req.max_sec)
                ss = f"{start//60:02d}:{start%60:02d}"
                ee = f"{end//60:02d}:{end%60:02d}"
                clips.append({"start": ss, "end": ee, "reason": "window"})
                start = end + 2

        if not clips:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨ç‰‡æ®µ")

        # 5) æ™ºèƒ½é€‰æ®µæˆ–ä¼ ç»Ÿé€‰æ®µ
        total = max(0, int(get_duration_seconds(dl_path)))
        if total == 0:
            raise HTTPException(status_code=400, detail="è§†é¢‘æ—¶é•¿ä¸å¯ç”¨")

        window_min = int(req.min_sec)
        window_max = int(req.max_sec)
        
        if req.smart_mode:
            # ä½¿ç”¨æ™ºèƒ½é€‰æ®µ
            logger.info("Using smart segment selection")
            try:
                smart_segments = get_smart_segments(dl_path, window_min, window_max, count=1)
                if smart_segments:
                    best_segment = smart_segments[0]
                    best_ss = best_segment['start_time']
                    best_dur = best_segment['duration']
                    logger.info(f"Smart selection: start={best_ss}s, duration={best_dur}s, score={best_segment['total_score']:.3f}")
                else:
                    raise ValueError("Smart selection failed")
            except Exception as e:
                logger.warning(f"Smart selection failed: {e}, falling back to traditional method")
                req.smart_mode = False  # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        
        if not req.smart_mode:
            # ä¼ ç»Ÿé€‰æ®µæ–¹æ³• (åŸæœ‰é€»è¾‘)
            scan_step = 1
            best_score = -1e9
            best_ss = 0
            best_dur = window_min
            max_scan = min(total - window_min, 15 * 60)  # up to 15 min scan
            for start_s in range(0, max(1, max_scan), scan_step):
                ss_hms = seconds_to_hms(start_s)
                # evaluate at min window first
                black_frac = black_fraction_in_segment(dl_path, ss_hms, float(window_min))
                if black_frac >= 0.5:
                    continue
                sil_frac = silence_fraction_in_segment(dl_path, ss_hms, float(window_min))
                # simple score: prefer less black/silence, earlier windows
                score = (1.0 - black_frac) * 1.5 + (1.0 - sil_frac) * 1.0 - 0.001 * start_s
                if score > best_score:
                    best_score = score
                    best_ss = start_s
                    best_dur = window_min
            if best_score < -1e8:
                # fallback to the first provided clip
                first = clips[0]
                best_ss = int(hms_to_seconds(normalize_time(first["start"])))
                best_dur = max(window_min, int(hms_to_seconds(normalize_time(first["end"])) - best_ss))

        start_time = seconds_to_hms(best_ss)
        dur_s = float(best_dur)
        fade_out_start = max(0.1, dur_s - 0.25)
        # pad-first to 1080x1920, avoiding black due to crop
        vf_pad = (
            "scale=1080:1920:force_original_aspect_ratio=decrease,"
            "pad=1080:1920:(ow-iw)/2:(oh-ih)/2,format=yuv420p,setsar=1:1,"
            f"fade=t=in:st=0:d=0.25,fade=t=out:st={fade_out_start:.2f}:d=0.25"
        )

        out_final = req.output if req.output else str((Path("output_data") / f"intro_{ts}_916.mp4").resolve())
        cmd_single = [
            "ffmpeg", "-y", "-hwaccel", "none",
            "-i", dl_path,
            "-ss", start_time, "-t", f"{dur_s:.2f}",
            "-vf", vf_pad,
            # do not force frame rate; honor input
            "-pix_fmt", "yuv420p",
            "-map", "0:v:0", "-map", "0:a?",
            "-c:v", "libx264", "-preset", "veryfast", "-crf", str(VIDEO_CRF),
            "-c:a", "aac", "-b:a", AUDIO_BITRATE,
            "-shortest", "-movflags", "+faststart",
            out_final
        ]
        safe_run_ffmpeg(cmd_single)

        # 7) optionally burn subtitles if we have srt and only if you want (requirement said: if has subs, do not add)
        # We skip burning here as per requirement to keep existing subtitles if present.

        return {"out": out_final, "parts": [], "clips": [{"start": seconds_to_hms(best_ss), "end": seconds_to_hms(best_ss + best_dur), "reason": "smart_window"}]}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Auto intro error: {e}")
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨ç®€ä»‹è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    from core.config import API_HOST, API_PORT
    
    logger.info(f"Starting API server on {API_HOST}:{API_PORT}")
    uvicorn.run(app, host=API_HOST, port=API_PORT)
